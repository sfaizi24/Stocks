{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Separated Data Collection - Stock Prices and Financial Statements\n",
        "\n",
        "This notebook collects the **SAME DATA** as yearly.ipynb but splits it into two separate tables:\n",
        "\n",
        "1. **Stock Price Data** (`stock_prices_YYYY.csv`):\n",
        "   - Aligned to calendar quarter ends (March 31, June 30, Sept 30, Dec 31)\n",
        "   - Contains: ticker, company_name, quarter_end_date, stock_price, market_cap, mkt_cap_rank, industry, sector, isETF, isFund\n",
        "\n",
        "2. **Financial Statement Data** (`financial_statements_YYYY.csv`):\n",
        "   - Aligned to company fiscal quarters with calendar date mapping\n",
        "   - Contains: ticker, company_name, fiscal_quarter, fiscal_year, calendar_date, debt_to_assets, book_to_market, earnings_yield, industry, sector\n",
        "   - Now captures all 4 fiscal quarters that overlap with the calendar year\n",
        "\n",
        "**Key features (SAME as yearly.ipynb):**\n",
        "- Market cap filter: Only collects data for stocks with market cap > $1B\n",
        "- Rate limiting: 750 API calls per minute\n",
        "- Year-by-year collection with historical ticker lists\n",
        "- Batch processing for efficiency\n",
        "- Error tracking and progress saves\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rate limit configured: 750 calls/minute (0.08 seconds/call)\n",
            "Market cap filter: > $1B\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from typing import Optional, List, Dict, Any, Tuple\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load API key from .env file\n",
        "load_dotenv(\".env\")\n",
        "API = os.getenv(\"API\")  \n",
        "\n",
        "# Rate limiting configuration (SAME as yearly.ipynb)\n",
        "API_CALLS_PER_MINUTE = 750\n",
        "SECONDS_PER_CALL = 60 / API_CALLS_PER_MINUTE  # 0.08 seconds per call\n",
        "\n",
        "# Session and timer for rate limiting\n",
        "session = requests.Session()\n",
        "LAST_API_CALL = 0.0\n",
        "\n",
        "# Market cap threshold (1 billion) - SAME as yearly.ipynb\n",
        "MARKET_CAP_THRESHOLD = 1e9\n",
        "\n",
        "print(f\"Rate limit configured: {API_CALLS_PER_MINUTE} calls/minute ({SECONDS_PER_CALL:.2f} seconds/call)\")\n",
        "print(f\"Market cap filter: > ${MARKET_CAP_THRESHOLD/1e9:.0f}B\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core helper functions (EXACT SAME as yearly.ipynb)\n",
        "def get_json(url: str, params: Dict[str, Any] = {}) -> Optional[Any]:\n",
        "    \"\"\"Safely get JSON data from API with error handling and rate limit retry\"\"\"\n",
        "    global LAST_API_CALL, session\n",
        "    try:\n",
        "        params['apikey'] = API\n",
        "        elapsed = time.time() - LAST_API_CALL\n",
        "        if elapsed < SECONDS_PER_CALL:\n",
        "            time.sleep(SECONDS_PER_CALL - elapsed)\n",
        "        response = session.get(url, params=params, timeout=10)\n",
        "        LAST_API_CALL = time.time()\n",
        "        if response.status_code == 429:\n",
        "            print('‚ö†Ô∏è  Rate limit hit! Waiting 30 seconds...')\n",
        "            time.sleep(30)\n",
        "            return get_json(url, params)\n",
        "        response.raise_for_status()\n",
        "        js = response.json()\n",
        "        if isinstance(js, dict) and 'historical' in js:\n",
        "            return js['historical']\n",
        "        elif isinstance(js, list):\n",
        "            return js\n",
        "        else:\n",
        "            return js\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f'HTTP Error {e.response.status_code}: {e}')\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f'Error fetching data: {e}')\n",
        "        return None\n",
        "\n",
        "def check_market_cap(ticker: str, year: int, precomputed: Optional[float] = None) -> Tuple[bool, Optional[float]]:\n",
        "    \"\"\"Check if ticker had market cap above threshold in given year\"\"\"\n",
        "    if precomputed is not None:\n",
        "        return precomputed > MARKET_CAP_THRESHOLD, precomputed\n",
        "    try:\n",
        "        start_date = f'{year}-01-01'\n",
        "        end_date = f'{year}-12-31'\n",
        "        mc_data = get_json(\n",
        "            f'https://financialmodelingprep.com/api/v3/historical-market-capitalization/{ticker}',\n",
        "            {'from': start_date, 'to': end_date}\n",
        "        )\n",
        "        if not mc_data:\n",
        "            return False, None\n",
        "        mc_df = pd.DataFrame(mc_data)\n",
        "        avg_market_cap = mc_df['marketCap'].mean()\n",
        "        return avg_market_cap > MARKET_CAP_THRESHOLD, avg_market_cap\n",
        "    except Exception as e:\n",
        "        print(f'Error checking market cap for {ticker}: {e}')\n",
        "        return False, None\n",
        "\n",
        "def get_bulk_profiles(tickers: List[str]) -> Dict[str, Any]:\n",
        "    \"\"\"Fetch company profiles in bulk.\"\"\"\n",
        "    data = get_json(f'https://financialmodelingprep.com/api/v3/profile/{','.join(tickers)}')\n",
        "    profiles = {}\n",
        "    if isinstance(data, list):\n",
        "        for item in data:\n",
        "            symbol = item.get('symbol')\n",
        "            profiles[symbol] = item\n",
        "    return profiles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CRITICAL FUNCTION - EXACT SAME LOGIC AS yearly.ipynb\n",
        "def get_historical_tickers(year: int) -> List[str]:\n",
        "    \"\"\"Get list of US tickers that existed in a specific year\"\"\"\n",
        "    print(f\"Fetching ticker list for year {year}...\")\n",
        "    \n",
        "    # Try to get historical ticker list from end of previous year\n",
        "    date = f\"{year-1}-12-31\"\n",
        "    \n",
        "    # First try to get available stocks for that date\n",
        "    available_stocks = get_json(\n",
        "        f\"https://financialmodelingprep.com/api/v3/available-traded/list\",\n",
        "        {\"date\": date}\n",
        "    )\n",
        "    \n",
        "    if available_stocks:\n",
        "        # Filter for US exchanges\n",
        "        us_tickers = [\n",
        "            stock[\"symbol\"] for stock in available_stocks \n",
        "            if stock.get(\"exchangeShortName\") in [\"NYSE\", \"NASDAQ\", \"AMEX\"]\n",
        "            and len(stock[\"symbol\"]) <= 5\n",
        "            and \".\" not in stock[\"symbol\"]\n",
        "        ]\n",
        "        print(f\"‚úÖ Found {len(us_tickers)} US tickers for {year}\")\n",
        "        return us_tickers\n",
        "    \n",
        "    # Fallback: use current ticker list with a warning\n",
        "    print(f\"‚ö†Ô∏è  Could not get historical ticker list for {year}, using current list\")\n",
        "    tickers_data = get_json(\"https://financialmodelingprep.com/api/v3/stock/list\")\n",
        "    \n",
        "    if tickers_data:\n",
        "        # Filter for US exchanges and remove penny stocks\n",
        "        us_tickers = [\n",
        "            d[\"symbol\"] for d in tickers_data \n",
        "            if d[\"exchangeShortName\"] in [\"NYSE\", \"NASDAQ\"] \n",
        "            and (d.get(\"price\") is not None and d.get(\"price\", 0) > 5)\n",
        "            and len(d[\"symbol\"]) <= 5\n",
        "            and \".\" not in d[\"symbol\"]\n",
        "        ]\n",
        "        \n",
        "        print(f\"‚úÖ Found {len(us_tickers)} current US tickers\")\n",
        "        return us_tickers\n",
        "    else:\n",
        "        print(\"‚ùå Failed to fetch ticker list. Using sample tickers.\")\n",
        "        return [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"META\", \"NVDA\", \"JPM\", \"JNJ\", \"V\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXED process_ticker_year to return separated data with proper fiscal quarter handling\n",
        "def process_ticker_year_separated(ticker: str, year: int, profile_data: Optional[Dict[str, Any]] = None, \n",
        "                                 avg_market_cap: Optional[float] = None) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame], Dict[str, Any], int]:\n",
        "    \"\"\"Process data for a single ticker for a specific year - returns separated price and statement data\"\"\"\n",
        "    error_log = {'ticker': ticker, 'year': year, 'errors': []}\n",
        "    api_calls = 0\n",
        "    \n",
        "    try:\n",
        "        # Check market cap (SAME as yearly.ipynb)\n",
        "        is_large_cap, avg_market_cap = check_market_cap(ticker, year, precomputed=avg_market_cap)\n",
        "        if avg_market_cap is None:\n",
        "            api_calls += 1\n",
        "        \n",
        "        if not is_large_cap:\n",
        "            error_log['errors'].append(f'Market cap below threshold (avg: ${avg_market_cap:,.0f})')\n",
        "            return None, None, error_log, api_calls\n",
        "        \n",
        "        start_date = datetime(year, 1, 1)\n",
        "        end_date = datetime(year, 12, 31)\n",
        "        \n",
        "        # Get all the data - use broader date range for financial statements\n",
        "        # Extend date range for financial statements to capture fiscal years\n",
        "        fs_start_date = datetime(year - 1, 1, 1)\n",
        "        fs_end_date = datetime(year + 1, 12, 31)\n",
        "        \n",
        "        bs = get_json(f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}', \n",
        "                     {'period': 'quarter', 'limit': 100})\n",
        "        api_calls += 1\n",
        "        \n",
        "        inc = get_json(f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}', \n",
        "                      {'period': 'quarter', 'limit': 100})\n",
        "        api_calls += 1\n",
        "        \n",
        "        # Get market cap data for broader range to cover all fiscal quarters\n",
        "        mc = get_json(f'https://financialmodelingprep.com/api/v3/historical-market-capitalization/{ticker}', \n",
        "                     {'from': fs_start_date.strftime('%Y-%m-%d'), 'to': fs_end_date.strftime('%Y-%m-%d')})\n",
        "        api_calls += 1\n",
        "        \n",
        "        # Get price data for broader range to cover all fiscal quarters\n",
        "        px = get_json(f'https://financialmodelingprep.com/api/v3/historical-price-full/{ticker}', \n",
        "                     {'from': fs_start_date.strftime('%Y-%m-%d'), 'to': fs_end_date.strftime('%Y-%m-%d')})\n",
        "        api_calls += 1\n",
        "        \n",
        "        if profile_data is None:\n",
        "            profile = get_json(f'https://financialmodelingprep.com/api/v3/profile/{ticker}')\n",
        "            api_calls += 1\n",
        "        else:\n",
        "            profile = [profile_data] if isinstance(profile_data, dict) else profile_data\n",
        "        \n",
        "        if not all([bs, inc, mc, px, profile]):\n",
        "            if not bs: error_log['errors'].append('No balance sheet data')\n",
        "            if not inc: error_log['errors'].append('No income statement data')\n",
        "            if not mc: error_log['errors'].append('No market cap data')\n",
        "            if not px: error_log['errors'].append('No price data')\n",
        "            if not profile: error_log['errors'].append('No profile data')\n",
        "            return None, None, error_log, api_calls\n",
        "        \n",
        "        # Extract profile info\n",
        "        profile_info = profile[0] if profile and len(profile) > 0 else {}\n",
        "        company_name = profile_info.get('companyName', '')\n",
        "        industry = profile_info.get('industry', 'Unknown')\n",
        "        sector = profile_info.get('sector', 'Unknown')\n",
        "        is_etf = profile_info.get('isEtf', False)\n",
        "        is_fund = profile_info.get('isFund', False)\n",
        "        \n",
        "        # Process all data\n",
        "        bs_df = pd.DataFrame(bs)\n",
        "        bs_df['date'] = pd.to_datetime(bs_df['date'])\n",
        "        \n",
        "        inc_df = pd.DataFrame(inc)\n",
        "        inc_df['date'] = pd.to_datetime(inc_df['date'])\n",
        "        \n",
        "        mc_df = pd.DataFrame(mc)\n",
        "        mc_df['date'] = pd.to_datetime(mc_df['date'])\n",
        "        \n",
        "        px_df = pd.DataFrame(px)\n",
        "        px_df['date'] = pd.to_datetime(px_df['date'])\n",
        "        \n",
        "        # Create calendar quarter end dates\n",
        "        quarter_dates = [\n",
        "            f\"{year}-03-31\",\n",
        "            f\"{year}-06-30\",\n",
        "            f\"{year}-09-30\",\n",
        "            f\"{year}-12-31\"\n",
        "        ]\n",
        "        \n",
        "        # 1. Create Stock Price Data (aligned to calendar quarters)\n",
        "        price_data_list = []\n",
        "        for quarter_date in quarter_dates:\n",
        "            # Find closest price to quarter end\n",
        "            quarter_dt = pd.to_datetime(quarter_date)\n",
        "            px_quarter = px_df[abs(px_df['date'] - quarter_dt) <= pd.Timedelta(days=7)]\n",
        "            \n",
        "            if len(px_quarter) > 0:\n",
        "                # Get closest date\n",
        "                closest_idx = abs(px_quarter['date'] - quarter_dt).idxmin()\n",
        "                price_row = px_quarter.loc[closest_idx]\n",
        "                \n",
        "                # Get market cap for this date\n",
        "                mc_quarter = mc_df[abs(mc_df['date'] - quarter_dt) <= pd.Timedelta(days=7)]\n",
        "                if len(mc_quarter) > 0:\n",
        "                    closest_mc_idx = abs(mc_quarter['date'] - quarter_dt).idxmin()\n",
        "                    market_cap = mc_quarter.loc[closest_mc_idx, 'marketCap']\n",
        "                else:\n",
        "                    market_cap = None\n",
        "                \n",
        "                if market_cap and market_cap >= MARKET_CAP_THRESHOLD:\n",
        "                    price_data_list.append({\n",
        "                        'ticker': ticker,\n",
        "                        'company_name': company_name,\n",
        "                        'quarter_end_date': quarter_date,\n",
        "                        'stock_price': price_row['adjClose'],\n",
        "                        'market_cap': market_cap,\n",
        "                        'industry': industry,\n",
        "                        'sector': sector,\n",
        "                        'isETF': is_etf,\n",
        "                        'isFund': is_fund\n",
        "                    })\n",
        "        \n",
        "        # 2. Create Financial Statement Data (find best 4 quarters for the calendar year)\n",
        "        statement_data_list = []\n",
        "        \n",
        "        # Merge balance sheet and income statement by date\n",
        "        bs_quarters = bs_df[['date', 'shortTermDebt', 'longTermDebt', 'totalAssets', \n",
        "                             'totalStockholdersEquity', 'commonStock']].copy()\n",
        "        inc_quarters = inc_df[['date', 'eps', 'weightedAverageShsOut', 'period', \n",
        "                              'calendarYear', 'netIncome']].copy()\n",
        "        \n",
        "        # Join on date\n",
        "        merged_statements = pd.merge(bs_quarters, inc_quarters, on='date', how='inner')\n",
        "        \n",
        "        if len(merged_statements) == 0:\n",
        "            # If no merged statements, just return price data\n",
        "            price_df = pd.DataFrame(price_data_list) if price_data_list else None\n",
        "            return price_df, None, error_log, api_calls\n",
        "        \n",
        "        # Sort by date\n",
        "        merged_statements = merged_statements.sort_values('date')\n",
        "        \n",
        "        # Find the 4 quarters that best overlap with the calendar year\n",
        "        # Score each quarter based on how well it represents the calendar year\n",
        "        scored_quarters = []\n",
        "        for _, row in merged_statements.iterrows():\n",
        "            quarter_date = row['date']\n",
        "            \n",
        "            # Calculate relevance score for this quarter to the calendar year\n",
        "            # Quarters ending in the calendar year get highest score\n",
        "            if quarter_date.year == year:\n",
        "                if quarter_date.month in [3, 6, 9, 12]:  # Standard quarters\n",
        "                    score = 10 + (12 - abs(quarter_date.month - 6))  # Prefer middle of year\n",
        "                else:\n",
        "                    score = 8\n",
        "            # Quarters ending in Q1 of following year (for companies with Dec fiscal year)\n",
        "            elif quarter_date.year == year + 1 and quarter_date.month <= 3:\n",
        "                score = 7\n",
        "            # Quarters ending in Q4 of previous year (for companies with early fiscal year)\n",
        "            elif quarter_date.year == year - 1 and quarter_date.month >= 10:\n",
        "                score = 6\n",
        "            else:\n",
        "                score = 0\n",
        "            \n",
        "            if score > 0:\n",
        "                scored_quarters.append((score, row))\n",
        "        \n",
        "        # Sort by score (descending) and take top 4\n",
        "        scored_quarters.sort(key=lambda x: x[0], reverse=True)\n",
        "        top_quarters = [quarter for score, quarter in scored_quarters[:4]]\n",
        "        \n",
        "        for row in top_quarters:\n",
        "            fiscal_date = row['date'].strftime('%Y-%m-%d')\n",
        "            \n",
        "            # Get market cap for this fiscal date using the broader dataset we already have\n",
        "            mc_fiscal = mc_df[abs(mc_df['date'] - row['date']) <= pd.Timedelta(days=10)]\n",
        "            if len(mc_fiscal) > 0:\n",
        "                closest_mc_idx = abs(mc_fiscal['date'] - row['date']).idxmin()\n",
        "                market_cap = mc_fiscal.loc[closest_mc_idx, 'marketCap']\n",
        "            else:\n",
        "                # Skip this quarter if no market cap data\n",
        "                continue\n",
        "            \n",
        "            if market_cap < MARKET_CAP_THRESHOLD:\n",
        "                continue\n",
        "            \n",
        "            # Get stock price for this date using the broader dataset we already have  \n",
        "            px_fiscal = px_df[abs(px_df['date'] - row['date']) <= pd.Timedelta(days=10)]\n",
        "            if len(px_fiscal) > 0:\n",
        "                closest_px_idx = abs(px_fiscal['date'] - row['date']).idxmin()\n",
        "                stock_price = px_fiscal.loc[closest_px_idx, 'adjClose']\n",
        "            else:\n",
        "                # Skip this quarter if no price data\n",
        "                continue\n",
        "            \n",
        "            # Calculate ratios\n",
        "            total_debt = (row['shortTermDebt'] or 0) + (row['longTermDebt'] or 0)\n",
        "            debt_to_assets = total_debt / row['totalAssets'] if row['totalAssets'] > 0 else None\n",
        "            \n",
        "            if stock_price and row['weightedAverageShsOut'] > 0:\n",
        "                book_to_market = (row['totalStockholdersEquity'] / row['weightedAverageShsOut']) / stock_price\n",
        "                earnings_yield = row['eps'] / stock_price if row['eps'] is not None else None\n",
        "            else:\n",
        "                book_to_market = None\n",
        "                earnings_yield = None\n",
        "            \n",
        "            statement_data_list.append({\n",
        "                'ticker': ticker,\n",
        "                'company_name': company_name,\n",
        "                'fiscal_quarter': row['period'],\n",
        "                'fiscal_year': row['calendarYear'],\n",
        "                'calendar_date': fiscal_date,\n",
        "                'debt_to_assets': debt_to_assets,\n",
        "                'book_to_market': book_to_market,\n",
        "                'earnings_yield': earnings_yield,\n",
        "                'industry': industry,\n",
        "                'sector': sector\n",
        "            })\n",
        "        \n",
        "        # Convert to DataFrames\n",
        "        price_df = pd.DataFrame(price_data_list) if price_data_list else None\n",
        "        statement_df = pd.DataFrame(statement_data_list) if statement_data_list else None\n",
        "        \n",
        "        if price_df is None and statement_df is None:\n",
        "            error_log['errors'].append('No valid data after processing')\n",
        "            return None, None, error_log, api_calls\n",
        "        \n",
        "        return price_df, statement_df, error_log, api_calls\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_log['errors'].append(f'Exception: {str(e)}')\n",
        "        return None, None, error_log, api_calls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main collection function - modified from yearly.ipynb to handle separated data\n",
        "def collect_year_data_separated(tickers: List[str], year: int, max_tickers: Optional[int] = None, \n",
        "                               save_progress: bool = True, progress_interval: int = 100, \n",
        "                               batch_size: int = 50) -> Tuple[pd.DataFrame, pd.DataFrame, List[Dict]]:\n",
        "    \"\"\"Collect separated price and statement data for multiple tickers for a specific year\"\"\"\n",
        "    all_price_data = []\n",
        "    all_statement_data = []\n",
        "    all_errors = []\n",
        "    successful_tickers = []\n",
        "    failed_tickers = []\n",
        "    skipped_tickers = []\n",
        "    total_api_calls = 0\n",
        "    \n",
        "    tickers_to_process = tickers[:max_tickers] if max_tickers else tickers\n",
        "    total_tickers = len(tickers_to_process)\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"  COLLECTING SEPARATED DATA FOR YEAR {year}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Total tickers to check: {total_tickers}\")\n",
        "    print(f\"Market cap filter: >${MARKET_CAP_THRESHOLD/1e9:.0f}B\")\n",
        "    print(f\"API rate limit: {API_CALLS_PER_MINUTE} calls/minute\")\n",
        "    print(f\"Batch size: {batch_size} tickers\")\n",
        "    print(f\"Progress saves: Every {progress_interval} tickers\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Process tickers in batches (SAME as yearly.ipynb)\n",
        "    for batch_start in range(0, total_tickers, batch_size):\n",
        "        batch_end = min(batch_start + batch_size, total_tickers)\n",
        "        batch_tickers = tickers_to_process[batch_start:batch_end]\n",
        "        \n",
        "        # Progress update\n",
        "        if batch_start > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            avg_time = elapsed / batch_start\n",
        "            remaining = (total_tickers - batch_start) * avg_time\n",
        "            \n",
        "            print(f\"\\n[Progress: {batch_start}/{total_tickers} ({batch_start/total_tickers*100:.1f}%)]\")\n",
        "            print(f\"  Time: {elapsed/60:.1f}min elapsed, ~{remaining/60:.1f}min remaining\")\n",
        "            print(f\"  Success: {len(successful_tickers)}, Failed: {len(failed_tickers)}, Skipped (small cap): {len(skipped_tickers)}\")\n",
        "            print(f\"  API calls: {total_api_calls} ({total_api_calls/elapsed*60:.0f}/minute avg)\")\n",
        "        \n",
        "        print(f\"\\n  Processing batch {batch_start//batch_size + 1}: tickers {batch_start+1}-{batch_end}\")\n",
        "        \n",
        "        # Get bulk profiles for the batch (1 API call for up to 50 tickers)\n",
        "        profiles = get_bulk_profiles(batch_tickers)\n",
        "        total_api_calls += 1\n",
        "        \n",
        "        # Process each ticker in the batch\n",
        "        for i, ticker in enumerate(batch_tickers):\n",
        "            profile_data = profiles.get(ticker)\n",
        "            \n",
        "            # Process ticker with pre-fetched profile\n",
        "            price_data, statement_data, error_log, api_calls = process_ticker_year_separated(\n",
        "                ticker, year, profile_data=profile_data\n",
        "            )\n",
        "            total_api_calls += api_calls\n",
        "            \n",
        "            if (price_data is not None and len(price_data) > 0) or (statement_data is not None and len(statement_data) > 0):\n",
        "                if price_data is not None:\n",
        "                    all_price_data.append(price_data)\n",
        "                if statement_data is not None:\n",
        "                    all_statement_data.append(statement_data)\n",
        "                successful_tickers.append(ticker)\n",
        "                print(\"‚úì\", end=\"\", flush=True)\n",
        "            elif any(\"Market cap below threshold\" in err for err in error_log.get(\"errors\", [])):\n",
        "                skipped_tickers.append(ticker)\n",
        "                print(\"‚óã\", end=\"\", flush=True)\n",
        "            else:\n",
        "                failed_tickers.append(ticker)\n",
        "                all_errors.append(error_log)\n",
        "                print(\"‚úó\", end=\"\", flush=True)\n",
        "        \n",
        "        # Save progress periodically\n",
        "        if save_progress and (batch_end % progress_interval == 0 or batch_end == total_tickers):\n",
        "            if all_price_data:\n",
        "                temp_price_df = pd.concat(all_price_data, ignore_index=True)\n",
        "                temp_price_df['mkt_cap_rank'] = temp_price_df.groupby('quarter_end_date')['market_cap'].rank(\n",
        "                    method='dense', ascending=False).astype(int)\n",
        "                progress_price_filename = f\"progress_prices_{year}_tickers_{batch_end}.csv\"\n",
        "                temp_price_df.to_csv(progress_price_filename, index=False)\n",
        "                print(f\"\\n  üíæ Price progress saved: {progress_price_filename} ({len(temp_price_df)} rows)\")\n",
        "            \n",
        "            if all_statement_data:\n",
        "                temp_statement_df = pd.concat(all_statement_data, ignore_index=True)\n",
        "                progress_statement_filename = f\"progress_statements_{year}_tickers_{batch_end}.csv\"\n",
        "                temp_statement_df.to_csv(progress_statement_filename, index=False)\n",
        "                print(f\"  üíæ Statement progress saved: {progress_statement_filename} ({len(temp_statement_df)} rows)\")\n",
        "    \n",
        "    # Final summary\n",
        "    total_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\n\\n{'='*70}\")\n",
        "    print(f\"  YEAR {year} COLLECTION COMPLETE\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Total time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
        "    print(f\"Successful: {len(successful_tickers)} tickers\")\n",
        "    print(f\"Failed: {len(failed_tickers)} tickers\")\n",
        "    print(f\"Skipped (small cap): {len(skipped_tickers)} tickers\")\n",
        "    print(f\"Total API calls: {total_api_calls:,} ({total_api_calls/total_time*60:.0f}/minute avg)\")\n",
        "    \n",
        "    # Combine all data\n",
        "    if all_price_data:\n",
        "        final_price_df = pd.concat(all_price_data, ignore_index=True)\n",
        "        # Add market cap ranking\n",
        "        final_price_df['mkt_cap_rank'] = final_price_df.groupby('quarter_end_date')['market_cap'].rank(\n",
        "            method='dense', ascending=False).astype(int)\n",
        "        # Sort by ticker and quarter\n",
        "        final_price_df = final_price_df.sort_values(['ticker', 'quarter_end_date']).reset_index(drop=True)\n",
        "    else:\n",
        "        final_price_df = pd.DataFrame()\n",
        "    \n",
        "    if all_statement_data:\n",
        "        final_statement_df = pd.concat(all_statement_data, ignore_index=True)\n",
        "        # Sort by ticker and date\n",
        "        final_statement_df = final_statement_df.sort_values(['ticker', 'calendar_date']).reset_index(drop=True)\n",
        "    else:\n",
        "        final_statement_df = pd.DataFrame()\n",
        "    \n",
        "    print(f\"\\nüìä Final datasets:\")\n",
        "    print(f\"   Price data: {len(final_price_df)} rows, {final_price_df['ticker'].nunique() if len(final_price_df) > 0 else 0} tickers\")\n",
        "    print(f\"   Statement data: {len(final_statement_df)} rows, {final_statement_df['ticker'].nunique() if len(final_statement_df) > 0 else 0} tickers\")\n",
        "    \n",
        "    # Save error log\n",
        "    if all_errors:\n",
        "        error_filename = f\"errors_{year}.json\"\n",
        "        with open(error_filename, 'w') as f:\n",
        "            json.dump(all_errors, f, indent=2, default=str)\n",
        "        print(f\"\\nüìù Error log saved: {error_filename} ({len(all_errors)} errors)\")\n",
        "    \n",
        "    # Clean up progress files\n",
        "    if save_progress:\n",
        "        for progress_file in [f for f in os.listdir('.') if f.startswith(f'progress_prices_{year}_') or f.startswith(f'progress_statements_{year}_')]:\n",
        "            os.remove(progress_file)\n",
        "        print(f\"üßπ Cleaned up progress files\")\n",
        "    \n",
        "    return final_price_df, final_statement_df, all_errors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Git commands to sync and push data\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "def git_push_year_data(year: int):\n",
        "    \"\"\"Push year data to GitHub with proper branch detection\"\"\"\n",
        "    try:\n",
        "        print(\"üîÑ Syncing with remote repository...\")\n",
        "        \n",
        "        # Get current branch name (run from parent directory)\n",
        "        result = subprocess.run([\"git\", \"branch\", \"--show-current\"], \n",
        "                              capture_output=True, text=True, check=True, cwd=\"..\")\n",
        "        current_branch = result.stdout.strip()\n",
        "        \n",
        "        # Add files with proper paths (files are in yearly runs directory)\n",
        "        files_to_add = [f\"yearly runs/stock_prices_{year}.csv\", \n",
        "                       f\"yearly runs/financial_statements_{year}.csv\", \n",
        "                       f\"yearly runs/errors_{year}.json\"]\n",
        "        \n",
        "        # Check which files exist\n",
        "        existing_files = []\n",
        "        for file_path in files_to_add:\n",
        "            if os.path.exists(f\"../{file_path}\"):  # Check from notebook's perspective\n",
        "                existing_files.append(file_path)\n",
        "        \n",
        "        if existing_files:\n",
        "            print(f\"üìÅ Adding files: {existing_files}\")\n",
        "            subprocess.run([\"git\", \"add\"] + existing_files, check=True, cwd=\"..\")\n",
        "            \n",
        "            print(f\"üíæ Committing: Add separated data for {year}\")\n",
        "            subprocess.run([\"git\", \"commit\", \"-m\", f\"Add separated data for {year}\"], check=True, cwd=\"..\")\n",
        "            \n",
        "            print(\"üöÄ Pushing to GitHub...\")\n",
        "            subprocess.run([\"git\", \"push\", \"origin\", current_branch], check=True, cwd=\"..\")\n",
        "            \n",
        "            print(f\"‚úÖ Successfully pushed data for {year} to {current_branch} branch!\")\n",
        "        else:\n",
        "            print(f\"‚ùå No files found to push for {year}\")\n",
        "            print(f\"   Looking for: {files_to_add}\")\n",
        "        \n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Git operation failed: {e}\")\n",
        "        print(\"üí° You may need to resolve conflicts manually\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "# Call this after collecting data for a year\n",
        "# git_push_year_data(2020)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test with Single Ticker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing with AAPL for year 2005...\n",
            "\n",
            "Test completed in 1.27 seconds with 4 API calls\n",
            "\n",
            "‚úÖ Price data collected: 4 records\n",
            "  ticker company_name quarter_end_date  stock_price   market_cap  \\\n",
            "0   AAPL   Apple Inc.       2005-03-31         1.25  34005638240   \n",
            "1   AAPL   Apple Inc.       2005-06-30         1.11  30131226160   \n",
            "2   AAPL   Apple Inc.       2005-09-30         1.61  44430167880   \n",
            "3   AAPL   Apple Inc.       2005-12-31         2.16  59783000760   \n",
            "\n",
            "               industry      sector  isETF  isFund  \n",
            "0  Consumer Electronics  Technology  False   False  \n",
            "1  Consumer Electronics  Technology  False   False  \n",
            "2  Consumer Electronics  Technology  False   False  \n",
            "3  Consumer Electronics  Technology  False   False  \n",
            "\n",
            "‚úÖ Statement data collected: 4 records\n",
            "  ticker company_name fiscal_quarter fiscal_year calendar_date  \\\n",
            "0   AAPL   Apple Inc.             Q3        2005    2005-06-25   \n",
            "1   AAPL   Apple Inc.             Q2        2005    2005-03-26   \n",
            "2   AAPL   Apple Inc.             Q4        2005    2005-09-24   \n",
            "3   AAPL   Apple Inc.             Q1        2006    2005-12-31   \n",
            "\n",
            "   debt_to_assets  book_to_market  earnings_yield              industry  \\\n",
            "0             0.0        0.264487        0.012389  Consumer Electronics   \n",
            "1             0.0        0.220474        0.010000  Consumer Electronics   \n",
            "2             0.0        0.201840        0.011687  Consumer Electronics   \n",
            "3             0.0        0.166781        0.011250  Consumer Electronics   \n",
            "\n",
            "       sector  \n",
            "0  Technology  \n",
            "1  Technology  \n",
            "2  Technology  \n",
            "3  Technology  \n",
            "\n",
            "================================================================================\n",
            "Now testing AAPL for 2012 (earliest available year) to verify older year functionality...\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Test with a single ticker (SAME approach as yearly.ipynb)\n",
        "def test_single_ticker(ticker: str, year: int):\n",
        "    \"\"\"Test data collection for a single ticker\"\"\"\n",
        "    print(f\"Testing with {ticker} for year {year}...\")\n",
        "    test_start = time.time()\n",
        "    \n",
        "    # Get profile first\n",
        "    profile_data = get_bulk_profiles([ticker]).get(ticker)\n",
        "    \n",
        "    # Process ticker\n",
        "    price_data, statement_data, error_log, api_calls = process_ticker_year_separated(ticker, year, profile_data)\n",
        "    \n",
        "    test_time = time.time() - test_start\n",
        "    print(f\"\\nTest completed in {test_time:.2f} seconds with {api_calls} API calls\")\n",
        "    \n",
        "    if price_data is not None:\n",
        "        print(f\"\\n‚úÖ Price data collected: {len(price_data)} records\")\n",
        "        print(price_data)\n",
        "    else:\n",
        "        print(\"\\n‚ùå No price data collected\")\n",
        "    \n",
        "    if statement_data is not None:\n",
        "        print(f\"\\n‚úÖ Statement data collected: {len(statement_data)} records\")\n",
        "        print(statement_data)\n",
        "    else:\n",
        "        print(\"\\n‚ùå No statement data collected\")\n",
        "    \n",
        "    if error_log['errors']:\n",
        "        print(f\"\\nErrors: {error_log}\")\n",
        "\n",
        "# Test with AAPL for 2018 to see if we get all 4 quarters\n",
        "test_single_ticker(\"AAPL\", 2005)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Now testing AAPL for 2012 (earliest available year) to verify older year functionality...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test with AAPL for 2012 to see if we get all 4 quarters (2005 data doesn't exist, API only goes back to 2012)\n",
        "#test_single_ticker(\"AAPL\", 2012)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Collect Data for Years\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect data for a specific year (SAME approach as yearly.ipynb)\n",
        "def collect_and_save_year(year: int, max_tickers: Optional[int] = None):\n",
        "    \"\"\"Collect and save separated data for a specific year\"\"\"\n",
        "    \n",
        "    # Get historical ticker list for the year (EXACT SAME as yearly.ipynb)\n",
        "    us_tickers = get_historical_tickers(year)\n",
        "    \n",
        "    # Collect data with optimized batch processing\n",
        "    price_df, statement_df, errors = collect_year_data_separated(\n",
        "        us_tickers, year=year, max_tickers=max_tickers\n",
        "    )\n",
        "    \n",
        "    # Save the data\n",
        "    if len(price_df) > 0:\n",
        "        price_filename = f\"stock_prices_{year}.csv\"\n",
        "        price_df.to_csv(price_filename, index=False)\n",
        "        print(f\"\\n‚úÖ Price data saved to '{price_filename}'\")\n",
        "        \n",
        "        # Show summary statistics\n",
        "        print(f\"\\nüìà Price Data Summary:\")\n",
        "        print(f\"   Records: {len(price_df)}\")\n",
        "        print(f\"   Unique tickers: {price_df['ticker'].nunique()}\")\n",
        "        print(f\"   Date range: {price_df['quarter_end_date'].min()} to {price_df['quarter_end_date'].max()}\")\n",
        "        \n",
        "        # Show top companies by market cap\n",
        "        latest_quarter = price_df['quarter_end_date'].max()\n",
        "        latest_data = price_df[price_df['quarter_end_date'] == latest_quarter]\n",
        "        if len(latest_data) > 0:\n",
        "            print(f\"\\nüèÜ Top 10 companies by market cap ({latest_quarter}):\")\n",
        "            top_10 = latest_data.nsmallest(10, 'mkt_cap_rank')[['ticker', 'company_name', 'mkt_cap_rank', 'market_cap', 'isETF', 'isFund']]\n",
        "            top_10['market_cap'] = top_10['market_cap'].apply(lambda x: f\"${x/1e9:.1f}B\")\n",
        "            print(top_10.to_string(index=False))\n",
        "    \n",
        "    if len(statement_df) > 0:\n",
        "        statement_filename = f\"financial_statements_{year}.csv\"\n",
        "        statement_df.to_csv(statement_filename, index=False)\n",
        "        print(f\"\\n‚úÖ Statement data saved to '{statement_filename}'\")\n",
        "        \n",
        "        # Show summary statistics\n",
        "        print(f\"\\nüìä Statement Data Summary:\")\n",
        "        print(f\"   Records: {len(statement_df)}\")\n",
        "        print(f\"   Unique tickers: {statement_df['ticker'].nunique()}\")\n",
        "        print(f\"   Date range: {statement_df['calendar_date'].min()} to {statement_df['calendar_date'].max()}\")\n",
        "        print(f\"   Fiscal years included: {sorted(statement_df['fiscal_year'].unique())}\")\n",
        "        print(f\"   Debt/Assets - Mean: {statement_df['debt_to_assets'].mean():.3f}, Median: {statement_df['debt_to_assets'].median():.3f}\")\n",
        "        print(f\"   Book/Market - Mean: {statement_df['book_to_market'].mean():.3f}, Median: {statement_df['book_to_market'].median():.3f}\")\n",
        "        print(f\"   Earnings Yield - Mean: {statement_df['earnings_yield'].mean():.3f}, Median: {statement_df['earnings_yield'].median():.3f}\")\n",
        "\n",
        "# Example: Collect 2024 data\n",
        "# To test with fewer tickers first, use max_tickers parameter\n",
        "# collect_and_save_year(2024, max_tickers=100)  # Test with 100 tickers\n",
        "# collect_and_save_year(2024)  # Full collection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect data for multiple years\n",
        "def collect_multiple_years(start_year: int, end_year: int, max_tickers: Optional[int] = None):\n",
        "    \"\"\"Collect data for a range of years\"\"\"\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"  STARTING COLLECTION FOR YEAR {year}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        try:\n",
        "            collect_and_save_year(year, max_tickers=max_tickers)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to collect data for {year}: {e}\")\n",
        "            continue\n",
        "\n",
        "# Example: Collect data for years 2020-2024\n",
        "# collect_multiple_years(2020, 2024, max_tickers=100)  # Test with 100 tickers per year\n",
        "# collect_multiple_years(2020, 2024)  # Full collection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collect Several Years at Once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "collect_multiple_years(2015, 2019, max_tickers=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Individual Year Collection Cells\n",
        "\n",
        "Run these cells one by one to collect data for each year. Each cell is independent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect 2024 data\n",
        "YEAR = 2024\n",
        "MAX_TICKERS = None  # Set to a number like 100 to test with fewer tickers\n",
        "\n",
        "collect_and_save_year(YEAR, max_tickers=MAX_TICKERS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect 2023 data\n",
        "YEAR = 2023\n",
        "MAX_TICKERS = None\n",
        "\n",
        "collect_and_save_year(YEAR, max_tickers=MAX_TICKERS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect 2022 data\n",
        "YEAR = 2022\n",
        "MAX_TICKERS = None\n",
        "\n",
        "collect_and_save_year(YEAR, max_tickers=MAX_TICKERS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect 2021 data\n",
        "YEAR = 2021\n",
        "MAX_TICKERS = None\n",
        "\n",
        "collect_and_save_year(YEAR, max_tickers=MAX_TICKERS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect 2020 data\n",
        "YEAR = 2020\n",
        "MAX_TICKERS = None\n",
        "\n",
        "collect_and_save_year(YEAR, max_tickers=MAX_TICKERS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect 2019 data\n",
        "YEAR = 2019\n",
        "MAX_TICKERS = None\n",
        "\n",
        "collect_and_save_year(YEAR, max_tickers=MAX_TICKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect 2018 data\n",
        "YEAR = 2018\n",
        "MAX_TICKERS = None\n",
        "\n",
        "collect_and_save_year(YEAR, max_tickers=MAX_TICKERS)\n",
        "\n",
        "git_push_year_data(YEAR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect 2017 data\n",
        "YEAR = 2017\n",
        "MAX_TICKERS = None\n",
        "\n",
        "collect_and_save_year(YEAR, max_tickers=MAX_TICKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect 2016 data\n",
        "YEAR = 2016\n",
        "MAX_TICKERS = None\n",
        "\n",
        "collect_and_save_year(YEAR, max_tickers=MAX_TICKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect 2015 data\n",
        "YEAR = 2015\n",
        "MAX_TICKERS = None\n",
        "\n",
        "collect_and_save_year(YEAR, max_tickers=MAX_TICKERS)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
