{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate-Limited Market Cap Filtered Data Collection (Year by Year)\n",
    "\n",
    "This notebook collects financial data for **US stocks with market cap > $1B** with strict rate limiting and saves each year separately.\n",
    "\n",
    "**Key features:**\n",
    "1. **Market cap filter**: Only collects data for stocks with market cap > $1B\n",
    "2. **Rate limiting**: 750 API calls per minute\n",
    "3. **Year-by-year collection**: Each year saved to separate CSV\n",
    "4. **Deduplication**: Ensures only one entry per quarter (Q1-Q4)\n",
    "5. **Enhanced metrics**: Includes book-to-market and earnings yield\n",
    "6. **Error tracking**: Detailed logs for debugging\n",
    "\n",
    "**Target columns:** `quarter`, `ticker`, `industry`, `sector`, `debt_to_assets`, `mkt_cap`, `stock_price`, `book_to_market`, `earnings_yield`, `mkt_cap_rank`\n",
    "\n",
    "**Time estimate:** ~70 minutes per year (vs 2.7 hours without filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🚀 Optimizations Applied\n",
    "\n",
    "This notebook has been optimized with three key improvements:\n",
    "\n",
    "### 1. **Removed Extra Sleep Delays**\n",
    "- The `get_json()` function already enforces rate limiting with sleep delays\n",
    "- Removed redundant sleep in `collect_year_data()` to avoid double-waiting\n",
    "- **Impact**: ~8% faster execution per ticker\n",
    "\n",
    "### 2. **Batch Processing with Bulk Endpoints**\n",
    "- Uses `/api/v3/profile/{ticker1,ticker2,...}` to fetch up to 50 company profiles in one API call\n",
    "- Processes tickers in batches of 50 instead of one-by-one\n",
    "- **Impact**: Reduces profile API calls by 98% (from 50 calls to 1 call per batch)\n",
    "\n",
    "### 3. **Year-Specific Ticker Lists**\n",
    "- Fetches ticker lists from the end of the previous year (e.g., Dec 31, 2022 for 2023 data)\n",
    "- Ensures we only process stocks that actually existed in each historical year\n",
    "- Falls back to current ticker list if historical data unavailable\n",
    "- **Impact**: More accurate historical analysis, avoids processing stocks that didn't exist yet\n",
    "\n",
    "### 📊 Expected Performance Improvements:\n",
    "- **Time savings**: ~30-40% faster overall\n",
    "- **API efficiency**: Better utilization of rate limits\n",
    "- **Data accuracy**: Historical ticker lists match the actual market composition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚠️ MARKET CAP FILTERED COLLECTION\n",
    "\n",
    "**This notebook filters for stocks with market cap > $1B**\n",
    "\n",
    "**Time requirements:**\n",
    "- **Per year:** ~70 minutes (with market cap filtering)\n",
    "- **All 6 years (2019-2024):** ~7 hours total\n",
    "- **API calls:** ~21,000 per year (vs 48,000 without filtering)\n",
    "- **Expected companies:** ~1,800 per year (15-20% of all tickers)\n",
    "\n",
    "**To start with a smaller test:**\n",
    "1. Change `MAX_TICKERS = None` to `MAX_TICKERS = 100` in any collection cell\n",
    "2. Run one year first to verify everything works\n",
    "3. Then change back to `MAX_TICKERS = None` for full collection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions with Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit configured: 750 calls/minute (0.08 seconds/call)\n",
      "Market cap filter: > $1B\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(\".env\")\n",
    "API = os.getenv(\"API\")  \n",
    "\n",
    "# Rate limiting configuration\n",
    "API_CALLS_PER_MINUTE = 750\n",
    "SECONDS_PER_CALL = 60 / API_CALLS_PER_MINUTE  # 0.08 seconds per call\n",
    "\n",
    "# Session and timer for rate limiting\n",
    "session = requests.Session()\n",
    "LAST_API_CALL = 0.0\n",
    "\n",
    "# Market cap threshold (1 billion)\n",
    "MARKET_CAP_THRESHOLD = 1e9\n",
    "\n",
    "print(f\"Rate limit configured: {API_CALLS_PER_MINUTE} calls/minute ({SECONDS_PER_CALL:.2f} seconds/call)\")\n",
    "print(f\"Market cap filter: > ${MARKET_CAP_THRESHOLD/1e9:.0f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7cNMpVzb43GKtm05iRTDWJtyJXSylX8J'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(url: str, params: Dict[str, Any] = {}) -> Optional[Any]:\n",
    "    \"\"\"Safely get JSON data from API with error handling and rate limit retry\"\"\"\n",
    "    global LAST_API_CALL, session\n",
    "    try:\n",
    "        params['apikey'] = API\n",
    "        elapsed = time.time() - LAST_API_CALL\n",
    "        if elapsed < SECONDS_PER_CALL:\n",
    "            time.sleep(SECONDS_PER_CALL - elapsed)\n",
    "        response = session.get(url, params=params, timeout=10)\n",
    "        LAST_API_CALL = time.time()\n",
    "        if response.status_code == 429:\n",
    "            print('⚠️  Rate limit hit! Waiting 30 seconds...')\n",
    "            time.sleep(30)\n",
    "            return get_json(url, params)\n",
    "        response.raise_for_status()\n",
    "        js = response.json()\n",
    "        if isinstance(js, dict) and 'historical' in js:\n",
    "            return js['historical']\n",
    "        elif isinstance(js, list):\n",
    "            return js\n",
    "        else:\n",
    "            return js\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f'HTTP Error {e.response.status_code}: {e}')\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f'Error fetching data: {e}')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_market_cap(ticker: str, year: int, precomputed: Optional[float] = None) -> Tuple[bool, Optional[float]]:\n",
    "    \"\"\"Check if ticker had market cap above threshold in given year\"\"\"\n",
    "    if precomputed is not None:\n",
    "        return precomputed > MARKET_CAP_THRESHOLD, precomputed\n",
    "    try:\n",
    "        start_date = f'{year}-01-01'\n",
    "        end_date = f'{year}-12-31'\n",
    "        mc_data = get_json(\n",
    "            f'https://financialmodelingprep.com/api/v3/historical-market-capitalization/{ticker}',\n",
    "            {'from': start_date, 'to': end_date}\n",
    "        )\n",
    "        if not mc_data:\n",
    "            return False, None\n",
    "        mc_df = pd.DataFrame(mc_data)\n",
    "        avg_market_cap = mc_df['marketCap'].mean()\n",
    "        return avg_market_cap > MARKET_CAP_THRESHOLD, avg_market_cap\n",
    "    except Exception as e:\n",
    "        print(f'Error checking market cap for {ticker}: {e}')\n",
    "        return False, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bulk_profiles(tickers: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Fetch company profiles in bulk.\"\"\"\n",
    "    data = get_json(f'https://financialmodelingprep.com/api/v3/profile/{','.join(tickers)}')\n",
    "    profiles = {}\n",
    "    if isinstance(data, list):\n",
    "        for item in data:\n",
    "            symbol = item.get('symbol')\n",
    "            profiles[symbol] = item\n",
    "    return profiles\n",
    "\n",
    "def get_bulk_market_caps(tickers: List[str], year: int) -> Dict[str, float]:\n",
    "    \"\"\"Fetch average market cap for a list of tickers.\"\"\"\n",
    "    start_date = f'{year}-01-01'\n",
    "    end_date = f'{year}-12-31'\n",
    "    data = get_json(\n",
    "        f'https://financialmodelingprep.com/api/v3/historical-market-capitalization/{','.join(tickers)}',\n",
    "        {'from': start_date, 'to': end_date}\n",
    "    )\n",
    "    caps = {}\n",
    "    if isinstance(data, dict):\n",
    "        for symbol, hist in data.items():\n",
    "            df = pd.DataFrame(hist)\n",
    "            caps[symbol] = df['marketCap'].mean()\n",
    "    return caps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ticker_year(ticker: str, year: int, profile_data: Optional[Dict[str, Any]] = None, avg_market_cap: Optional[float] = None) -> Tuple[Optional[pd.DataFrame], Dict[str, Any], int]:\n",
    "    \"\"\"Process data for a single ticker for a specific year\"\"\"\n",
    "    error_log = {'ticker': ticker, 'year': year, 'errors': []}\n",
    "    api_calls = 0\n",
    "    try:\n",
    "        is_large_cap, avg_market_cap = check_market_cap(ticker, year, precomputed=avg_market_cap)\n",
    "        if avg_market_cap is None:\n",
    "            api_calls += 1\n",
    "        if not is_large_cap:\n",
    "            error_log['errors'].append(f'Market cap below threshold (avg: ${avg_market_cap:,.0f})')\n",
    "            return None, error_log, api_calls\n",
    "        start_date = datetime(year, 1, 1)\n",
    "        end_date = datetime(year, 12, 31)\n",
    "        bs = get_json(f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}', {'period': 'quarter', 'limit': 20})\n",
    "        api_calls += 1\n",
    "        inc = get_json(f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}', {'period': 'quarter', 'limit': 20})\n",
    "        api_calls += 1\n",
    "        mc = get_json(f'https://financialmodelingprep.com/api/v3/historical-market-capitalization/{ticker}', {'from': start_date.strftime('%Y-%m-%d'), 'to': end_date.strftime('%Y-%m-%d')})\n",
    "        api_calls += 1\n",
    "        px = get_json(f'https://financialmodelingprep.com/api/v3/historical-price-full/{ticker}', {'from': start_date.strftime('%Y-%m-%d'), 'to': end_date.strftime('%Y-%m-%d')})\n",
    "        api_calls += 1\n",
    "        if profile_data is None:\n",
    "            profile = get_json(f'https://financialmodelingprep.com/api/v3/profile/{ticker}')\n",
    "            api_calls += 1\n",
    "        else:\n",
    "            profile = [profile_data] if isinstance(profile_data, dict) else profile_data\n",
    "        if not all([bs, inc, mc, px, profile]):\n",
    "            if not bs: error_log['errors'].append('No balance sheet data')\n",
    "            if not inc: error_log['errors'].append('No income statement data')\n",
    "            if not mc: error_log['errors'].append('No market cap data')\n",
    "            if not px: error_log['errors'].append('No price data')\n",
    "            if not profile: error_log['errors'].append('No profile data')\n",
    "            return None, error_log, api_calls\n",
    "        industry = profile[0].get('industry', 'Unknown') if profile and len(profile) > 0 else 'Unknown'\n",
    "        sector = profile[0].get('sector', 'Unknown') if profile and len(profile) > 0 else 'Unknown'\n",
    "        bs_df = pd.DataFrame(bs)\n",
    "        bs_df['date'] = pd.to_datetime(bs_df['date'])\n",
    "        bs_df = bs_df[bs_df['date'].dt.year == year]\n",
    "        if len(bs_df) == 0:\n",
    "            error_log['errors'].append(f'No balance sheet data for year {year}')\n",
    "            return None, error_log, api_calls\n",
    "        bs_df = (bs_df[['date', 'shortTermDebt', 'longTermDebt', 'totalAssets', 'totalStockholdersEquity', 'commonStock']]\n",
    "            .assign(quarter=lambda d: d.date.dt.to_period('Q'),\n",
    "                    debt_to_assets=lambda d: ((d.shortTermDebt.fillna(0) + d.longTermDebt.fillna(0)) / d.totalAssets.replace(0, pd.NA)),\n",
    "                    book_value=lambda d: d.totalStockholdersEquity)\n",
    "            .dropna(subset=['debt_to_assets'])\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('quarter', keep='last'))\n",
    "        inc_df = pd.DataFrame(inc)\n",
    "        inc_df['date'] = pd.to_datetime(inc_df['date'])\n",
    "        inc_df = inc_df[inc_df['date'].dt.year == year]\n",
    "        inc_df = (inc_df[['date', 'eps', 'weightedAverageShsOut']]\n",
    "            .assign(quarter=lambda d: d.date.dt.to_period('Q'))\n",
    "            .rename(columns={'weightedAverageShsOut': 'shares_outstanding'})\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('quarter', keep='last'))\n",
    "        mc_df = (pd.DataFrame(mc)\n",
    "            .assign(date=lambda d: pd.to_datetime(d.date), quarter=lambda d: d.date.dt.to_period('Q'))\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('quarter', keep='last')\n",
    "            .rename(columns={'marketCap': 'mkt_cap'})[['quarter', 'mkt_cap']])\n",
    "        px_df = (pd.DataFrame(px)\n",
    "            .assign(date=lambda d: pd.to_datetime(d.date), quarter=lambda d: d.date.dt.to_period('Q'))\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('quarter', keep='last')\n",
    "            .rename(columns={'adjClose': 'stock_price'})[['quarter', 'stock_price']])\n",
    "        merged = (bs_df.merge(inc_df, on='quarter', how='left')\n",
    "                     .merge(mc_df, on='quarter', how='left')\n",
    "                     .merge(px_df, on='quarter', how='left'))\n",
    "        merged = merged.assign(ticker=ticker, industry=industry, sector=sector,\n",
    "                               book_to_market=lambda d: (d.book_value / d.shares_outstanding) / d.stock_price,\n",
    "                               earnings_yield=lambda d: d.eps / d.stock_price)\n",
    "        merged = merged[['quarter', 'ticker', 'industry', 'sector', 'debt_to_assets', 'mkt_cap', 'stock_price', 'book_to_market', 'earnings_yield']].dropna()\n",
    "        valid_quarters = [f'{year}Q1', f'{year}Q2', f'{year}Q3', f'{year}Q4']\n",
    "        merged = merged[merged['quarter'].astype(str).isin(valid_quarters)]\n",
    "        if len(merged) == 0:\n",
    "            error_log['errors'].append('No valid data after merging')\n",
    "            return None, error_log, api_calls\n",
    "        return merged, error_log, api_calls\n",
    "    except Exception as e:\n",
    "        error_log['errors'].append(f'Exception: {str(e)}')\n",
    "        return None, error_log, api_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_year_data(tickers: List[str], year: int, max_tickers: Optional[int] = None, \n",
    "                     save_progress: bool = True, progress_interval: int = 100, batch_size: int = 50) -> Tuple[pd.DataFrame, List[Dict]]:\n",
    "    \"\"\"Collect data for multiple tickers for a specific year with optimized batch processing\"\"\"\n",
    "    all_data = []\n",
    "    all_errors = []\n",
    "    successful_tickers = []\n",
    "    failed_tickers = []\n",
    "    skipped_tickers = []\n",
    "    total_api_calls = 0\n",
    "    \n",
    "    tickers_to_process = tickers[:max_tickers] if max_tickers else tickers\n",
    "    total_tickers = len(tickers_to_process)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  COLLECTING DATA FOR YEAR {year}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total tickers to check: {total_tickers}\")\n",
    "    print(f\"Market cap filter: >${MARKET_CAP_THRESHOLD/1e9:.0f}B\")\n",
    "    print(f\"API rate limit: {API_CALLS_PER_MINUTE} calls/minute\")\n",
    "    print(f\"Batch size: {batch_size} tickers\")\n",
    "    print(f\"Progress saves: Every {progress_interval} tickers\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process tickers in batches\n",
    "    for batch_start in range(0, total_tickers, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total_tickers)\n",
    "        batch_tickers = tickers_to_process[batch_start:batch_end]\n",
    "        \n",
    "        # Progress update\n",
    "        if batch_start > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_time = elapsed / batch_start\n",
    "            remaining = (total_tickers - batch_start) * avg_time\n",
    "            \n",
    "            print(f\"\\n[Progress: {batch_start}/{total_tickers} ({batch_start/total_tickers*100:.1f}%)]\")\n",
    "            print(f\"  Time: {elapsed/60:.1f}min elapsed, ~{remaining/60:.1f}min remaining\")\n",
    "            print(f\"  Success: {len(successful_tickers)}, Failed: {len(failed_tickers)}, Skipped (small cap): {len(skipped_tickers)}\")\n",
    "            print(f\"  API calls: {total_api_calls} ({total_api_calls/elapsed*60:.0f}/minute avg)\")\n",
    "        \n",
    "        print(f\"\\n  Processing batch {batch_start//batch_size + 1}: tickers {batch_start+1}-{batch_end}\")\n",
    "        \n",
    "        # Get bulk profiles for the batch (1 API call for up to 50 tickers)\n",
    "        profiles = get_bulk_profiles(batch_tickers)\n",
    "        total_api_calls += 1\n",
    "        \n",
    "        # Process each ticker in the batch\n",
    "        for i, ticker in enumerate(batch_tickers):\n",
    "            profile_data = profiles.get(ticker)\n",
    "            \n",
    "            # Process ticker with pre-fetched profile\n",
    "            ticker_data, error_log, api_calls = process_ticker_year(ticker, year, profile_data=profile_data)\n",
    "            total_api_calls += api_calls\n",
    "            \n",
    "            if ticker_data is not None and len(ticker_data) > 0:\n",
    "                all_data.append(ticker_data)\n",
    "                successful_tickers.append(ticker)\n",
    "                print(\"✓\", end=\"\", flush=True)\n",
    "            elif any(\"Market cap below threshold\" in err for err in error_log.get(\"errors\", [])):\n",
    "                skipped_tickers.append(ticker)\n",
    "                print(\"○\", end=\"\", flush=True)\n",
    "            else:\n",
    "                failed_tickers.append(ticker)\n",
    "                all_errors.append(error_log)\n",
    "                print(\"✗\", end=\"\", flush=True)\n",
    "        \n",
    "        # Save progress periodically\n",
    "        if save_progress and (batch_end % progress_interval == 0 or batch_end == total_tickers) and all_data:\n",
    "            temp_df = pd.concat(all_data, ignore_index=True)\n",
    "            temp_df['mkt_cap_rank'] = temp_df.groupby('quarter')['mkt_cap'].rank(method='dense', ascending=False).astype(int)\n",
    "            progress_filename = f\"progress_{year}_tickers_{batch_end}.csv\"\n",
    "            temp_df.to_csv(progress_filename, index=False)\n",
    "            print(f\"\\n  💾 Progress saved: {progress_filename} ({len(temp_df)} rows)\")\n",
    "    \n",
    "    # Final summary\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n\\n{'='*70}\")\n",
    "    print(f\"  YEAR {year} COLLECTION COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
    "    print(f\"Successful: {len(successful_tickers)} tickers\")\n",
    "    print(f\"Failed: {len(failed_tickers)} tickers\")\n",
    "    print(f\"Skipped (small cap): {len(skipped_tickers)} tickers\")\n",
    "    print(f\"Total API calls: {total_api_calls:,} ({total_api_calls/total_time*60:.0f}/minute avg)\")\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        print(\"\\n⚠️  No data collected!\")\n",
    "        return pd.DataFrame(columns=[\"quarter\", \"ticker\", \"industry\", \"sector\", \n",
    "                                    \"debt_to_assets\", \"mkt_cap\", \"stock_price\", \n",
    "                                    \"book_to_market\", \"earnings_yield\", \"mkt_cap_rank\"]), all_errors\n",
    "    \n",
    "    # Combine all data\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Final deduplication - ensure only one entry per ticker-quarter\n",
    "    final_df = final_df.sort_values(['ticker', 'quarter']).drop_duplicates(['ticker', 'quarter'], keep='last')\n",
    "    \n",
    "    # Add market cap ranking\n",
    "    final_df['mkt_cap_rank'] = final_df.groupby('quarter')['mkt_cap'].rank(method='dense', ascending=False).astype(int)\n",
    "    \n",
    "    # Sort by ticker and quarter\n",
    "    final_df = final_df.sort_values(['ticker', 'quarter']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n📊 Final dataset: {len(final_df)} rows, {final_df['ticker'].nunique()} tickers\")\n",
    "    print(f\"   Quarters: {sorted(final_df['quarter'].unique())}\")\n",
    "    \n",
    "    # Verify quarter coverage\n",
    "    expected_quarters = {f\"{year}Q1\", f\"{year}Q2\", f\"{year}Q3\", f\"{year}Q4\"}\n",
    "    actual_quarters = set(final_df['quarter'].astype(str).unique())\n",
    "    missing_quarters = expected_quarters - actual_quarters\n",
    "    if missing_quarters:\n",
    "        print(f\"   ⚠️  Missing quarters: {sorted(missing_quarters)}\")\n",
    "    \n",
    "    # Save error log\n",
    "    if all_errors:\n",
    "        error_filename = f\"errors_{year}.json\"\n",
    "        with open(error_filename, 'w') as f:\n",
    "            json.dump(all_errors, f, indent=2, default=str)\n",
    "        print(f\"\\n📝 Error log saved: {error_filename} ({len(all_errors)} errors)\")\n",
    "    \n",
    "    # Clean up progress files\n",
    "    if save_progress:\n",
    "        for progress_file in [f for f in os.listdir('.') if f.startswith(f'progress_{year}_')]:\n",
    "            os.remove(progress_file)\n",
    "        print(f\"🧹 Cleaned up progress files\")\n",
    "    \n",
    "    return final_df, all_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_tickers(year: int) -> List[str]:\n",
    "    \"\"\"Get list of US tickers that existed in a specific year\"\"\"\n",
    "    print(f\"Fetching ticker list for year {year}...\")\n",
    "    \n",
    "    # Try to get historical ticker list from end of previous year\n",
    "    date = f\"{year-1}-12-31\"\n",
    "    \n",
    "    # First try to get available stocks for that date\n",
    "    available_stocks = get_json(\n",
    "        f\"https://financialmodelingprep.com/api/v3/available-traded/list\",\n",
    "        {\"date\": date}\n",
    "    )\n",
    "    \n",
    "    if available_stocks:\n",
    "        # Filter for US exchanges\n",
    "        us_tickers = [\n",
    "            stock[\"symbol\"] for stock in available_stocks \n",
    "            if stock.get(\"exchangeShortName\") in [\"NYSE\", \"NASDAQ\", \"AMEX\"]\n",
    "            and len(stock[\"symbol\"]) <= 5\n",
    "            and \".\" not in stock[\"symbol\"]\n",
    "        ]\n",
    "        print(f\"✅ Found {len(us_tickers)} US tickers for {year}\")\n",
    "        return us_tickers\n",
    "    \n",
    "    # Fallback: use current ticker list with a warning\n",
    "    print(f\"⚠️  Could not get historical ticker list for {year}, using current list\")\n",
    "    tickers_data = get_json(\"https://financialmodelingprep.com/api/v3/stock/list\")\n",
    "    \n",
    "    if tickers_data:\n",
    "        # Filter for US exchanges and remove penny stocks\n",
    "        us_tickers = [\n",
    "            d[\"symbol\"] for d in tickers_data \n",
    "            if d[\"exchangeShortName\"] in [\"NYSE\", \"NASDAQ\"] \n",
    "            and (d.get(\"price\") is not None and d.get(\"price\", 0) > 5)\n",
    "            and len(d[\"symbol\"]) <= 5\n",
    "            and \".\" not in d[\"symbol\"]\n",
    "        ]\n",
    "        \n",
    "        print(f\"✅ Found {len(us_tickers)} current US tickers\")\n",
    "        return us_tickers\n",
    "    else:\n",
    "        print(\"❌ Failed to fetch ticker list. Using sample tickers.\")\n",
    "        return [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"META\", \"NVDA\", \"JPM\", \"JNJ\", \"V\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get List of US Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Ticker lists will be fetched for each specific year during collection\n",
      "   This ensures we only process stocks that existed in that year\n",
      "\n",
      "💡 The new approach:\n",
      "   1. Fetches historical ticker list for each year\n",
      "   2. Uses batch processing to reduce API calls\n",
      "   3. Removes redundant sleep delays\n",
      "\n",
      "⚡ Expected performance improvements:\n",
      "   - ~30% faster with batch profile fetching\n",
      "   - More accurate historical data with year-specific tickers\n",
      "   - Better API rate limit utilization\n"
     ]
    }
   ],
   "source": [
    "# Note: We'll fetch year-specific ticker lists when collecting data for each year\n",
    "print(\"📌 Ticker lists will be fetched for each specific year during collection\")\n",
    "print(\"   This ensures we only process stocks that existed in that year\")\n",
    "print(\"\\n💡 The new approach:\")\n",
    "print(\"   1. Fetches historical ticker list for each year\")\n",
    "print(\"   2. Uses batch processing to reduce API calls\")\n",
    "print(\"   3. Removes redundant sleep delays\")\n",
    "print(\"\\n⚡ Expected performance improvements:\")\n",
    "print(\"   - ~30% faster with batch profile fetching\")\n",
    "print(\"   - More accurate historical data with year-specific tickers\")\n",
    "print(\"   - Better API rate limit utilization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Test with Single Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with AAPL for year 2023...\n",
      "\n",
      "Test completed in 1.69 seconds with 5 API calls\n",
      "\n",
      "✅ Test successful!\n",
      "  quarter ticker              industry      sector  debt_to_assets  \\\n",
      "0  2023Q2   AAPL  Consumer Electronics  Technology        0.330007   \n",
      "1  2023Q3   AAPL  Consumer Electronics  Technology        0.351492   \n",
      "2  2023Q4   AAPL  Consumer Electronics  Technology        0.305617   \n",
      "\n",
      "         mkt_cap  stock_price  book_to_market  earnings_yield  \n",
      "0  3044866187580       192.05        0.020501        0.007967  \n",
      "1  2670779095140       169.74        0.023470        0.008660  \n",
      "2  2986094670390       191.13        0.024997        0.011458  \n",
      "\n",
      "Quarters found: [Period('2023Q2', 'Q-DEC'), Period('2023Q3', 'Q-DEC'), Period('2023Q4', 'Q-DEC')]\n",
      "\n",
      "Sample metrics:\n",
      "  Book-to-Market: 0.023\n",
      "  Earnings Yield: 0.009\n"
     ]
    }
   ],
   "source": [
    "# Test with AAPL for 2023\n",
    "print(\"Testing with AAPL for year 2023...\")\n",
    "test_start = time.time()\n",
    "\n",
    "test_data, test_errors, test_api_calls = process_ticker_year(\"AAPL\", 2023)\n",
    "\n",
    "test_time = time.time() - test_start\n",
    "print(f\"\\nTest completed in {test_time:.2f} seconds with {test_api_calls} API calls\")\n",
    "\n",
    "if test_data is not None:\n",
    "    print(\"\\n✅ Test successful!\")\n",
    "    print(test_data)\n",
    "    print(f\"\\nQuarters found: {sorted(test_data['quarter'].unique())}\")\n",
    "    print(f\"\\nSample metrics:\")\n",
    "    print(f\"  Book-to-Market: {test_data['book_to_market'].mean():.3f}\")\n",
    "    print(f\"  Earnings Yield: {test_data['earnings_yield'].mean():.3f}\")\n",
    "else:\n",
    "    print(\"\\n❌ Test failed!\")\n",
    "    print(\"Errors:\", test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Collect Data for 2024\n",
    "\n",
    "Collect data for US stocks with market cap > $1B for 2024. This will take approximately **70 minutes** with filtering.\n",
    "\n",
    "**Note:** Since 2024 is ongoing, you may have partial data (Q1-Q3 or Q1-Q4 depending on current date).\n",
    "\n",
    "**Time estimate:** ~21,000 API calls @ 300/minute = 70 minutes\n",
    "\n",
    "To test with fewer tickers first, change `MAX_TICKERS = None` to `MAX_TICKERS = 100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2024 data\n",
    "YEAR = 2024\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2024\n",
    "\n",
    "# Get historical ticker list for 2024\n",
    "us_tickers_2024 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2024, errors_2024 = collect_year_data(us_tickers_2024, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2024) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2024.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")\n",
    "    \n",
    "    # Show summary statistics\n",
    "    print(f\"\\n📈 Summary Statistics:\")\n",
    "    print(f\"   Debt/Assets - Mean: {data_2024['debt_to_assets'].mean():.3f}, Median: {data_2024['debt_to_assets'].median():.3f}\")\n",
    "    print(f\"   Book/Market - Mean: {data_2024['book_to_market'].mean():.3f}, Median: {data_2024['book_to_market'].median():.3f}\")\n",
    "    print(f\"   Earnings Yield - Mean: {data_2024['earnings_yield'].mean():.3f}, Median: {data_2024['earnings_yield'].median():.3f}\")\n",
    "    \n",
    "    # Show top companies (use latest quarter available)\n",
    "    latest_quarter = data_2024['quarter'].max()\n",
    "    latest_data = data_2024[data_2024['quarter'] == latest_quarter]\n",
    "    if len(latest_data) > 0:\n",
    "        print(f\"\\n🏆 Top 10 companies by market cap ({latest_quarter}):\")\n",
    "        top_10 = latest_data.nsmallest(10, 'mkt_cap_rank')[['ticker', 'mkt_cap_rank', 'mkt_cap', 'book_to_market', 'earnings_yield', 'industry']]\n",
    "        top_10['mkt_cap'] = top_10['mkt_cap'].apply(lambda x: f\"${x/1e9:.1f}B\")\n",
    "        print(top_10.to_string(index=False))\n",
    "    \n",
    "    # Show available quarters\n",
    "    print(f\"\\n📅 Available quarters for 2024: {sorted(data_2024['quarter'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Collect Data for 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2023 data\n",
    "YEAR = 2023\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2023\n",
    "\n",
    "# Get historical ticker list for 2023\n",
    "us_tickers_2023 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2023, errors_2023 = collect_year_data(us_tickers_2023, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2023) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2023.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")\n",
    "    \n",
    "    # Show summary statistics\n",
    "    print(f\"\\n📈 Summary Statistics:\")\n",
    "    print(f\"   Debt/Assets - Mean: {data_2023['debt_to_assets'].mean():.3f}, Median: {data_2023['debt_to_assets'].median():.3f}\")\n",
    "    print(f\"   Book/Market - Mean: {data_2023['book_to_market'].mean():.3f}, Median: {data_2023['book_to_market'].median():.3f}\")\n",
    "    print(f\"   Earnings Yield - Mean: {data_2023['earnings_yield'].mean():.3f}, Median: {data_2023['earnings_yield'].median():.3f}\")\n",
    "    \n",
    "    # Show top companies\n",
    "    q4_data = data_2023[data_2023['quarter'] == f'{YEAR}Q4']\n",
    "    if len(q4_data) > 0:\n",
    "        print(f\"\\n🏆 Top 10 companies by market cap (Q4 {YEAR}):\")\n",
    "        top_10 = q4_data.nsmallest(10, 'mkt_cap_rank')[['ticker', 'mkt_cap_rank', 'mkt_cap', 'book_to_market', 'earnings_yield', 'industry']]\n",
    "        top_10['mkt_cap'] = top_10['mkt_cap'].apply(lambda x: f\"${x/1e9:.1f}B\")\n",
    "        print(top_10.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Template for Remaining Years\n",
    "\n",
    "Use this template for years 2022 and earlier:\n",
    "\n",
    "```python\n",
    "# Collect YEAR data\n",
    "YEAR = 20XX  # Change this\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in YEAR\n",
    "\n",
    "# Get historical ticker list for YEAR\n",
    "us_tickers_YEAR = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_YEAR, errors_YEAR = collect_year_data(us_tickers_YEAR, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_YEAR) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_YEAR.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Collect Data for 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2022 data\n",
    "YEAR = 2022\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2022\n",
    "\n",
    "# Get historical ticker list for 2022\n",
    "us_tickers_2022 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2022, errors_2022 = collect_year_data(us_tickers_2022, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2022) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2022.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Collect Data for 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2021 data\n",
    "YEAR = 2021\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2021\n",
    "\n",
    "# Get historical ticker list for 2021\n",
    "us_tickers_2021 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2021, errors_2021 = collect_year_data(us_tickers_2021, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2021) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2021.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Collect Data for 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2020 data\n",
    "YEAR = 2020\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2020\n",
    "\n",
    "# Get historical ticker list for 2020\n",
    "us_tickers_2020 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2020, errors_2020 = collect_year_data(us_tickers_2020, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2020) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2020.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Collect Data for 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2019 data\n",
    "YEAR = 2019\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2019\n",
    "\n",
    "# Get historical ticker list for 2019\n",
    "us_tickers_2019 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2019, errors_2019 = collect_year_data(us_tickers_2019, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2019) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2019.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2019.csv errors_2019.json\n",
    "!git commit -m 'Add data for 2019'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Collect Data for 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2018 data\n",
    "YEAR = 2018\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2018\n",
    "\n",
    "# Get historical ticker list for 2018\n",
    "us_tickers_2018 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2018, errors_2018 = collect_year_data(us_tickers_2018, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2018) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2018.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2018.csv errors_2018.json\n",
    "!git commit -m 'Add data for 2018'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Collect Data for 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2017 data\n",
    "YEAR = 2017\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2017\n",
    "\n",
    "# Get historical ticker list for 2017\n",
    "us_tickers_2017 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2017, errors_2017 = collect_year_data(us_tickers_2017, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2017) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2017.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2017.csv errors_2017.json\n",
    "!git commit -m 'Add data for 2017'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Collect Data for 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2016 data\n",
    "YEAR = 2016\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2016\n",
    "\n",
    "# Get historical ticker list for 2016\n",
    "us_tickers_2016 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2016, errors_2016 = collect_year_data(us_tickers_2016, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2016) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2016.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2016.csv errors_2016.json\n",
    "!git commit -m 'Add data for 2016'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Collect Data for 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2015 data\n",
    "YEAR = 2015\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2015\n",
    "\n",
    "# Get historical ticker list for 2015\n",
    "us_tickers_2015 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2015, errors_2015 = collect_year_data(us_tickers_2015, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2015) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2015.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2015.csv errors_2015.json\n",
    "!git commit -m 'Add data for 2015'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Collect Data for 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2014 data\n",
    "YEAR = 2014\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2014\n",
    "\n",
    "# Get historical ticker list for 2014\n",
    "us_tickers_2014 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2014, errors_2014 = collect_year_data(us_tickers_2014, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2014) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2014.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2014.csv errors_2014.json\n",
    "!git commit -m 'Add data for 2014'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Collect Data for 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2013 data\n",
    "YEAR = 2013\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2013\n",
    "\n",
    "# Get historical ticker list for 2013\n",
    "us_tickers_2013 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2013, errors_2013 = collect_year_data(us_tickers_2013, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2013) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2013.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2013.csv errors_2013.json\n",
    "!git commit -m 'Add data for 2013'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Collect Data for 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2012 data\n",
    "YEAR = 2012\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2012\n",
    "\n",
    "# Get historical ticker list for 2012\n",
    "us_tickers_2012 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2012, errors_2012 = collect_year_data(us_tickers_2012, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2012) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2012.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2012.csv errors_2012.json\n",
    "!git commit -m 'Add data for 2012'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Collect Data for 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2011 data\n",
    "YEAR = 2011\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2011\n",
    "\n",
    "# Get historical ticker list for 2011\n",
    "us_tickers_2011 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2011, errors_2011 = collect_year_data(us_tickers_2011, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2011) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2011.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2011.csv errors_2011.json\n",
    "!git commit -m 'Add data for 2011'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Collect Data for 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2010 data\n",
    "YEAR = 2010\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2010\n",
    "\n",
    "# Get historical ticker list for 2010\n",
    "us_tickers_2010 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2010, errors_2010 = collect_year_data(us_tickers_2010, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2010) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2010.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2010.csv errors_2010.json\n",
    "!git commit -m 'Add data for 2010'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18: Collect Data for 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2009 data\n",
    "YEAR = 2009\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2009\n",
    "\n",
    "# Get historical ticker list for 2009\n",
    "us_tickers_2009 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2009, errors_2009 = collect_year_data(us_tickers_2009, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2009) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2009.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2009.csv errors_2009.json\n",
    "!git commit -m 'Add data for 2009'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 19: Collect Data for 2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2008 data\n",
    "YEAR = 2008\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2008\n",
    "\n",
    "# Get historical ticker list for 2008\n",
    "us_tickers_2008 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2008, errors_2008 = collect_year_data(us_tickers_2008, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2008) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2008.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2008.csv errors_2008.json\n",
    "!git commit -m 'Add data for 2008'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 20: Collect Data for 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2007 data\n",
    "YEAR = 2007\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2007\n",
    "\n",
    "# Get historical ticker list for 2007\n",
    "us_tickers_2007 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2007, errors_2007 = collect_year_data(us_tickers_2007, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2007) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2007.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2007.csv errors_2007.json\n",
    "!git commit -m 'Add data for 2007'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 21: Collect Data for 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2006 data\n",
    "YEAR = 2006\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2006\n",
    "\n",
    "# Get historical ticker list for 2006\n",
    "us_tickers_2006 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2006, errors_2006 = collect_year_data(us_tickers_2006, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2006) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2006.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2006.csv errors_2006.json\n",
    "!git commit -m 'Add data for 2006'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 22: Collect Data for 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2005 data\n",
    "YEAR = 2005\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2005\n",
    "\n",
    "# Get historical ticker list for 2005\n",
    "us_tickers_2005 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2005, errors_2005 = collect_year_data(us_tickers_2005, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2005) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2005.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2005.csv errors_2005.json\n",
    "!git commit -m 'Add data for 2005'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 23: Collect Data for 2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2004 data\n",
    "YEAR = 2004\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2004\n",
    "\n",
    "# Get historical ticker list for 2004\n",
    "us_tickers_2004 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2004, errors_2004 = collect_year_data(us_tickers_2004, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2004) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2004.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2004.csv errors_2004.json\n",
    "!git commit -m 'Add data for 2004'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 24: Collect Data for 2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2003 data\n",
    "YEAR = 2003\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2003\n",
    "\n",
    "# Get historical ticker list for 2003\n",
    "us_tickers_2003 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2003, errors_2003 = collect_year_data(us_tickers_2003, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2003) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2003.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2003.csv errors_2003.json\n",
    "!git commit -m 'Add data for 2003'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 25: Collect Data for 2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2002 data\n",
    "YEAR = 2002\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2002\n",
    "\n",
    "# Get historical ticker list for 2002\n",
    "us_tickers_2002 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2002, errors_2002 = collect_year_data(us_tickers_2002, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2002) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2002.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2002.csv errors_2002.json\n",
    "!git commit -m 'Add data for 2002'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 26: Collect Data for 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2001 data\n",
    "YEAR = 2001\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2001\n",
    "\n",
    "# Get historical ticker list for 2001\n",
    "us_tickers_2001 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2001, errors_2001 = collect_year_data(us_tickers_2001, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2001) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2001.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2001.csv errors_2001.json\n",
    "!git commit -m 'Add data for 2001'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 27: Collect Data for 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2000 data\n",
    "YEAR = 2000\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2000\n",
    "\n",
    "# Get historical ticker list for 2000\n",
    "us_tickers_2000 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2000, errors_2000 = collect_year_data(us_tickers_2000, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2000) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2000.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2000.csv errors_2000.json\n",
    "!git commit -m 'Add data for 2000'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 28: Collect Data for 1999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 1999 data\n",
    "YEAR = 1999\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 1999\n",
    "\n",
    "# Get historical ticker list for 1999\n",
    "us_tickers_1999 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_1999, errors_1999 = collect_year_data(us_tickers_1999, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_1999) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_1999.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_1999.csv errors_1999.json\n",
    "!git commit -m 'Add data for 1999'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 29: Collect Data for 1998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 1998 data\n",
    "YEAR = 1998\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 1998\n",
    "\n",
    "# Get historical ticker list for 1998\n",
    "us_tickers_1998 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_1998, errors_1998 = collect_year_data(us_tickers_1998, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_1998) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_1998.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_1998.csv errors_1998.json\n",
    "!git commit -m 'Add data for 1998'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 30: Collect Data for 1997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 1997 data\n",
    "YEAR = 1997\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 1997\n",
    "\n",
    "# Get historical ticker list for 1997\n",
    "us_tickers_1997 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_1997, errors_1997 = collect_year_data(us_tickers_1997, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_1997) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_1997.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_1997.csv errors_1997.json\n",
    "!git commit -m 'Add data for 1997'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 31: Collect Data for 1996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 1996 data\n",
    "YEAR = 1996\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 1996\n",
    "\n",
    "# Get historical ticker list for 1996\n",
    "us_tickers_1996 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_1996, errors_1996 = collect_year_data(us_tickers_1996, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_1996) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_1996.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_1996.csv errors_1996.json\n",
    "!git commit -m 'Add data for 1996'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Review Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review what we've collected\n",
    "import glob\n",
    "\n",
    "print(\"📁 Available data files:\")\n",
    "data_files = sorted(glob.glob(\"stock_data_*.csv\"))\n",
    "\n",
    "total_rows = 0\n",
    "for file in data_files:\n",
    "    df = pd.read_csv(file)\n",
    "    total_rows += len(df)\n",
    "    print(f\"  {file}: {len(df):,} rows, {df['ticker'].nunique()} tickers\")\n",
    "    # Check for duplicates\n",
    "    duplicates = df.groupby(['ticker', 'quarter']).size()\n",
    "    if (duplicates > 1).any():\n",
    "        print(f\"    ⚠️  Found {(duplicates > 1).sum()} duplicate ticker-quarter combinations\")\n",
    "\n",
    "print(f\"\\n📊 Total: {total_rows:,} rows across {len(data_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors\n",
    "error_files = sorted(glob.glob(\"errors_*.json\"))\n",
    "\n",
    "if error_files:\n",
    "    print(\"📝 Error analysis:\")\n",
    "    \n",
    "    for error_file in error_files:\n",
    "        with open(error_file, 'r') as f:\n",
    "            errors = json.load(f)\n",
    "        \n",
    "        # Count error types\n",
    "        error_types = {}\n",
    "        for error in errors:\n",
    "            for err_msg in error.get('errors', []):\n",
    "                err_type = err_msg.split(':')[0] if ':' in err_msg else err_msg\n",
    "                error_types[err_type] = error_types.get(err_type, 0) + 1\n",
    "        \n",
    "        print(f\"\\n{error_file}: {len(errors)} failed tickers\")\n",
    "        for err_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "            print(f\"  - {err_type}: {count}\")\n",
    "else:\n",
    "    print(\"No error files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Combine All Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all years into one file\n",
    "years = [2019, 2020, 2021, 2022, 2023, 2024]\n",
    "all_data = []\n",
    "\n",
    "for year in years:\n",
    "    filename = f\"stock_data_{year}.csv\"\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df['quarter'] = pd.PeriodIndex(df['quarter'], freq='Q')\n",
    "        all_data.append(df)\n",
    "        print(f\"✓ Loaded {year}: {len(df)} rows\")\n",
    "    else:\n",
    "        print(f\"✗ {filename} not found\")\n",
    "\n",
    "if all_data:\n",
    "    combined = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Final deduplication across all years\n",
    "    combined = combined.sort_values(['ticker', 'quarter']).drop_duplicates(['ticker', 'quarter'], keep='last')\n",
    "    \n",
    "    # Recalculate rankings\n",
    "    combined['mkt_cap_rank'] = combined.groupby('quarter')['mkt_cap'].rank(method='dense', ascending=False).astype(int)\n",
    "    \n",
    "    combined.to_csv(\"stock_data_combined_6years.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\n✅ Combined dataset saved!\")\n",
    "    print(f\"   Total: {len(combined):,} rows\")\n",
    "    print(f\"   Tickers: {combined['ticker'].nunique()}\")\n",
    "    print(f\"   Period: {combined['quarter'].min()} to {combined['quarter'].max()}\")\n",
    "    \n",
    "    # Show metric distributions\n",
    "    print(f\"\\n📊 Metric distributions:\")\n",
    "    print(f\"   Debt/Assets: {combined['debt_to_assets'].describe()[[\"mean\", \"50%\", \"std\"]].round(3).to_dict()}\")\n",
    "    print(f\"   Book/Market: {combined['book_to_market'].describe()[[\"mean\", \"50%\", \"std\"]].round(3).to_dict()}\")\n",
    "    print(f\"   Earnings Yield: {combined['earnings_yield'].describe()[[\"mean\", \"50%\", \"std\"]].round(3).to_dict()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
