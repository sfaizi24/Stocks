{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate-Limited Market Cap Filtered Data Collection (Year by Year)\n",
    "\n",
    "This notebook collects financial data for **US stocks with market cap > $1B** with strict rate limiting and saves each year separately.\n",
    "\n",
    "**Key features:**\n",
    "1. **Market cap filter**: Only collects data for stocks with market cap > $1B\n",
    "2. **Rate limiting**: 750 API calls per minute\n",
    "3. **Year-by-year collection**: Each year saved to separate CSV\n",
    "4. **Deduplication**: Ensures only one entry per quarter (Q1-Q4)\n",
    "5. **Enhanced metrics**: Includes book-to-market and earnings yield\n",
    "6. **Error tracking**: Detailed logs for debugging\n",
    "\n",
    "**Target columns:** `quarter`, `ticker`, `industry`, `sector`, `debt_to_assets`, `mkt_cap`, `stock_price`, `book_to_market`, `earnings_yield`, `mkt_cap_rank`\n",
    "\n",
    "**Time estimate:** ~70 minutes per year (vs 2.7 hours without filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš ï¸ MARKET CAP FILTERED COLLECTION\n",
    "\n",
    "**This notebook filters for stocks with market cap > $1B**\n",
    "\n",
    "**Time requirements:**\n",
    "- **Per year:** ~70 minutes (with market cap filtering)\n",
    "- **All 6 years (2019-2024):** ~7 hours total\n",
    "- **API calls:** ~21,000 per year (vs 48,000 without filtering)\n",
    "- **Expected companies:** ~1,800 per year (15-20% of all tickers)\n",
    "\n",
    "**To start with a smaller test:**\n",
    "1. Change `MAX_TICKERS = None` to `MAX_TICKERS = 100` in any collection cell\n",
    "2. Run one year first to verify everything works\n",
    "3. Then change back to `MAX_TICKERS = None` for full collection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "import threading\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Your API key\n",
    "API = \"7cNMpVzb43GKtm05iRTDWJtyJXSylX8J\"\n",
    "\n",
    "# Rate limiting configuration\n",
    "API_CALLS_PER_MINUTE = 750\n",
    "SECONDS_PER_CALL = 60 / API_CALLS_PER_MINUTE  # 0.08 seconds per call\n",
    "\n",
    "# Session and timer for rate limiting\n",
    "session = requests.Session()\n",
    "rate_limit_lock = threading.Lock()\n",
    "LAST_API_CALL = 0.0\n",
    "\n",
    "# Market cap threshold (1 billion)\n",
    "MARKET_CAP_THRESHOLD = 1e9\n",
    "\n",
    "print(f\"Rate limit configured: {API_CALLS_PER_MINUTE} calls/minute ({SECONDS_PER_CALL:.2f} seconds/call)\")\n",
    "print(f\"Market cap filter: > ${MARKET_CAP_THRESHOLD/1e9:.0f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions with Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(url: str, params: Dict[str, Any] = {}) -> Optional[Any]:\n",
    "    \"\"\"Safely get JSON data from API with error handling and rate limit retry\"\"\"\n",
    "    global LAST_API_CALL, session\n",
    "    try:\n",
    "        params['apikey'] = API\n",
    "        with rate_limit_lock:\n",
    "            elapsed = time.time() - LAST_API_CALL\n",
    "            if elapsed < SECONDS_PER_CALL:\n",
    "                time.sleep(SECONDS_PER_CALL - elapsed)\n",
    "            LAST_API_CALL = time.time()\n",
    "        response = session.get(url, params=params, timeout=10)\n",
    "        if response.status_code == 429:\n",
    "            print('âš ï¸  Rate limit hit! Waiting 30 seconds...')\n",
    "            time.sleep(30)\n",
    "            return get_json(url, params)\n",
    "        response.raise_for_status()\n",
    "        js = response.json()\n",
    "        if isinstance(js, dict) and 'historical' in js:\n",
    "            return js['historical']\n",
    "        elif isinstance(js, list):\n",
    "            return js\n",
    "        else:\n",
    "            return js\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f'HTTP Error {e.response.status_code}: {e}')\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f'Error fetching data: {e}')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_market_cap(ticker: str, year: int, precomputed: Optional[float] = None) -> Tuple[bool, Optional[float]]:\n",
    "    \"\"\"Check if ticker had market cap above threshold in given year\"\"\"\n",
    "    if precomputed is not None:\n",
    "        return precomputed > MARKET_CAP_THRESHOLD, precomputed\n",
    "    try:\n",
    "        start_date = f'{year}-01-01'\n",
    "        end_date = f'{year}-12-31'\n",
    "        mc_data = get_json(\n",
    "            f'https://financialmodelingprep.com/api/v3/historical-market-capitalization/{ticker}',\n",
    "            {'from': start_date, 'to': end_date}\n",
    "        )\n",
    "        if not mc_data:\n",
    "            return False, None\n",
    "        mc_df = pd.DataFrame(mc_data)\n",
    "        avg_market_cap = mc_df['marketCap'].mean()\n",
    "        return avg_market_cap > MARKET_CAP_THRESHOLD, avg_market_cap\n",
    "    except Exception as e:\n",
    "        print(f'Error checking market cap for {ticker}: {e}')\n",
    "        return False, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bulk_profiles(tickers: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Fetch company profiles in bulk.\"\"\"\n",
    "    data = get_json(f'https://financialmodelingprep.com/api/v3/profile/{','.join(tickers)}')\n",
    "    profiles = {}\n",
    "    if isinstance(data, list):\n",
    "        for item in data:\n",
    "            symbol = item.get('symbol')\n",
    "            profiles[symbol] = item\n",
    "    return profiles\n",
    "\n",
    "def get_bulk_market_caps(tickers: List[str], year: int) -> Dict[str, float]:\n",
    "    \"\"\"Fetch average market cap for a list of tickers.\"\"\"\n",
    "    start_date = f'{year}-01-01'\n",
    "    end_date = f'{year}-12-31'\n",
    "    data = get_json(\n",
    "        f'https://financialmodelingprep.com/api/v3/historical-market-capitalization/{','.join(tickers)}',\n",
    "        {'from': start_date, 'to': end_date}\n",
    "    )\n",
    "    caps = {}\n",
    "    if isinstance(data, dict):\n",
    "        for symbol, hist in data.items():\n",
    "            df = pd.DataFrame(hist)\n",
    "            caps[symbol] = df['marketCap'].mean()\n",
    "    return caps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ticker_year(ticker: str, year: int, profile_data: Optional[Dict[str, Any]] = None, avg_market_cap: Optional[float] = None) -> Tuple[Optional[pd.DataFrame], Dict[str, Any], int]:\n",
    "    \"\"\"Process data for a single ticker for a specific year\"\"\"\n",
    "    error_log = {'ticker': ticker, 'year': year, 'errors': []}\n",
    "    api_calls = 0\n",
    "    try:\n",
    "        is_large_cap, avg_market_cap = check_market_cap(ticker, year, precomputed=avg_market_cap)\n",
    "        if avg_market_cap is None:\n",
    "            api_calls += 1\n",
    "        if not is_large_cap:\n",
    "            error_log['errors'].append(f'Market cap below threshold (avg: ${avg_market_cap:,.0f})')\n",
    "            return None, error_log, api_calls\n",
    "        start_date = datetime(year, 1, 1)\n",
    "        end_date = datetime(year, 12, 31)\n",
    "        bs = get_json(f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}', {'period': 'quarter', 'limit': 20})\n",
    "        api_calls += 1\n",
    "        inc = get_json(f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}', {'period': 'quarter', 'limit': 20})\n",
    "        api_calls += 1\n",
    "        mc = get_json(f'https://financialmodelingprep.com/api/v3/historical-market-capitalization/{ticker}', {'from': start_date.strftime('%Y-%m-%d'), 'to': end_date.strftime('%Y-%m-%d')})\n",
    "        api_calls += 1\n",
    "        px = get_json(f'https://financialmodelingprep.com/api/v3/historical-price-full/{ticker}', {'from': start_date.strftime('%Y-%m-%d'), 'to': end_date.strftime('%Y-%m-%d')})\n",
    "        api_calls += 1\n",
    "        if profile_data is None:\n",
    "            profile = get_json(f'https://financialmodelingprep.com/api/v3/profile/{ticker}')\n",
    "            api_calls += 1\n",
    "        else:\n",
    "            profile = profile_data\n",
    "        if not all([bs, inc, mc, px, profile]):\n",
    "            if not bs: error_log['errors'].append('No balance sheet data')\n",
    "            if not inc: error_log['errors'].append('No income statement data')\n",
    "            if not mc: error_log['errors'].append('No market cap data')\n",
    "            if not px: error_log['errors'].append('No price data')\n",
    "            if not profile: error_log['errors'].append('No profile data')\n",
    "            return None, error_log, api_calls\n",
    "        industry = profile[0].get('industry', 'Unknown')\n",
    "        sector = profile[0].get('sector', 'Unknown')\n",
    "        bs_df = pd.DataFrame(bs)\n",
    "        bs_df['date'] = pd.to_datetime(bs_df['date'])\n",
    "        bs_df = bs_df[bs_df['date'].dt.year == year]\n",
    "        if len(bs_df) == 0:\n",
    "            error_log['errors'].append(f'No balance sheet data for year {year}')\n",
    "            return None, error_log, api_calls\n",
    "        bs_df = (bs_df[['date', 'shortTermDebt', 'longTermDebt', 'totalAssets', 'totalStockholdersEquity', 'commonStock']]\n",
    "            .assign(quarter=lambda d: d.date.dt.to_period('Q'),\n",
    "                    debt_to_assets=lambda d: ((d.shortTermDebt.fillna(0) + d.longTermDebt.fillna(0)) / d.totalAssets.replace(0, pd.NA)),\n",
    "                    book_value=lambda d: d.totalStockholdersEquity)\n",
    "            .dropna(subset=['debt_to_assets'])\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('quarter', keep='last'))\n",
    "        inc_df = pd.DataFrame(inc)\n",
    "        inc_df['date'] = pd.to_datetime(inc_df['date'])\n",
    "        inc_df = inc_df[inc_df['date'].dt.year == year]\n",
    "        inc_df = (inc_df[['date', 'eps', 'weightedAverageShsOut']]\n",
    "            .assign(quarter=lambda d: d.date.dt.to_period('Q'))\n",
    "            .rename(columns={'weightedAverageShsOut': 'shares_outstanding'})\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('quarter', keep='last'))\n",
    "        mc_df = (pd.DataFrame(mc)\n",
    "            .assign(date=lambda d: pd.to_datetime(d.date), quarter=lambda d: d.date.dt.to_period('Q'))\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('quarter', keep='last')\n",
    "            .rename(columns={'marketCap': 'mkt_cap'})[['quarter', 'mkt_cap']])\n",
    "        px_df = (pd.DataFrame(px)\n",
    "            .assign(date=lambda d: pd.to_datetime(d.date), quarter=lambda d: d.date.dt.to_period('Q'))\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('quarter', keep='last')\n",
    "            .rename(columns={'adjClose': 'stock_price'})[['quarter', 'stock_price']])\n",
    "        merged = (bs_df.merge(inc_df, on='quarter', how='left')\n",
    "                     .merge(mc_df, on='quarter', how='left')\n",
    "                     .merge(px_df, on='quarter', how='left'))\n",
    "        merged = merged.assign(ticker=ticker, industry=industry, sector=sector,\n",
    "                               book_to_market=lambda d: (d.book_value / d.shares_outstanding) / d.stock_price,\n",
    "                               earnings_yield=lambda d: d.eps / d.stock_price)\n",
    "        merged = merged[['quarter', 'ticker', 'industry', 'sector', 'debt_to_assets', 'mkt_cap', 'stock_price', 'book_to_market', 'earnings_yield']].dropna()\n",
    "        valid_quarters = [f'{year}Q1', f'{year}Q2', f'{year}Q3', f'{year}Q4']\n",
    "        merged = merged[merged['quarter'].astype(str).isin(valid_quarters)]\n",
    "        if len(merged) == 0:\n",
    "            error_log['errors'].append('No valid data after merging')\n",
    "            return None, error_log, api_calls\n",
    "        return merged, error_log, api_calls\n",
    "    except Exception as e:\n",
    "        error_log['errors'].append(f'Exception: {str(e)}')\n",
    "        return None, error_log, api_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def collect_year_data(tickers: List[str], year: int, max_tickers: Optional[int] = None,\n",
    "                     save_progress: bool = True, progress_interval: int = 100,\n",
    "                     workers: int = 5) -> Tuple[pd.DataFrame, List[Dict]]:\n",
    "    \"\"\"Collect data for multiple tickers for a specific year using threads.\"\"\"\n",
    "    all_data = []\n",
    "    all_errors = []\n",
    "    successful_tickers = []\n",
    "    failed_tickers = []\n",
    "    skipped_tickers = []\n",
    "    total_api_calls = 0\n",
    "\n",
    "    tickers_to_process = tickers[:max_tickers] if max_tickers else tickers\n",
    "    total_tickers = len(tickers_to_process)\n",
    "\n",
    "    print(f\"\n",
    "{'='*70}\")\n",
    "    print(f\"  COLLECTING DATA FOR YEAR {year}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total tickers to check: {total_tickers}\")\n",
    "    print(f\"Market cap filter: >${MARKET_CAP_THRESHOLD/1e9:.0f}B\")\n",
    "    print(f\"API rate limit: {API_CALLS_PER_MINUTE} calls/minute\")\n",
    "    print(f\"Progress saves: Every {progress_interval} tickers\")\n",
    "    print(f\"{'='*70}\n",
    "\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    progress_lock = threading.Lock()\n",
    "    profile_cache = {}\n",
    "    market_cap_cache = {}\n",
    "    batch_size = 50\n",
    "    for i in range(0, total_tickers, batch_size):\n",
    "        batch = tickers_to_process[i:i+batch_size]\n",
    "        profile_cache.update(get_bulk_profiles(batch))\n",
    "        market_cap_cache.update(get_bulk_market_caps(batch, year))\n",
    "\n",
    "\n",
    "    def worker(ticker):\n",
    "        nonlocal total_api_calls\n",
    "        data, error_log, api_calls = process_ticker_year(ticker, year, profile_cache.get(ticker), market_cap_cache.get(ticker))\n",
    "        with progress_lock:\n",
    "            total_api_calls += api_calls\n",
    "            if data is not None and len(data) > 0:\n",
    "                all_data.append(data)\n",
    "                successful_tickers.append(ticker)\n",
    "                ch = 'âœ“'\n",
    "            elif any('Market cap below threshold' in err for err in error_log.get('errors', [])):\n",
    "                skipped_tickers.append(ticker)\n",
    "                ch = 'â—‹'\n",
    "            else:\n",
    "                failed_tickers.append(ticker)\n",
    "                all_errors.append(error_log)\n",
    "                ch = 'âœ—'\n",
    "            print(ch, end='', flush=True)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        futures = {executor.submit(worker, t): t for t in tickers_to_process}\n",
    "        for i, f in enumerate(as_completed(futures), 1):\n",
    "            if i % 20 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                avg_time = elapsed / i\n",
    "                remaining = (total_tickers - i) * avg_time\n",
    "                print(f\"\n",
    "[Progress: {i}/{total_tickers} ({i/total_tickers*100:.1f}%)]\")\n",
    "                print(f\"  Time: {elapsed/60:.1f}min elapsed, ~{remaining/60:.1f}min remaining\")\n",
    "                print(f\"  Success: {len(successful_tickers)}, Failed: {len(failed_tickers)}, Skipped (small cap): {len(skipped_tickers)}\")\n",
    "                print(f\"  API calls: {total_api_calls} ({total_api_calls/elapsed*60:.0f}/minute avg)\")\n",
    "                print(f\"  Current batch: \", end='')\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    print(f\"\n",
    "\n",
    "{'='*70}\")\n",
    "    print(f\"  YEAR {year} COLLECTION COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
    "    print(f\"Successful: {len(successful_tickers)} tickers\")\n",
    "    print(f\"Failed: {len(failed_tickers)} tickers\")\n",
    "    print(f\"Skipped (small cap): {len(skipped_tickers)} tickers\")\n",
    "    print(f\"Total API calls: {total_api_calls:,} ({total_api_calls/total_time*60:.0f}/minute avg)\")\n",
    "\n",
    "    if len(all_data) == 0:\n",
    "        print(\"\n",
    "âš ï¸  No data collected!\")\n",
    "        return pd.DataFrame(columns=['quarter','ticker','industry','sector','debt_to_assets','mkt_cap','stock_price','book_to_market','earnings_yield','mkt_cap_rank']), all_errors\n",
    "\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df = final_df.sort_values(['ticker','quarter']).drop_duplicates(['ticker','quarter'], keep='last')\n",
    "    final_df['mkt_cap_rank'] = final_df.groupby('quarter')['mkt_cap'].rank(method='dense', ascending=False).astype(int)\n",
    "    final_df = final_df.sort_values(['ticker','quarter']).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\n",
    "ðŸ“Š Final dataset: {len(final_df)} rows, {final_df['ticker'].nunique()} tickers\")\n",
    "    print(f\"   Quarters: {sorted(final_df['quarter'].unique())}\")\n",
    "\n",
    "    expected_quarters = {f\"{year}Q1\", f\"{year}Q2\", f\"{year}Q3\", f\"{year}Q4\"}\n",
    "    actual_quarters = set(final_df['quarter'].astype(str).unique())\n",
    "    missing_quarters = expected_quarters - actual_quarters\n",
    "    if missing_quarters:\n",
    "        print(f\"   âš ï¸  Missing quarters: {sorted(missing_quarters)}\")\n",
    "\n",
    "    if all_errors:\n",
    "        error_filename = f\"errors_{year}.json\"\n",
    "        with open(error_filename, 'w') as f:\n",
    "            json.dump(all_errors, f, indent=2, default=str)\n",
    "        print(f\"\n",
    "ðŸ“ Error log saved: {error_filename} ({len(all_errors)} errors)\")\n",
    "\n",
    "    if save_progress:\n",
    "        for progress_file in [f for f in os.listdir('.') if f.startswith(f'progress_{year}_')]:\n",
    "            os.remove(progress_file)\n",
    "        print(f\"ðŸ§¹ Cleaned up progress files\")\n",
    "\n",
    "    return final_df, all_errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get List of US Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of US tickers for a specific year\n",
    "\n",
    "def get_tickers_for_year(year: int) -> List[str]:\n",
    "    \"\"\"Fetch tickers as of the end of the previous year using FMP available-traded/list.\"\"\"\n",
    "    date = f\"{year-1}-12-31\"\n",
    "    available_stocks = get_json(\n",
    "        \"https://financialmodelingprep.com/api/v3/available-traded/list\",\n",
    "        {\"date\": date}\n",
    "    ) or []\n",
    "    tickers = [\n",
    "        d[\"symbol\"]\n",
    "        for d in available_stocks\n",
    "        if d.get(\"exchangeShortName\") in [\"NYSE\", \"NASDAQ\"]\n",
    "    ]\n",
    "    if not tickers:\n",
    "        tickers_data = get_json(\"https://financialmodelingprep.com/api/v3/stock/list\") or []\n",
    "        tickers = [\n",
    "            d[\"symbol\"]\n",
    "            for d in tickers_data\n",
    "            if d.get(\"exchangeShortName\") in [\"NYSE\", \"NASDAQ\"]\n",
    "            and (d.get(\"price\") is not None and d.get(\"price\", 0) > 5)\n",
    "            and len(d[\"symbol\"]) <= 5\n",
    "            and '.' not in d[\"symbol\"]\n",
    "        ]\n",
    "    return sorted(set(tickers))\n",
    "\n",
    "us_tickers = get_tickers_for_year(2024)\n",
    "print(f\"âœ… Found {len(us_tickers)} tickers for 2024\")\n",
    "print(f\"   Sample: {us_tickers[:10]}\")\n",
    "\n",
    "estimated_large_cap = int(len(us_tickers) * 0.15)\n",
    "print(\"\n",
    "ðŸ“Š Estimates:\")\n",
    "print(f\"   Expected large cap (>$1B): ~{estimated_large_cap} stocks\")\n",
    "print(f\"   Estimated API calls per year: ~{len(us_tickers) + estimated_large_cap * 5:,}\")\n",
    "print(f\"   Estimated time per year: ~{(len(us_tickers) + estimated_large_cap * 5) / API_CALLS_PER_MINUTE:.0f} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Test with Single Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with AAPL for 2023\n",
    "print(\"Testing with AAPL for year 2023...\")\n",
    "test_start = time.time()\n",
    "\n",
    "test_data, test_errors, test_api_calls = process_ticker_year(\"AAPL\", 2023)\n",
    "\n",
    "test_time = time.time() - test_start\n",
    "print(f\"\\nTest completed in {test_time:.2f} seconds with {test_api_calls} API calls\")\n",
    "\n",
    "if test_data is not None:\n",
    "    print(\"\\nâœ… Test successful!\")\n",
    "    print(test_data)\n",
    "    print(f\"\\nQuarters found: {sorted(test_data['quarter'].unique())}\")\n",
    "    print(f\"\\nSample metrics:\")\n",
    "    print(f\"  Book-to-Market: {test_data['book_to_market'].mean():.3f}\")\n",
    "    print(f\"  Earnings Yield: {test_data['earnings_yield'].mean():.3f}\")\n",
    "else:\n",
    "    print(\"\\nâŒ Test failed!\")\n",
    "    print(\"Errors:\", test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Collect Data for 2024\n",
    "\n",
    "Collect data for US stocks with market cap > $1B for 2024. This will take approximately **70 minutes** with filtering.\n",
    "\n",
    "**Note:** Since 2024 is ongoing, you may have partial data (Q1-Q3 or Q1-Q4 depending on current date).\n",
    "\n",
    "**Time estimate:** ~21,000 API calls @ 300/minute = 70 minutes\n",
    "\n",
    "To test with fewer tickers first, change `MAX_TICKERS = None` to `MAX_TICKERS = 100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2024 data\n",
    "YEAR = 2024\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "data_2024, errors_2024 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2024) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2024.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")\n",
    "    \n",
    "    # Show summary statistics\n",
    "    print(f\"\\nðŸ“ˆ Summary Statistics:\")\n",
    "    print(f\"   Debt/Assets - Mean: {data_2024['debt_to_assets'].mean():.3f}, Median: {data_2024['debt_to_assets'].median():.3f}\")\n",
    "    print(f\"   Book/Market - Mean: {data_2024['book_to_market'].mean():.3f}, Median: {data_2024['book_to_market'].median():.3f}\")\n",
    "    print(f\"   Earnings Yield - Mean: {data_2024['earnings_yield'].mean():.3f}, Median: {data_2024['earnings_yield'].median():.3f}\")\n",
    "    \n",
    "    # Show top companies (use latest quarter available)\n",
    "    latest_quarter = data_2024['quarter'].max()\n",
    "    latest_data = data_2024[data_2024['quarter'] == latest_quarter]\n",
    "    if len(latest_data) > 0:\n",
    "        print(f\"\\nðŸ† Top 10 companies by market cap ({latest_quarter}):\")\n",
    "        top_10 = latest_data.nsmallest(10, 'mkt_cap_rank')[['ticker', 'mkt_cap_rank', 'mkt_cap', 'book_to_market', 'earnings_yield', 'industry']]\n",
    "        top_10['mkt_cap'] = top_10['mkt_cap'].apply(lambda x: f\"${x/1e9:.1f}B\")\n",
    "        print(top_10.to_string(index=False))\n",
    "    \n",
    "    # Show available quarters\n",
    "    print(f\"\\nðŸ“… Available quarters for 2024: {sorted(data_2024['quarter'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Collect Data for 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2023 data\n",
    "YEAR = 2023\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2023, errors_2023 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2023) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2023.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")\n",
    "    \n",
    "    # Show summary statistics\n",
    "    print(f\"\\nðŸ“ˆ Summary Statistics:\")\n",
    "    print(f\"   Debt/Assets - Mean: {data_2023['debt_to_assets'].mean():.3f}, Median: {data_2023['debt_to_assets'].median():.3f}\")\n",
    "    print(f\"   Book/Market - Mean: {data_2023['book_to_market'].mean():.3f}, Median: {data_2023['book_to_market'].median():.3f}\")\n",
    "    print(f\"   Earnings Yield - Mean: {data_2023['earnings_yield'].mean():.3f}, Median: {data_2023['earnings_yield'].median():.3f}\")\n",
    "    \n",
    "    # Show top companies\n",
    "    q4_data = data_2023[data_2023['quarter'] == f'{YEAR}Q4']\n",
    "    if len(q4_data) > 0:\n",
    "        print(f\"\\nðŸ† Top 10 companies by market cap (Q4 {YEAR}):\")\n",
    "        top_10 = q4_data.nsmallest(10, 'mkt_cap_rank')[['ticker', 'mkt_cap_rank', 'mkt_cap', 'book_to_market', 'earnings_yield', 'industry']]\n",
    "        top_10['mkt_cap'] = top_10['mkt_cap'].apply(lambda x: f\"${x/1e9:.1f}B\")\n",
    "        print(top_10.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Collect Data for 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2022 data\n",
    "YEAR = 2022\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2022, errors_2022 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2022) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2022.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Collect Data for 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2021 data\n",
    "YEAR = 2021\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2021, errors_2021 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2021) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2021.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Collect Data for 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2020 data\n",
    "YEAR = 2020\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2020, errors_2020 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2020) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2020.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Collect Data for 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2019 data\n",
    "YEAR = 2019\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2019, errors_2019 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2019) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2019.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Collect Data for 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2018 data\n",
    "YEAR = 2018\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2018, errors_2018 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2018) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2018.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2018.csv errors_2018.json\n",
    "!git commit -m 'Add data for 2018'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Collect Data for 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2017 data\n",
    "YEAR = 2017\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2017, errors_2017 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2017) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2017.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2017.csv errors_2017.json\n",
    "!git commit -m 'Add data for 2017'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Collect Data for 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2016 data\n",
    "YEAR = 2016\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2016, errors_2016 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2016) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2016.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2016.csv errors_2016.json\n",
    "!git commit -m 'Add data for 2016'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Collect Data for 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2015 data\n",
    "YEAR = 2015\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2015, errors_2015 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2015) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2015.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2015.csv errors_2015.json\n",
    "!git commit -m 'Add data for 2015'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Collect Data for 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2014 data\n",
    "YEAR = 2014\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2014, errors_2014 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2014) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2014.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2014.csv errors_2014.json\n",
    "!git commit -m 'Add data for 2014'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Collect Data for 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2013 data\n",
    "YEAR = 2013\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2013, errors_2013 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2013) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2013.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2013.csv errors_2013.json\n",
    "!git commit -m 'Add data for 2013'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Collect Data for 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2012 data\n",
    "YEAR = 2012\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2012, errors_2012 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2012) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2012.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2012.csv errors_2012.json\n",
    "!git commit -m 'Add data for 2012'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Collect Data for 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2011 data\n",
    "YEAR = 2011\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2011, errors_2011 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2011) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2011.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2011.csv errors_2011.json\n",
    "!git commit -m 'Add data for 2011'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Collect Data for 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2010 data\n",
    "YEAR = 2010\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2010, errors_2010 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2010) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2010.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2010.csv errors_2010.json\n",
    "!git commit -m 'Add data for 2010'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18: Collect Data for 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2009 data\n",
    "YEAR = 2009\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2009, errors_2009 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2009) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2009.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2009.csv errors_2009.json\n",
    "!git commit -m 'Add data for 2009'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 19: Collect Data for 2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2008 data\n",
    "YEAR = 2008\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2008, errors_2008 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2008) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2008.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2008.csv errors_2008.json\n",
    "!git commit -m 'Add data for 2008'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 20: Collect Data for 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2007 data\n",
    "YEAR = 2007\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2007, errors_2007 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2007) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2007.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2007.csv errors_2007.json\n",
    "!git commit -m 'Add data for 2007'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 21: Collect Data for 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2006 data\n",
    "YEAR = 2006\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2006, errors_2006 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2006) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2006.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2006.csv errors_2006.json\n",
    "!git commit -m 'Add data for 2006'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 22: Collect Data for 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2005 data\n",
    "YEAR = 2005\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2005, errors_2005 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2005) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2005.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2005.csv errors_2005.json\n",
    "!git commit -m 'Add data for 2005'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 23: Collect Data for 2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2004 data\n",
    "YEAR = 2004\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2004, errors_2004 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2004) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2004.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2004.csv errors_2004.json\n",
    "!git commit -m 'Add data for 2004'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 24: Collect Data for 2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2003 data\n",
    "YEAR = 2003\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2003, errors_2003 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2003) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2003.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2003.csv errors_2003.json\n",
    "!git commit -m 'Add data for 2003'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 25: Collect Data for 2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2002 data\n",
    "YEAR = 2002\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2002, errors_2002 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2002) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2002.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2002.csv errors_2002.json\n",
    "!git commit -m 'Add data for 2002'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 26: Collect Data for 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2001 data\n",
    "YEAR = 2001\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2001, errors_2001 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2001) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2001.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2001.csv errors_2001.json\n",
    "!git commit -m 'Add data for 2001'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 27: Collect Data for 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2000 data\n",
    "YEAR = 2000\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2000, errors_2000 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2000) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2000.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2000.csv errors_2000.json\n",
    "!git commit -m 'Add data for 2000'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 28: Collect Data for 1999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 1999 data\n",
    "YEAR = 1999\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_1999, errors_1999 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_1999) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_1999.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_1999.csv errors_1999.json\n",
    "!git commit -m 'Add data for 1999'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 29: Collect Data for 1998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 1998 data\n",
    "YEAR = 1998\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_1998, errors_1998 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_1998) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_1998.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_1998.csv errors_1998.json\n",
    "!git commit -m 'Add data for 1998'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 30: Collect Data for 1997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 1997 data\n",
    "YEAR = 1997\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_1997, errors_1997 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_1997) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_1997.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_1997.csv errors_1997.json\n",
    "!git commit -m 'Add data for 1997'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 31: Collect Data for 1996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 1996 data\n",
    "YEAR = 1996\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_1996, errors_1996 = collect_year_data(get_tickers_for_year(YEAR), year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_1996) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_1996.to_csv(filename, index=False)\n",
    "    print(f\n",
    "âœ… Data saved to '{filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_1996.csv errors_1996.json\n",
    "!git commit -m 'Add data for 1996'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Review Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review what we've collected\n",
    "import glob\n",
    "\n",
    "print(\"ðŸ“ Available data files:\")\n",
    "data_files = sorted(glob.glob(\"stock_data_*.csv\"))\n",
    "\n",
    "total_rows = 0\n",
    "for file in data_files:\n",
    "    df = pd.read_csv(file)\n",
    "    total_rows += len(df)\n",
    "    print(f\"  {file}: {len(df):,} rows, {df['ticker'].nunique()} tickers\")\n",
    "    # Check for duplicates\n",
    "    duplicates = df.groupby(['ticker', 'quarter']).size()\n",
    "    if (duplicates > 1).any():\n",
    "        print(f\"    âš ï¸  Found {(duplicates > 1).sum()} duplicate ticker-quarter combinations\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Total: {total_rows:,} rows across {len(data_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors\n",
    "error_files = sorted(glob.glob(\"errors_*.json\"))\n",
    "\n",
    "if error_files:\n",
    "    print(\"ðŸ“ Error analysis:\")\n",
    "    \n",
    "    for error_file in error_files:\n",
    "        with open(error_file, 'r') as f:\n",
    "            errors = json.load(f)\n",
    "        \n",
    "        # Count error types\n",
    "        error_types = {}\n",
    "        for error in errors:\n",
    "            for err_msg in error.get('errors', []):\n",
    "                err_type = err_msg.split(':')[0] if ':' in err_msg else err_msg\n",
    "                error_types[err_type] = error_types.get(err_type, 0) + 1\n",
    "        \n",
    "        print(f\"\\n{error_file}: {len(errors)} failed tickers\")\n",
    "        for err_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "            print(f\"  - {err_type}: {count}\")\n",
    "else:\n",
    "    print(\"No error files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Combine All Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all years into one file\n",
    "years = [2019, 2020, 2021, 2022, 2023, 2024]\n",
    "all_data = []\n",
    "\n",
    "for year in years:\n",
    "    filename = f\"stock_data_{year}.csv\"\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df['quarter'] = pd.PeriodIndex(df['quarter'], freq='Q')\n",
    "        all_data.append(df)\n",
    "        print(f\"âœ“ Loaded {year}: {len(df)} rows\")\n",
    "    else:\n",
    "        print(f\"âœ— {filename} not found\")\n",
    "\n",
    "if all_data:\n",
    "    combined = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Final deduplication across all years\n",
    "    combined = combined.sort_values(['ticker', 'quarter']).drop_duplicates(['ticker', 'quarter'], keep='last')\n",
    "    \n",
    "    # Recalculate rankings\n",
    "    combined['mkt_cap_rank'] = combined.groupby('quarter')['mkt_cap'].rank(method='dense', ascending=False).astype(int)\n",
    "    \n",
    "    combined.to_csv(\"stock_data_combined_6years.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… Combined dataset saved!\")\n",
    "    print(f\"   Total: {len(combined):,} rows\")\n",
    "    print(f\"   Tickers: {combined['ticker'].nunique()}\")\n",
    "    print(f\"   Period: {combined['quarter'].min()} to {combined['quarter'].max()}\")\n",
    "    \n",
    "    # Show metric distributions\n",
    "    print(f\"\\nðŸ“Š Metric distributions:\")\n",
    "    print(f\"   Debt/Assets: {combined['debt_to_assets'].describe()[[\"mean\", \"50%\", \"std\"]].round(3).to_dict()}\")\n",
    "    print(f\"   Book/Market: {combined['book_to_market'].describe()[[\"mean\", \"50%\", \"std\"]].round(3).to_dict()}\")\n",
    "    print(f\"   Earnings Yield: {combined['earnings_yield'].describe()[[\"mean\", \"50%\", \"std\"]].round(3).to_dict()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
