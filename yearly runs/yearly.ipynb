{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate-Limited Market Cap Filtered Data Collection (Year by Year)\n\n",
    "This notebook collects financial data for **US stocks with market cap > $1B** with strict rate limiting and saves each year separately.\n\n",
    "**Key features:**\n",
    "1. **Market cap filter**: Only collects data for stocks with market cap > $1B\n",
    "2. **Rate limiting**: 300 API calls per minute\n",
    "3. **Year-by-year collection**: Each year saved to separate CSV\n",
    "4. **Deduplication**: Ensures only one entry per quarter (Q1-Q4)\n",
    "5. **Enhanced metrics**: Includes book-to-market and earnings yield\n",
    "6. **Error tracking**: Detailed logs for debugging\n\n",
    "**Target columns:** `quarter`, `ticker`, `industry`, `sector`, `debt_to_assets`, `mkt_cap`, `stock_price`, `book_to_market`, `earnings_yield`, `mkt_cap_rank`\n\n",
    "**Time estimate:** ~70 minutes per year (vs 2.7 hours without filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚠️ MARKET CAP FILTERED COLLECTION\n\n**This notebook filters for stocks with market cap > $1B**\n\n**Time requirements:**\n- **Per year:** ~70 minutes (with market cap filtering)\n- **All 6 years (2019-2024):** ~7 hours total\n- **API calls:** ~21,000 per year (vs 48,000 without filtering)\n- **Expected companies:** ~1,800 per year (15-20% of all tickers)\n\n**To start with a smaller test:**\n1. Change `MAX_TICKERS = None` to `MAX_TICKERS = 100` in any collection cell\n2. Run one year first to verify everything works\n3. Then change back to `MAX_TICKERS = None` for full collection\n\n---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Your API key\n",
    "API = \"7cNMpVzb43GKtm05iRTDWJtyJXSylX8J\"\n",
    "\n",
    "# Rate limiting configuration\n",
    "API_CALLS_PER_MINUTE = 300\n",
    "SECONDS_PER_CALL = 60 / API_CALLS_PER_MINUTE  # 0.2 seconds per call\n",
    "\n",
    "# Market cap threshold (1 billion)\n",
    "MARKET_CAP_THRESHOLD = 1e9\n",
    "\n",
    "print(f\"Rate limit configured: {API_CALLS_PER_MINUTE} calls/minute ({SECONDS_PER_CALL:.2f} seconds/call)\")\n",
    "print(f\"Market cap filter: > ${MARKET_CAP_THRESHOLD/1e9:.0f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions with Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(url: str, params: Dict[str, Any] = {}) -> Optional[Any]:\n",
    "    \"\"\"Safely get JSON data from API with error handling and rate limit retry\"\"\"\n",
    "    try:\n",
    "        params[\"apikey\"] = API\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        \n",
    "        # Handle rate limiting\n",
    "        if response.status_code == 429:\n",
    "            print(f\"⚠️  Rate limit hit! Waiting 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "            return get_json(url, params)  # Retry\n",
    "            \n",
    "        response.raise_for_status()\n",
    "        js = response.json()\n",
    "        \n",
    "        # Handle different response formats\n",
    "        if isinstance(js, dict) and \"historical\" in js:\n",
    "            return js[\"historical\"]\n",
    "        elif isinstance(js, list):\n",
    "            return js\n",
    "        else:\n",
    "            return js\n",
    "            \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error {e.response.status_code}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_market_cap(ticker: str, year: int) -> Tuple[bool, Optional[float]]:\n",
    "    \"\"\"Check if ticker had market cap above threshold in given year\"\"\"\n",
    "    try:\n",
    "        # Get market cap for the year\n",
    "        start_date = f\"{year}-01-01\"\n",
    "        end_date = f\"{year}-12-31\"\n",
    "        \n",
    "        mc_data = get_json(\n",
    "            f\"https://financialmodelingprep.com/api/v3/historical-market-capitalization/{ticker}\",\n",
    "            {\"from\": start_date, \"to\": end_date}\n",
    "        )\n",
    "        \n",
    "        if not mc_data:\n",
    "            return False, None\n",
    "            \n",
    "        # Calculate average market cap for the year\n",
    "        mc_df = pd.DataFrame(mc_data)\n",
    "        avg_market_cap = mc_df[\"marketCap\"].mean()\n",
    "        \n",
    "        return avg_market_cap > MARKET_CAP_THRESHOLD, avg_market_cap\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking market cap for {ticker}: {e}\")\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ticker_year(ticker: str, year: int) -> Tuple[Optional[pd.DataFrame], Dict[str, Any], int]:\n",
    "    \"\"\"Process data for a single ticker for a specific year\"\"\"\n",
    "    error_log = {\"ticker\": ticker, \"year\": year, \"errors\": []}\n",
    "    api_calls = 0\n",
    "    \n",
    "    try:\n",
    "        # First check market cap (1 API call)\n",
    "        is_large_cap, avg_market_cap = check_market_cap(ticker, year)\n",
    "        api_calls += 1\n",
    "        \n",
    "        if not is_large_cap:\n",
    "            error_log[\"errors\"].append(f\"Market cap below threshold (avg: ${avg_market_cap:,.0f})\")\n",
    "            return None, error_log, api_calls\n",
    "        \n",
    "        # Calculate date range for the specific year\n",
    "        start_date = datetime(year, 1, 1)\n",
    "        end_date = datetime(year, 12, 31)\n",
    "        \n",
    "        # Get balance sheet data (API call 2)\n",
    "        bs = get_json(\n",
    "            f\"https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}\", \n",
    "            {\"period\": \"quarter\", \"limit\": 20}  # Get enough quarters\n",
    "        )\n",
    "        api_calls += 1\n",
    "        \n",
    "        # Get income statement data for EPS (API call 3)\n",
    "        inc = get_json(\n",
    "            f\"https://financialmodelingprep.com/api/v3/income-statement/{ticker}\",\n",
    "            {\"period\": \"quarter\", \"limit\": 20}\n",
    "        )\n",
    "        api_calls += 1\n",
    "        \n",
    "        # Get market cap data (API call 4)\n",
    "        mc = get_json(\n",
    "            f\"https://financialmodelingprep.com/api/v3/historical-market-capitalization/{ticker}\", \n",
    "            {\"from\": start_date.strftime(\"%Y-%m-%d\"), \"to\": end_date.strftime(\"%Y-%m-%d\")}\n",
    "        )\n",
    "        api_calls += 1\n",
    "        \n",
    "        # Get price data (API call 5)\n",
    "        px = get_json(\n",
    "            f\"https://financialmodelingprep.com/api/v3/historical-price-full/{ticker}\", \n",
    "            {\"from\": start_date.strftime(\"%Y-%m-%d\"), \"to\": end_date.strftime(\"%Y-%m-%d\")}\n",
    "        )\n",
    "        api_calls += 1\n",
    "        \n",
    "        # Get company profile (API call 6)\n",
    "        profile = get_json(f\"https://financialmodelingprep.com/api/v3/profile/{ticker}\")\n",
    "        api_calls += 1\n",
    "        \n",
    "        # Track missing data\n",
    "        if not bs:\n",
    "            error_log[\"errors\"].append(\"No balance sheet data\")\n",
    "        if not inc:\n",
    "            error_log[\"errors\"].append(\"No income statement data\")\n",
    "        if not mc:\n",
    "            error_log[\"errors\"].append(\"No market cap data\")\n",
    "        if not px:\n",
    "            error_log[\"errors\"].append(\"No price data\")\n",
    "        if not profile:\n",
    "            error_log[\"errors\"].append(\"No profile data\")\n",
    "            \n",
    "        if not all([bs, inc, mc, px, profile]):\n",
    "            return None, error_log, api_calls\n",
    "        \n",
    "        # Extract company info\n",
    "        industry = profile[0].get(\"industry\", \"Unknown\")\n",
    "        sector = profile[0].get(\"sector\", \"Unknown\")\n",
    "        \n",
    "        # Process balance sheet data\n",
    "        bs_df = pd.DataFrame(bs)\n",
    "        bs_df['date'] = pd.to_datetime(bs_df['date'])\n",
    "        # Filter for specific year\n",
    "        bs_df = bs_df[bs_df['date'].dt.year == year]\n",
    "        \n",
    "        if len(bs_df) == 0:\n",
    "            error_log[\"errors\"].append(f\"No balance sheet data for year {year}\")\n",
    "            return None, error_log, api_calls\n",
    "            \n",
    "        bs_df = (\n",
    "            bs_df[['date', 'shortTermDebt', 'longTermDebt', 'totalAssets', \n",
    "                   'totalStockholdersEquity', 'commonStock']]\n",
    "            .assign(\n",
    "                quarter=lambda d: d.date.dt.to_period(\"Q\"),\n",
    "                debt_to_assets=lambda d: (\n",
    "                    (d.shortTermDebt.fillna(0) + d.longTermDebt.fillna(0)) / \n",
    "                    d.totalAssets.replace(0, pd.NA)\n",
    "                ),\n",
    "                book_value=lambda d: d.totalStockholdersEquity\n",
    "            )\n",
    "            .dropna(subset=[\"debt_to_assets\"])\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('quarter', keep='last')  # Keep latest data for each quarter\n",
    "        )\n",
    "        \n",
    "        # Process income statement data\n",
    "        inc_df = pd.DataFrame(inc)\n",
    "        inc_df['date'] = pd.to_datetime(inc_df['date'])\n",
    "        inc_df = inc_df[inc_df['date'].dt.year == year]\n",
    "        \n",
    "        inc_df = (\n",
    "            inc_df[['date', 'eps', 'weightedAverageShsOut']]\n",
    "            .assign(quarter=lambda d: d.date.dt.to_period(\"Q\"))\n",
    "            .rename(columns={\"weightedAverageShsOut\": \"shares_outstanding\"})\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('quarter', keep='last')  # Keep latest data for each quarter\n",
    "        )\n",
    "        \n",
    "        # Process market cap data\n",
    "        mc_df = (\n",
    "            pd.DataFrame(mc)\n",
    "            .assign(\n",
    "                date=lambda d: pd.to_datetime(d.date),\n",
    "                quarter=lambda d: d.date.dt.to_period(\"Q\")\n",
    "            )\n",
    "            .sort_values(\"date\")\n",
    "            .drop_duplicates(\"quarter\", keep=\"last\")\n",
    "            .rename(columns={\"marketCap\": \"mkt_cap\"})\n",
    "            [['quarter', 'mkt_cap']]\n",
    "        )\n",
    "        \n",
    "        # Process price data (using adjusted close)\n",
    "        px_df = (\n",
    "            pd.DataFrame(px)\n",
    "            .assign(\n",
    "                date=lambda d: pd.to_datetime(d.date),\n",
    "                quarter=lambda d: d.date.dt.to_period(\"Q\")\n",
    "            )\n",
    "            .sort_values(\"date\")\n",
    "            .drop_duplicates(\"quarter\", keep=\"last\")\n",
    "            .rename(columns={\"adjClose\": \"stock_price\"})  # Using adjusted close\n",
    "            [['quarter', 'stock_price']]\n",
    "        )\n",
    "        \n",
    "        # Merge all data\n",
    "        merged = (\n",
    "            bs_df.merge(inc_df, on=\"quarter\", how=\"left\")\n",
    "                 .merge(mc_df, on=\"quarter\", how=\"left\")\n",
    "                 .merge(px_df, on=\"quarter\", how=\"left\")\n",
    "        )\n",
    "        \n",
    "        # Calculate book-to-market and earnings yield\n",
    "        merged = merged.assign(\n",
    "            ticker=ticker,\n",
    "            industry=industry,\n",
    "            sector=sector,\n",
    "            # Book-to-Market = Book Value per Share / Price per Share\n",
    "            book_to_market=lambda d: (d.book_value / d.shares_outstanding) / d.stock_price,\n",
    "            # Earnings Yield = EPS / Price\n",
    "            earnings_yield=lambda d: d.eps / d.stock_price\n",
    "        )\n",
    "        \n",
    "        # Select final columns\n",
    "        merged = merged[[\n",
    "            'quarter', 'ticker', 'industry', 'sector', 'debt_to_assets', \n",
    "            'mkt_cap', 'stock_price', 'book_to_market', 'earnings_yield'\n",
    "        ]].dropna()\n",
    "        \n",
    "        # Ensure we only have Q1-Q4 for the specified year\n",
    "        valid_quarters = [f\"{year}Q1\", f\"{year}Q2\", f\"{year}Q3\", f\"{year}Q4\"]\n",
    "        merged = merged[merged['quarter'].astype(str).isin(valid_quarters)]\n",
    "        \n",
    "        if len(merged) == 0:\n",
    "            error_log[\"errors\"].append(\"No valid data after merging\")\n",
    "            return None, error_log, api_calls\n",
    "            \n",
    "        return merged, error_log, api_calls\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_log[\"errors\"].append(f\"Exception: {str(e)}\")\n",
    "        return None, error_log, api_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_year_data(tickers: List[str], year: int, max_tickers: Optional[int] = None, \n",
    "                     save_progress: bool = True, progress_interval: int = 100) -> Tuple[pd.DataFrame, List[Dict]]:\n",
    "    \"\"\"Collect data for multiple tickers for a specific year with strict rate limiting\"\"\"\n",
    "    all_data = []\n",
    "    all_errors = []\n",
    "    successful_tickers = []\n",
    "    failed_tickers = []\n",
    "    skipped_tickers = []\n",
    "    total_api_calls = 0\n",
    "    \n",
    "    tickers_to_process = tickers[:max_tickers] if max_tickers else tickers\n",
    "    total_tickers = len(tickers_to_process)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  COLLECTING DATA FOR YEAR {year}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total tickers to check: {total_tickers}\")\n",
    "    print(f\"Market cap filter: >${MARKET_CAP_THRESHOLD/1e9:.0f}B\")\n",
    "    print(f\"API rate limit: {API_CALLS_PER_MINUTE} calls/minute\")\n",
    "    print(f\"Progress saves: Every {progress_interval} tickers\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    last_call_time = time.time()\n",
    "    \n",
    "    for i, ticker in enumerate(tickers_to_process):\n",
    "        # Progress update\n",
    "        if i > 0 and i % 20 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_time = elapsed / i\n",
    "            remaining = (total_tickers - i) * avg_time\n",
    "            \n",
    "            print(f\"\\n[Progress: {i}/{total_tickers} ({i/total_tickers*100:.1f}%)]\")\n",
    "            print(f\"  Time: {elapsed/60:.1f}min elapsed, ~{remaining/60:.1f}min remaining\")\n",
    "            print(f\"  Success: {len(successful_tickers)}, Failed: {len(failed_tickers)}, Skipped (small cap): {len(skipped_tickers)}\")\n",
    "            print(f\"  API calls: {total_api_calls} ({total_api_calls/elapsed*60:.0f}/minute avg)\")\n",
    "            print(f\"  Current batch: \", end=\"\")\n",
    "        \n",
    "        # Rate limiting\n",
    "        time_since_last_call = time.time() - last_call_time\n",
    "        if time_since_last_call < SECONDS_PER_CALL:\n",
    "            time.sleep(SECONDS_PER_CALL - time_since_last_call)\n",
    "        last_call_time = time.time()\n",
    "        \n",
    "        # Process ticker\n",
    "        ticker_data, error_log, api_calls = process_ticker_year(ticker, year)\n",
    "        total_api_calls += api_calls\n",
    "        \n",
    "        if ticker_data is not None and len(ticker_data) > 0:\n",
    "            all_data.append(ticker_data)\n",
    "            successful_tickers.append(ticker)\n",
    "            print(\"✓\", end=\"\", flush=True)\n",
    "        elif any(\"Market cap below threshold\" in err for err in error_log.get(\"errors\", [])):\n",
    "            skipped_tickers.append(ticker)\n",
    "            print(\"○\", end=\"\", flush=True)\n",
    "        else:\n",
    "            failed_tickers.append(ticker)\n",
    "            all_errors.append(error_log)\n",
    "            print(\"✗\", end=\"\", flush=True)\n",
    "        \n",
    "        # Save progress periodically\n",
    "        if save_progress and (i + 1) % progress_interval == 0 and all_data:\n",
    "            temp_df = pd.concat(all_data, ignore_index=True)\n",
    "            temp_df['mkt_cap_rank'] = temp_df.groupby('quarter')['mkt_cap'].rank(method='dense', ascending=False).astype(int)\n",
    "            progress_filename = f\"progress_{year}_tickers_{i+1}.csv\"\n",
    "            temp_df.to_csv(progress_filename, index=False)\n",
    "            print(f\"\\n  💾 Progress saved: {progress_filename} ({len(temp_df)} rows)\")\n",
    "    \n",
    "    # Final summary\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n\\n{'='*70}\")\n",
    "    print(f\"  YEAR {year} COLLECTION COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
    "    print(f\"Successful: {len(successful_tickers)} tickers\")\n",
    "    print(f\"Failed: {len(failed_tickers)} tickers\")\n",
    "    print(f\"Skipped (small cap): {len(skipped_tickers)} tickers\")\n",
    "    print(f\"Total API calls: {total_api_calls:,} ({total_api_calls/total_time*60:.0f}/minute avg)\")\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        print(\"\\n⚠️  No data collected!\")\n",
    "        return pd.DataFrame(columns=[\"quarter\", \"ticker\", \"industry\", \"sector\", \n",
    "                                    \"debt_to_assets\", \"mkt_cap\", \"stock_price\", \n",
    "                                    \"book_to_market\", \"earnings_yield\", \"mkt_cap_rank\"]), all_errors\n",
    "    \n",
    "    # Combine all data\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Final deduplication - ensure only one entry per ticker-quarter\n",
    "    final_df = final_df.sort_values(['ticker', 'quarter']).drop_duplicates(['ticker', 'quarter'], keep='last')\n",
    "    \n",
    "    # Add market cap ranking\n",
    "    final_df['mkt_cap_rank'] = final_df.groupby('quarter')['mkt_cap'].rank(method='dense', ascending=False).astype(int)\n",
    "    \n",
    "    # Sort by ticker and quarter\n",
    "    final_df = final_df.sort_values(['ticker', 'quarter']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n📊 Final dataset: {len(final_df)} rows, {final_df['ticker'].nunique()} tickers\")\n",
    "    print(f\"   Quarters: {sorted(final_df['quarter'].unique())}\")\n",
    "    \n",
    "    # Verify quarter coverage\n",
    "    expected_quarters = {f\"{year}Q1\", f\"{year}Q2\", f\"{year}Q3\", f\"{year}Q4\"}\n",
    "    actual_quarters = set(final_df['quarter'].astype(str).unique())\n",
    "    missing_quarters = expected_quarters - actual_quarters\n",
    "    if missing_quarters:\n",
    "        print(f\"   ⚠️  Missing quarters: {sorted(missing_quarters)}\")\n",
    "    \n",
    "    # Save error log\n",
    "    if all_errors:\n",
    "        error_filename = f\"errors_{year}.json\"\n",
    "        with open(error_filename, 'w') as f:\n",
    "            json.dump(all_errors, f, indent=2, default=str)\n",
    "        print(f\"\\n📝 Error log saved: {error_filename} ({len(all_errors)} errors)\")\n",
    "    \n",
    "    # Clean up progress files\n",
    "    if save_progress:\n",
    "        for progress_file in [f for f in os.listdir('.') if f.startswith(f'progress_{year}_')]:\n",
    "            os.remove(progress_file)\n",
    "        print(f\"🧹 Cleaned up progress files\")\n",
    "    \n",
    "    return final_df, all_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get List of US Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of US tickers\n",
    "print(\"Fetching ticker list...\")\n",
    "tickers_data = get_json(\"https://financialmodelingprep.com/api/v3/stock/list\")\n",
    "\n",
    "if tickers_data:\n",
    "    # Filter for US exchanges and remove penny stocks\n",
    "    us_tickers = [\n",
    "        d[\"symbol\"] for d in tickers_data \n",
    "        if d[\"exchangeShortName\"] in [\"NYSE\", \"NASDAQ\"] \n",
    "        and (d.get(\"price\") is not None and d.get(\"price\", 0) > 5)\n",
    "        and len(d[\"symbol\"]) <= 5\n",
    "        and \".\" not in d[\"symbol\"]\n",
    "    ]\n",
    "    \n",
    "    print(f\"✅ Found {len(us_tickers)} US tickers\")\n",
    "    print(f\"   Sample: {us_tickers[:10]}\")\n",
    "    \n",
    "    # Estimate how many will be large cap\n",
    "    estimated_large_cap = int(len(us_tickers) * 0.15)\n",
    "    print(f\"\\n📊 Estimates:\")\n",
    "    print(f\"   Expected large cap (>$1B): ~{estimated_large_cap} stocks\")\n",
    "    print(f\"   Estimated API calls per year: ~{len(us_tickers) + estimated_large_cap * 5:,}\")\n",
    "    print(f\"   Estimated time per year: ~{(len(us_tickers) + estimated_large_cap * 5) / API_CALLS_PER_MINUTE:.0f} minutes\")\n",
    "else:\n",
    "    print(\"❌ Failed to fetch ticker list. Using sample tickers.\")\n",
    "    us_tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"META\", \"NVDA\", \"JPM\", \"JNJ\", \"V\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Test with Single Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with AAPL for 2023\n",
    "print(\"Testing with AAPL for year 2023...\")\n",
    "test_start = time.time()\n",
    "\n",
    "test_data, test_errors, test_api_calls = process_ticker_year(\"AAPL\", 2023)\n",
    "\n",
    "test_time = time.time() - test_start\n",
    "print(f\"\\nTest completed in {test_time:.2f} seconds with {test_api_calls} API calls\")\n",
    "\n",
    "if test_data is not None:\n",
    "    print(\"\\n✅ Test successful!\")\n",
    "    print(test_data)\n",
    "    print(f\"\\nQuarters found: {sorted(test_data['quarter'].unique())}\")\n",
    "    print(f\"\\nSample metrics:\")\n",
    "    print(f\"  Book-to-Market: {test_data['book_to_market'].mean():.3f}\")\n",
    "    print(f\"  Earnings Yield: {test_data['earnings_yield'].mean():.3f}\")\n",
    "else:\n",
    "    print(\"\\n❌ Test failed!\")\n",
    "    print(\"Errors:\", test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Collect Data for 2024\n\nCollect data for US stocks with market cap > $1B for 2024. This will take approximately **70 minutes** with filtering.\n\n**Note:** Since 2024 is ongoing, you may have partial data (Q1-Q3 or Q1-Q4 depending on current date).\n\n**Time estimate:** ~21,000 API calls @ 300/minute = 70 minutes\n\nTo test with fewer tickers first, change `MAX_TICKERS = None` to `MAX_TICKERS = 100`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2024 data\n",
    "YEAR = 2024\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "data_2024, errors_2024 = collect_year_data(us_tickers, year=YEAR, max_tickers=MAX_TICKERS)\n\n",
    "if len(data_2024) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2024.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")\n",
    "    \n",
    "    # Show summary statistics\n",
    "    print(f\"\\n📈 Summary Statistics:\")\n",
    "    print(f\"   Debt/Assets - Mean: {data_2024['debt_to_assets'].mean():.3f}, Median: {data_2024['debt_to_assets'].median():.3f}\")\n",
    "    print(f\"   Book/Market - Mean: {data_2024['book_to_market'].mean():.3f}, Median: {data_2024['book_to_market'].median():.3f}\")\n",
    "    print(f\"   Earnings Yield - Mean: {data_2024['earnings_yield'].mean():.3f}, Median: {data_2024['earnings_yield'].median():.3f}\")\n",
    "    \n",
    "    # Show top companies (use latest quarter available)\n",
    "    latest_quarter = data_2024['quarter'].max()\n",
    "    latest_data = data_2024[data_2024['quarter'] == latest_quarter]\n",
    "    if len(latest_data) > 0:\n",
    "        print(f\"\\n🏆 Top 10 companies by market cap ({latest_quarter}):\")\n",
    "        top_10 = latest_data.nsmallest(10, 'mkt_cap_rank')[['ticker', 'mkt_cap_rank', 'mkt_cap', 'book_to_market', 'earnings_yield', 'industry']]\n",
    "        top_10['mkt_cap'] = top_10['mkt_cap'].apply(lambda x: f\"${x/1e9:.1f}B\")\n",
    "        print(top_10.to_string(index=False))\n",
    "    \n",
    "    # Show available quarters\n",
    "    print(f\"\\n📅 Available quarters for 2024: {sorted(data_2024['quarter'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Collect Data for 2023"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2023 data\n",
    "YEAR = 2023\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2023, errors_2023 = collect_year_data(us_tickers, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2023) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2023.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")\n",
    "    \n",
    "    # Show summary statistics\n",
    "    print(f\"\\n📈 Summary Statistics:\")\n",
    "    print(f\"   Debt/Assets - Mean: {data_2023['debt_to_assets'].mean():.3f}, Median: {data_2023['debt_to_assets'].median():.3f}\")\n",
    "    print(f\"   Book/Market - Mean: {data_2023['book_to_market'].mean():.3f}, Median: {data_2023['book_to_market'].median():.3f}\")\n",
    "    print(f\"   Earnings Yield - Mean: {data_2023['earnings_yield'].mean():.3f}, Median: {data_2023['earnings_yield'].median():.3f}\")\n",
    "    \n",
    "    # Show top companies\n",
    "    q4_data = data_2023[data_2023['quarter'] == f'{YEAR}Q4']\n",
    "    if len(q4_data) > 0:\n",
    "        print(f\"\\n🏆 Top 10 companies by market cap (Q4 {YEAR}):\")\n",
    "        top_10 = q4_data.nsmallest(10, 'mkt_cap_rank')[['ticker', 'mkt_cap_rank', 'mkt_cap', 'book_to_market', 'earnings_yield', 'industry']]\n",
    "        top_10['mkt_cap'] = top_10['mkt_cap'].apply(lambda x: f\"${x/1e9:.1f}B\")\n",
    "        print(top_10.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Collect Data for 2022"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2022 data\n",
    "YEAR = 2022\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2022, errors_2022 = collect_year_data(us_tickers, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2022) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2022.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Collect Data for 2021"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2021 data\n",
    "YEAR = 2021\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2021, errors_2021 = collect_year_data(us_tickers, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2021) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2021.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7: Collect Data for 2020"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2020 data\n",
    "YEAR = 2020\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2020, errors_2020 = collect_year_data(us_tickers, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2020) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2020.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 8: Collect Data for 2019"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2019 data\n",
    "YEAR = 2019\n",
    "MAX_TICKERS = None  # Collect ALL US tickers (~12,000)\n",
    "\n",
    "data_2019, errors_2019 = collect_year_data(us_tickers, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2019) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2019.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Review Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review what we've collected\n",
    "import glob\n",
    "\n",
    "print(\"📁 Available data files:\")\n",
    "data_files = sorted(glob.glob(\"stock_data_*.csv\"))\n",
    "\n",
    "total_rows = 0\n",
    "for file in data_files:\n",
    "    df = pd.read_csv(file)\n",
    "    total_rows += len(df)\n",
    "    print(f\"  {file}: {len(df):,} rows, {df['ticker'].nunique()} tickers\")\n",
    "    # Check for duplicates\n",
    "    duplicates = df.groupby(['ticker', 'quarter']).size()\n",
    "    if (duplicates > 1).any():\n",
    "        print(f\"    ⚠️  Found {(duplicates > 1).sum()} duplicate ticker-quarter combinations\")\n",
    "\n",
    "print(f\"\\n📊 Total: {total_rows:,} rows across {len(data_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors\n",
    "error_files = sorted(glob.glob(\"errors_*.json\"))\n",
    "\n",
    "if error_files:\n",
    "    print(\"📝 Error analysis:\")\n",
    "    \n",
    "    for error_file in error_files:\n",
    "        with open(error_file, 'r') as f:\n",
    "            errors = json.load(f)\n",
    "        \n",
    "        # Count error types\n",
    "        error_types = {}\n",
    "        for error in errors:\n",
    "            for err_msg in error.get('errors', []):\n",
    "                err_type = err_msg.split(':')[0] if ':' in err_msg else err_msg\n",
    "                error_types[err_type] = error_types.get(err_type, 0) + 1\n",
    "        \n",
    "        print(f\"\\n{error_file}: {len(errors)} failed tickers\")\n",
    "        for err_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "            print(f\"  - {err_type}: {count}\")\n",
    "else:\n",
    "    print(\"No error files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Combine All Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all years into one file\n",
    "years = [2019, 2020, 2021, 2022, 2023, 2024]\n",
    "all_data = []\n",
    "\n",
    "for year in years:\n",
    "    filename = f\"stock_data_{year}.csv\"\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df['quarter'] = pd.PeriodIndex(df['quarter'], freq='Q')\n",
    "        all_data.append(df)\n",
    "        print(f\"✓ Loaded {year}: {len(df)} rows\")\n",
    "    else:\n",
    "        print(f\"✗ {filename} not found\")\n",
    "\n",
    "if all_data:\n",
    "    combined = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Final deduplication across all years\n",
    "    combined = combined.sort_values(['ticker', 'quarter']).drop_duplicates(['ticker', 'quarter'], keep='last')\n",
    "    \n",
    "    # Recalculate rankings\n",
    "    combined['mkt_cap_rank'] = combined.groupby('quarter')['mkt_cap'].rank(method='dense', ascending=False).astype(int)\n",
    "    \n",
    "    combined.to_csv(\"stock_data_combined_6years.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\n✅ Combined dataset saved!\")\n",
    "    print(f\"   Total: {len(combined):,} rows\")\n",
    "    print(f\"   Tickers: {combined['ticker'].nunique()}\")\n",
    "    print(f\"   Period: {combined['quarter'].min()} to {combined['quarter'].max()}\")\n",
    "    \n",
    "    # Show metric distributions\n",
    "    print(f\"\\n📊 Metric distributions:\")\n",
    "    print(f\"   Debt/Assets: {combined['debt_to_assets'].describe()[[\"mean\", \"50%\", \"std\"]].round(3).to_dict()}\")\n",
    "    print(f\"   Book/Market: {combined['book_to_market'].describe()[[\"mean\", \"50%\", \"std\"]].round(3).to_dict()}\")\n",
    "    print(f\"   Earnings Yield: {combined['earnings_yield'].describe()[[\"mean\", \"50%\", \"std\"]].round(3).to_dict()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}