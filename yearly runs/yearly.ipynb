{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate-Limited Market Cap Filtered Data Collection (Year by Year)\n",
    "\n",
    "This notebook collects financial data for **US stocks with market cap > $1B** with strict rate limiting and saves each year separately.\n",
    "\n",
    "**Key features:**\n",
    "1. **Market cap filter**: Only collects data for stocks with market cap > $1B\n",
    "2. **Rate limiting**: 750 API calls per minute\n",
    "3. **Year-by-year collection**: Each year saved to separate CSV\n",
    "4. **Deduplication**: Ensures only one entry per quarter (Q1-Q4)\n",
    "5. **Enhanced metrics**: Includes book-to-market and earnings yield\n",
    "6. **Error tracking**: Detailed logs for debugging\n",
    "\n",
    "**Target columns:** `quarter`, `ticker`, `industry`, `sector`, `debt_to_assets`, `mkt_cap`, `stock_price`, `book_to_market`, `earnings_yield`, `mkt_cap_rank`\n",
    "\n",
    "**Time estimate:** ~70 minutes per year (vs 2.7 hours without filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ðŸš€ Optimizations Applied\n",
    "\n",
    "This notebook has been optimized with three key improvements:\n",
    "\n",
    "### 1. **Removed Extra Sleep Delays**\n",
    "- The `get_json()` function already enforces rate limiting with sleep delays\n",
    "- Removed redundant sleep in `collect_year_data()` to avoid double-waiting\n",
    "- **Impact**: ~8% faster execution per ticker\n",
    "\n",
    "### 2. **Batch Processing with Bulk Endpoints**\n",
    "- Uses `/api/v3/profile/{ticker1,ticker2,...}` to fetch up to 50 company profiles in one API call\n",
    "- Processes tickers in batches of 50 instead of one-by-one\n",
    "- **Impact**: Reduces profile API calls by 98% (from 50 calls to 1 call per batch)\n",
    "\n",
    "### 3. **Year-Specific Ticker Lists**\n",
    "- Fetches ticker lists from the end of the previous year (e.g., Dec 31, 2022 for 2023 data)\n",
    "- Ensures we only process stocks that actually existed in each historical year\n",
    "- Falls back to current ticker list if historical data unavailable\n",
    "- **Impact**: More accurate historical analysis, avoids processing stocks that didn't exist yet\n",
    "\n",
    "### ðŸ“Š Expected Performance Improvements:\n",
    "- **Time savings**: ~30-40% faster overall\n",
    "- **API efficiency**: Better utilization of rate limits\n",
    "- **Data accuracy**: Historical ticker lists match the actual market composition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš ï¸ MARKET CAP FILTERED COLLECTION\n",
    "\n",
    "**This notebook filters for stocks with market cap > $1B**\n",
    "\n",
    "**Time requirements:**\n",
    "- **Per year:** ~70 minutes (with market cap filtering)\n",
    "- **All 6 years (2019-2024):** ~7 hours total\n",
    "- **API calls:** ~21,000 per year (vs 48,000 without filtering)\n",
    "- **Expected companies:** ~1,800 per year (15-20% of all tickers)\n",
    "\n",
    "**To start with a smaller test:**\n",
    "1. Change `MAX_TICKERS = None` to `MAX_TICKERS = 100` in any collection cell\n",
    "2. Run one year first to verify everything works\n",
    "3. Then change back to `MAX_TICKERS = None` for full collection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions with Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit configured: 750 calls/minute (0.08 seconds/call)\n",
      "Market cap filter: > $1B\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(\".env\")\n",
    "API = os.getenv(\"API\")  \n",
    "\n",
    "# Rate limiting configuration\n",
    "API_CALLS_PER_MINUTE = 750\n",
    "SECONDS_PER_CALL = 60 / API_CALLS_PER_MINUTE  # 0.08 seconds per call\n",
    "\n",
    "# Session and timer for rate limiting\n",
    "session = requests.Session()\n",
    "LAST_API_CALL = 0.0\n",
    "\n",
    "# Market cap threshold (1 billion)\n",
    "MARKET_CAP_THRESHOLD = 1e9\n",
    "\n",
    "print(f\"Rate limit configured: {API_CALLS_PER_MINUTE} calls/minute ({SECONDS_PER_CALL:.2f} seconds/call)\")\n",
    "print(f\"Market cap filter: > ${MARKET_CAP_THRESHOLD/1e9:.0f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7cNMpVzb43GKtm05iRTDWJtyJXSylX8J'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(url: str, params: Dict[str, Any] = {}) -> Optional[Any]:\n",
    "    \"\"\"Safely get JSON data from API with error handling and rate limit retry\"\"\"\n",
    "    global LAST_API_CALL, session\n",
    "    try:\n",
    "        params['apikey'] = API\n",
    "        elapsed = time.time() - LAST_API_CALL\n",
    "        if elapsed < SECONDS_PER_CALL:\n",
    "            time.sleep(SECONDS_PER_CALL - elapsed)\n",
    "        response = session.get(url, params=params, timeout=10)\n",
    "        LAST_API_CALL = time.time()\n",
    "        if response.status_code == 429:\n",
    "            print('âš ï¸  Rate limit hit! Waiting 30 seconds...')\n",
    "            time.sleep(30)\n",
    "            return get_json(url, params)\n",
    "        response.raise_for_status()\n",
    "        js = response.json()\n",
    "        if isinstance(js, dict) and 'historical' in js:\n",
    "            return js['historical']\n",
    "        elif isinstance(js, list):\n",
    "            return js\n",
    "        else:\n",
    "            return js\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f'HTTP Error {e.response.status_code}: {e}')\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f'Error fetching data: {e}')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_market_cap(ticker: str, year: int, precomputed: Optional[float] = None) -> Tuple[bool, Optional[float]]:\n",
    "    \"\"\"Check if ticker had market cap above threshold in given year\"\"\"\n",
    "    if precomputed is not None:\n",
    "        return precomputed > MARKET_CAP_THRESHOLD, precomputed\n",
    "    try:\n",
    "        start_date = f'{year}-01-01'\n",
    "        end_date = f'{year}-12-31'\n",
    "        mc_data = get_json(\n",
    "            f'https://financialmodelingprep.com/api/v3/historical-market-capitalization/{ticker}',\n",
    "            {'from': start_date, 'to': end_date}\n",
    "        )\n",
    "        if not mc_data:\n",
    "            return False, None\n",
    "        mc_df = pd.DataFrame(mc_data)\n",
    "        avg_market_cap = mc_df['marketCap'].mean()\n",
    "        return avg_market_cap > MARKET_CAP_THRESHOLD, avg_market_cap\n",
    "    except Exception as e:\n",
    "        print(f'Error checking market cap for {ticker}: {e}')\n",
    "        return False, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bulk_profiles(tickers: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Fetch company profiles in bulk.\"\"\"\n",
    "    data = get_json(f'https://financialmodelingprep.com/api/v3/profile/{','.join(tickers)}')\n",
    "    profiles = {}\n",
    "    if isinstance(data, list):\n",
    "        for item in data:\n",
    "            symbol = item.get('symbol')\n",
    "            profiles[symbol] = item\n",
    "    return profiles\n",
    "\n",
    "def get_bulk_market_caps(tickers: List[str], year: int) -> Dict[str, float]:\n",
    "    \"\"\"Fetch average market cap for a list of tickers.\"\"\"\n",
    "    start_date = f'{year}-01-01'\n",
    "    end_date = f'{year}-12-31'\n",
    "    data = get_json(\n",
    "        f'https://financialmodelingprep.com/api/v3/historical-market-capitalization/{','.join(tickers)}',\n",
    "        {'from': start_date, 'to': end_date}\n",
    "    )\n",
    "    caps = {}\n",
    "    if isinstance(data, dict):\n",
    "        for symbol, hist in data.items():\n",
    "            df = pd.DataFrame(hist)\n",
    "            caps[symbol] = df['marketCap'].mean()\n",
    "    return caps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ticker_year(ticker: str, year: int, profile_data: Optional[Dict[str, Any]] = None, avg_market_cap: Optional[float] = None) -> Tuple[Optional[pd.DataFrame], Dict[str, Any], int]:\n",
    "    \"\"\"Process data for a single ticker for a specific year\"\"\"\n",
    "    error_log = {'ticker': ticker, 'year': year, 'errors': []}\n",
    "    api_calls = 0\n",
    "    try:\n",
    "        is_large_cap, avg_market_cap = check_market_cap(ticker, year, precomputed=avg_market_cap)\n",
    "        if avg_market_cap is None:\n",
    "            api_calls += 1\n",
    "        if not is_large_cap:\n",
    "            error_log['errors'].append(f'Market cap below threshold (avg: ${avg_market_cap:,.0f})')\n",
    "            return None, error_log, api_calls\n",
    "        start_date = datetime(year, 1, 1)\n",
    "        end_date = datetime(year, 12, 31)\n",
    "        bs = get_json(f'https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}', {'period': 'quarter', 'limit': 20})\n",
    "        api_calls += 1\n",
    "        inc = get_json(f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}', {'period': 'quarter', 'limit': 20})\n",
    "        api_calls += 1\n",
    "        mc = get_json(f'https://financialmodelingprep.com/api/v3/historical-market-capitalization/{ticker}', {'from': start_date.strftime('%Y-%m-%d'), 'to': end_date.strftime('%Y-%m-%d')})\n",
    "        api_calls += 1\n",
    "        px = get_json(f'https://financialmodelingprep.com/api/v3/historical-price-full/{ticker}', {'from': start_date.strftime('%Y-%m-%d'), 'to': end_date.strftime('%Y-%m-%d')})\n",
    "        api_calls += 1\n",
    "        if profile_data is None:\n",
    "            profile = get_json(f'https://financialmodelingprep.com/api/v3/profile/{ticker}')\n",
    "            api_calls += 1\n",
    "        else:\n",
    "            profile = [profile_data] if isinstance(profile_data, dict) else profile_data\n",
    "        if not all([bs, inc, mc, px, profile]):\n",
    "            if not bs: error_log['errors'].append('No balance sheet data')\n",
    "            if not inc: error_log['errors'].append('No income statement data')\n",
    "            if not mc: error_log['errors'].append('No market cap data')\n",
    "            if not px: error_log['errors'].append('No price data')\n",
    "            if not profile: error_log['errors'].append('No profile data')\n",
    "            return None, error_log, api_calls\n",
    "        industry = profile[0].get('industry', 'Unknown') if profile and len(profile) > 0 else 'Unknown'\n",
    "        sector = profile[0].get('sector', 'Unknown') if profile and len(profile) > 0 else 'Unknown'\n",
    "        bs_df = pd.DataFrame(bs)\n",
    "        bs_df['date'] = pd.to_datetime(bs_df['date'])\n",
    "        bs_df = bs_df[bs_df['date'].dt.year == year]\n",
    "        if len(bs_df) == 0:\n",
    "            error_log['errors'].append(f'No balance sheet data for year {year}')\n",
    "            return None, error_log, api_calls\n",
    "        bs_df = (bs_df[['date', 'shortTermDebt', 'longTermDebt', 'totalAssets', 'totalStockholdersEquity', 'commonStock']]\n",
    "            .assign(quarter=lambda d: d.date.dt.to_period('Q'),\n",
    "                    debt_to_assets=lambda d: ((d.shortTermDebt.fillna(0) + d.longTermDebt.fillna(0)) / d.totalAssets.replace(0, pd.NA)),\n",
    "                    book_value=lambda d: d.totalStockholdersEquity)\n",
    "            .dropna(subset=['debt_to_assets'])\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('quarter', keep='last'))\n",
    "        inc_df = pd.DataFrame(inc)\n",
    "        inc_df['date'] = pd.to_datetime(inc_df['date'])\n",
    "        inc_df = inc_df[inc_df['date'].dt.year == year]\n",
    "        inc_df = (inc_df[['date', 'eps', 'weightedAverageShsOut']]\n",
    "            .assign(quarter=lambda d: d.date.dt.to_period('Q'))\n",
    "            .rename(columns={'weightedAverageShsOut': 'shares_outstanding'})\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('quarter', keep='last'))\n",
    "        mc_df = (pd.DataFrame(mc)\n",
    "            .assign(date=lambda d: pd.to_datetime(d.date), quarter=lambda d: d.date.dt.to_period('Q'))\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('quarter', keep='last')\n",
    "            .rename(columns={'marketCap': 'mkt_cap'})[['quarter', 'mkt_cap']])\n",
    "        px_df = (pd.DataFrame(px)\n",
    "            .assign(date=lambda d: pd.to_datetime(d.date), quarter=lambda d: d.date.dt.to_period('Q'))\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('quarter', keep='last')\n",
    "            .rename(columns={'adjClose': 'stock_price'})[['quarter', 'stock_price']])\n",
    "        merged = (bs_df.merge(inc_df, on='quarter', how='left')\n",
    "                     .merge(mc_df, on='quarter', how='left')\n",
    "                     .merge(px_df, on='quarter', how='left'))\n",
    "        merged = merged.assign(ticker=ticker, industry=industry, sector=sector,\n",
    "                               book_to_market=lambda d: (d.book_value / d.shares_outstanding) / d.stock_price,\n",
    "                               earnings_yield=lambda d: d.eps / d.stock_price)\n",
    "        merged = merged[['quarter', 'ticker', 'industry', 'sector', 'debt_to_assets', 'mkt_cap', 'stock_price', 'book_to_market', 'earnings_yield']].dropna()\n",
    "        valid_quarters = [f'{year}Q1', f'{year}Q2', f'{year}Q3', f'{year}Q4']\n",
    "        merged = merged[merged['quarter'].astype(str).isin(valid_quarters)]\n",
    "        if len(merged) == 0:\n",
    "            error_log['errors'].append('No valid data after merging')\n",
    "            return None, error_log, api_calls\n",
    "        return merged, error_log, api_calls\n",
    "    except Exception as e:\n",
    "        error_log['errors'].append(f'Exception: {str(e)}')\n",
    "        return None, error_log, api_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_year_data(tickers: List[str], year: int, max_tickers: Optional[int] = None, \n",
    "                     save_progress: bool = True, progress_interval: int = 100, batch_size: int = 50) -> Tuple[pd.DataFrame, List[Dict]]:\n",
    "    \"\"\"Collect data for multiple tickers for a specific year with optimized batch processing\"\"\"\n",
    "    all_data = []\n",
    "    all_errors = []\n",
    "    successful_tickers = []\n",
    "    failed_tickers = []\n",
    "    skipped_tickers = []\n",
    "    total_api_calls = 0\n",
    "    \n",
    "    tickers_to_process = tickers[:max_tickers] if max_tickers else tickers\n",
    "    total_tickers = len(tickers_to_process)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  COLLECTING DATA FOR YEAR {year}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total tickers to check: {total_tickers}\")\n",
    "    print(f\"Market cap filter: >${MARKET_CAP_THRESHOLD/1e9:.0f}B\")\n",
    "    print(f\"API rate limit: {API_CALLS_PER_MINUTE} calls/minute\")\n",
    "    print(f\"Batch size: {batch_size} tickers\")\n",
    "    print(f\"Progress saves: Every {progress_interval} tickers\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process tickers in batches\n",
    "    for batch_start in range(0, total_tickers, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total_tickers)\n",
    "        batch_tickers = tickers_to_process[batch_start:batch_end]\n",
    "        \n",
    "        # Progress update\n",
    "        if batch_start > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_time = elapsed / batch_start\n",
    "            remaining = (total_tickers - batch_start) * avg_time\n",
    "            \n",
    "            print(f\"\\n[Progress: {batch_start}/{total_tickers} ({batch_start/total_tickers*100:.1f}%)]\")\n",
    "            print(f\"  Time: {elapsed/60:.1f}min elapsed, ~{remaining/60:.1f}min remaining\")\n",
    "            print(f\"  Success: {len(successful_tickers)}, Failed: {len(failed_tickers)}, Skipped (small cap): {len(skipped_tickers)}\")\n",
    "            print(f\"  API calls: {total_api_calls} ({total_api_calls/elapsed*60:.0f}/minute avg)\")\n",
    "        \n",
    "        print(f\"\\n  Processing batch {batch_start//batch_size + 1}: tickers {batch_start+1}-{batch_end}\")\n",
    "        \n",
    "        # Get bulk profiles for the batch (1 API call for up to 50 tickers)\n",
    "        profiles = get_bulk_profiles(batch_tickers)\n",
    "        total_api_calls += 1\n",
    "        \n",
    "        # Process each ticker in the batch\n",
    "        for i, ticker in enumerate(batch_tickers):\n",
    "            profile_data = profiles.get(ticker)\n",
    "            \n",
    "            # Process ticker with pre-fetched profile\n",
    "            ticker_data, error_log, api_calls = process_ticker_year(ticker, year, profile_data=profile_data)\n",
    "            total_api_calls += api_calls\n",
    "            \n",
    "            if ticker_data is not None and len(ticker_data) > 0:\n",
    "                all_data.append(ticker_data)\n",
    "                successful_tickers.append(ticker)\n",
    "                print(\"âœ“\", end=\"\", flush=True)\n",
    "            elif any(\"Market cap below threshold\" in err for err in error_log.get(\"errors\", [])):\n",
    "                skipped_tickers.append(ticker)\n",
    "                print(\"â—‹\", end=\"\", flush=True)\n",
    "            else:\n",
    "                failed_tickers.append(ticker)\n",
    "                all_errors.append(error_log)\n",
    "                print(\"âœ—\", end=\"\", flush=True)\n",
    "        \n",
    "        # Save progress periodically\n",
    "        if save_progress and (batch_end % progress_interval == 0 or batch_end == total_tickers) and all_data:\n",
    "            temp_df = pd.concat(all_data, ignore_index=True)\n",
    "            temp_df['mkt_cap_rank'] = temp_df.groupby('quarter')['mkt_cap'].rank(method='dense', ascending=False).astype(int)\n",
    "            progress_filename = f\"progress_{year}_tickers_{batch_end}.csv\"\n",
    "            temp_df.to_csv(progress_filename, index=False)\n",
    "            print(f\"\\n  ðŸ’¾ Progress saved: {progress_filename} ({len(temp_df)} rows)\")\n",
    "    \n",
    "    # Final summary\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n\\n{'='*70}\")\n",
    "    print(f\"  YEAR {year} COLLECTION COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
    "    print(f\"Successful: {len(successful_tickers)} tickers\")\n",
    "    print(f\"Failed: {len(failed_tickers)} tickers\")\n",
    "    print(f\"Skipped (small cap): {len(skipped_tickers)} tickers\")\n",
    "    print(f\"Total API calls: {total_api_calls:,} ({total_api_calls/total_time*60:.0f}/minute avg)\")\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        print(\"\\nâš ï¸  No data collected!\")\n",
    "        return pd.DataFrame(columns=[\"quarter\", \"ticker\", \"industry\", \"sector\", \n",
    "                                    \"debt_to_assets\", \"mkt_cap\", \"stock_price\", \n",
    "                                    \"book_to_market\", \"earnings_yield\", \"mkt_cap_rank\"]), all_errors\n",
    "    \n",
    "    # Combine all data\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Final deduplication - ensure only one entry per ticker-quarter\n",
    "    final_df = final_df.sort_values(['ticker', 'quarter']).drop_duplicates(['ticker', 'quarter'], keep='last')\n",
    "    \n",
    "    # Add market cap ranking\n",
    "    final_df['mkt_cap_rank'] = final_df.groupby('quarter')['mkt_cap'].rank(method='dense', ascending=False).astype(int)\n",
    "    \n",
    "    # Sort by ticker and quarter\n",
    "    final_df = final_df.sort_values(['ticker', 'quarter']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Final dataset: {len(final_df)} rows, {final_df['ticker'].nunique()} tickers\")\n",
    "    print(f\"   Quarters: {sorted(final_df['quarter'].unique())}\")\n",
    "    \n",
    "    # Verify quarter coverage\n",
    "    expected_quarters = {f\"{year}Q1\", f\"{year}Q2\", f\"{year}Q3\", f\"{year}Q4\"}\n",
    "    actual_quarters = set(final_df['quarter'].astype(str).unique())\n",
    "    missing_quarters = expected_quarters - actual_quarters\n",
    "    if missing_quarters:\n",
    "        print(f\"   âš ï¸  Missing quarters: {sorted(missing_quarters)}\")\n",
    "    \n",
    "    # Save error log\n",
    "    if all_errors:\n",
    "        error_filename = f\"errors_{year}.json\"\n",
    "        with open(error_filename, 'w') as f:\n",
    "            json.dump(all_errors, f, indent=2, default=str)\n",
    "        print(f\"\\nðŸ“ Error log saved: {error_filename} ({len(all_errors)} errors)\")\n",
    "    \n",
    "    # Clean up progress files\n",
    "    if save_progress:\n",
    "        for progress_file in [f for f in os.listdir('.') if f.startswith(f'progress_{year}_')]:\n",
    "            os.remove(progress_file)\n",
    "        print(f\"ðŸ§¹ Cleaned up progress files\")\n",
    "    \n",
    "    return final_df, all_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_tickers(year: int) -> List[str]:\n",
    "    \"\"\"Get list of US tickers that existed in a specific year\"\"\"\n",
    "    print(f\"Fetching ticker list for year {year}...\")\n",
    "    \n",
    "    # Try to get historical ticker list from end of previous year\n",
    "    date = f\"{year-1}-12-31\"\n",
    "    \n",
    "    # First try to get available stocks for that date\n",
    "    available_stocks = get_json(\n",
    "        f\"https://financialmodelingprep.com/api/v3/available-traded/list\",\n",
    "        {\"date\": date}\n",
    "    )\n",
    "    \n",
    "    if available_stocks:\n",
    "        # Filter for US exchanges\n",
    "        us_tickers = [\n",
    "            stock[\"symbol\"] for stock in available_stocks \n",
    "            if stock.get(\"exchangeShortName\") in [\"NYSE\", \"NASDAQ\", \"AMEX\"]\n",
    "            and len(stock[\"symbol\"]) <= 5\n",
    "            and \".\" not in stock[\"symbol\"]\n",
    "        ]\n",
    "        print(f\"âœ… Found {len(us_tickers)} US tickers for {year}\")\n",
    "        return us_tickers\n",
    "    \n",
    "    # Fallback: use current ticker list with a warning\n",
    "    print(f\"âš ï¸  Could not get historical ticker list for {year}, using current list\")\n",
    "    tickers_data = get_json(\"https://financialmodelingprep.com/api/v3/stock/list\")\n",
    "    \n",
    "    if tickers_data:\n",
    "        # Filter for US exchanges and remove penny stocks\n",
    "        us_tickers = [\n",
    "            d[\"symbol\"] for d in tickers_data \n",
    "            if d[\"exchangeShortName\"] in [\"NYSE\", \"NASDAQ\"] \n",
    "            and (d.get(\"price\") is not None and d.get(\"price\", 0) > 5)\n",
    "            and len(d[\"symbol\"]) <= 5\n",
    "            and \".\" not in d[\"symbol\"]\n",
    "        ]\n",
    "        \n",
    "        print(f\"âœ… Found {len(us_tickers)} current US tickers\")\n",
    "        return us_tickers\n",
    "    else:\n",
    "        print(\"âŒ Failed to fetch ticker list. Using sample tickers.\")\n",
    "        return [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"META\", \"NVDA\", \"JPM\", \"JNJ\", \"V\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get List of US Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Ticker lists will be fetched for each specific year during collection\n",
      "   This ensures we only process stocks that existed in that year\n",
      "\n",
      "ðŸ’¡ The new approach:\n",
      "   1. Fetches historical ticker list for each year\n",
      "   2. Uses batch processing to reduce API calls\n",
      "   3. Removes redundant sleep delays\n",
      "\n",
      "âš¡ Expected performance improvements:\n",
      "   - ~30% faster with batch profile fetching\n",
      "   - More accurate historical data with year-specific tickers\n",
      "   - Better API rate limit utilization\n"
     ]
    }
   ],
   "source": [
    "# Note: We'll fetch year-specific ticker lists when collecting data for each year\n",
    "print(\"ðŸ“Œ Ticker lists will be fetched for each specific year during collection\")\n",
    "print(\"   This ensures we only process stocks that existed in that year\")\n",
    "print(\"\\nðŸ’¡ The new approach:\")\n",
    "print(\"   1. Fetches historical ticker list for each year\")\n",
    "print(\"   2. Uses batch processing to reduce API calls\")\n",
    "print(\"   3. Removes redundant sleep delays\")\n",
    "print(\"\\nâš¡ Expected performance improvements:\")\n",
    "print(\"   - ~30% faster with batch profile fetching\")\n",
    "print(\"   - More accurate historical data with year-specific tickers\")\n",
    "print(\"   - Better API rate limit utilization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Test with Single Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with AAPL for year 2023...\n",
      "\n",
      "Test completed in 1.69 seconds with 5 API calls\n",
      "\n",
      "âœ… Test successful!\n",
      "  quarter ticker              industry      sector  debt_to_assets  \\\n",
      "0  2023Q2   AAPL  Consumer Electronics  Technology        0.330007   \n",
      "1  2023Q3   AAPL  Consumer Electronics  Technology        0.351492   \n",
      "2  2023Q4   AAPL  Consumer Electronics  Technology        0.305617   \n",
      "\n",
      "         mkt_cap  stock_price  book_to_market  earnings_yield  \n",
      "0  3044866187580       192.05        0.020501        0.007967  \n",
      "1  2670779095140       169.74        0.023470        0.008660  \n",
      "2  2986094670390       191.13        0.024997        0.011458  \n",
      "\n",
      "Quarters found: [Period('2023Q2', 'Q-DEC'), Period('2023Q3', 'Q-DEC'), Period('2023Q4', 'Q-DEC')]\n",
      "\n",
      "Sample metrics:\n",
      "  Book-to-Market: 0.023\n",
      "  Earnings Yield: 0.009\n"
     ]
    }
   ],
   "source": [
    "# Test with AAPL for 2023\n",
    "print(\"Testing with AAPL for year 2023...\")\n",
    "test_start = time.time()\n",
    "\n",
    "test_data, test_errors, test_api_calls = process_ticker_year(\"AAPL\", 2023)\n",
    "\n",
    "test_time = time.time() - test_start\n",
    "print(f\"\\nTest completed in {test_time:.2f} seconds with {test_api_calls} API calls\")\n",
    "\n",
    "if test_data is not None:\n",
    "    print(\"\\nâœ… Test successful!\")\n",
    "    print(test_data)\n",
    "    print(f\"\\nQuarters found: {sorted(test_data['quarter'].unique())}\")\n",
    "    print(f\"\\nSample metrics:\")\n",
    "    print(f\"  Book-to-Market: {test_data['book_to_market'].mean():.3f}\")\n",
    "    print(f\"  Earnings Yield: {test_data['earnings_yield'].mean():.3f}\")\n",
    "else:\n",
    "    print(\"\\nâŒ Test failed!\")\n",
    "    print(\"Errors:\", test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Collect Data for 2024\n",
    "\n",
    "Collect data for US stocks with market cap > $1B for 2024. This will take approximately **70 minutes** with filtering.\n",
    "\n",
    "**Note:** Since 2024 is ongoing, you may have partial data (Q1-Q3 or Q1-Q4 depending on current date).\n",
    "\n",
    "**Time estimate:** ~21,000 API calls @ 300/minute = 70 minutes\n",
    "\n",
    "To test with fewer tickers first, change `MAX_TICKERS = None` to `MAX_TICKERS = 100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2024 data\n",
    "YEAR = 2024\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2024\n",
    "\n",
    "# Get historical ticker list for 2024\n",
    "us_tickers_2024 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2024, errors_2024 = collect_year_data(us_tickers_2024, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2024) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2024.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")\n",
    "    \n",
    "    # Show summary statistics\n",
    "    print(f\"\\nðŸ“ˆ Summary Statistics:\")\n",
    "    print(f\"   Debt/Assets - Mean: {data_2024['debt_to_assets'].mean():.3f}, Median: {data_2024['debt_to_assets'].median():.3f}\")\n",
    "    print(f\"   Book/Market - Mean: {data_2024['book_to_market'].mean():.3f}, Median: {data_2024['book_to_market'].median():.3f}\")\n",
    "    print(f\"   Earnings Yield - Mean: {data_2024['earnings_yield'].mean():.3f}, Median: {data_2024['earnings_yield'].median():.3f}\")\n",
    "    \n",
    "    # Show top companies (use latest quarter available)\n",
    "    latest_quarter = data_2024['quarter'].max()\n",
    "    latest_data = data_2024[data_2024['quarter'] == latest_quarter]\n",
    "    if len(latest_data) > 0:\n",
    "        print(f\"\\nðŸ† Top 10 companies by market cap ({latest_quarter}):\")\n",
    "        top_10 = latest_data.nsmallest(10, 'mkt_cap_rank')[['ticker', 'mkt_cap_rank', 'mkt_cap', 'book_to_market', 'earnings_yield', 'industry']]\n",
    "        top_10['mkt_cap'] = top_10['mkt_cap'].apply(lambda x: f\"${x/1e9:.1f}B\")\n",
    "        print(top_10.to_string(index=False))\n",
    "    \n",
    "    # Show available quarters\n",
    "    print(f\"\\nðŸ“… Available quarters for 2024: {sorted(data_2024['quarter'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Collect Data for 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2023 data\n",
    "YEAR = 2023\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2023\n",
    "\n",
    "# Get historical ticker list for 2023\n",
    "us_tickers_2023 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2023, errors_2023 = collect_year_data(us_tickers_2023, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2023) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2023.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")\n",
    "    \n",
    "    # Show summary statistics\n",
    "    print(f\"\\nðŸ“ˆ Summary Statistics:\")\n",
    "    print(f\"   Debt/Assets - Mean: {data_2023['debt_to_assets'].mean():.3f}, Median: {data_2023['debt_to_assets'].median():.3f}\")\n",
    "    print(f\"   Book/Market - Mean: {data_2023['book_to_market'].mean():.3f}, Median: {data_2023['book_to_market'].median():.3f}\")\n",
    "    print(f\"   Earnings Yield - Mean: {data_2023['earnings_yield'].mean():.3f}, Median: {data_2023['earnings_yield'].median():.3f}\")\n",
    "    \n",
    "    # Show top companies\n",
    "    q4_data = data_2023[data_2023['quarter'] == f'{YEAR}Q4']\n",
    "    if len(q4_data) > 0:\n",
    "        print(f\"\\nðŸ† Top 10 companies by market cap (Q4 {YEAR}):\")\n",
    "        top_10 = q4_data.nsmallest(10, 'mkt_cap_rank')[['ticker', 'mkt_cap_rank', 'mkt_cap', 'book_to_market', 'earnings_yield', 'industry']]\n",
    "        top_10['mkt_cap'] = top_10['mkt_cap'].apply(lambda x: f\"${x/1e9:.1f}B\")\n",
    "        print(top_10.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Template for Remaining Years\n",
    "\n",
    "Use this template for years 2022 and earlier:\n",
    "\n",
    "```python\n",
    "# Collect YEAR data\n",
    "YEAR = 20XX  # Change this\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in YEAR\n",
    "\n",
    "# Get historical ticker list for YEAR\n",
    "us_tickers_YEAR = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_YEAR, errors_YEAR = collect_year_data(us_tickers_YEAR, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_YEAR) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_YEAR.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Collect Data for 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2022 data\n",
    "YEAR = 2022\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2022\n",
    "\n",
    "# Get historical ticker list for 2022\n",
    "us_tickers_2022 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2022, errors_2022 = collect_year_data(us_tickers_2022, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2022) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2022.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Collect Data for 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2021 data\n",
    "YEAR = 2021\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2021\n",
    "\n",
    "# Get historical ticker list for 2021\n",
    "us_tickers_2021 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2021, errors_2021 = collect_year_data(us_tickers_2021, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2021) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2021.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Collect Data for 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2020 data\n",
    "YEAR = 2020\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2020\n",
    "\n",
    "# Get historical ticker list for 2020\n",
    "us_tickers_2020 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2020, errors_2020 = collect_year_data(us_tickers_2020, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2020) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2020.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Collect Data for 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2019 data\n",
    "YEAR = 2019\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2019\n",
    "\n",
    "# Get historical ticker list for 2019\n",
    "us_tickers_2019 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2019, errors_2019 = collect_year_data(us_tickers_2019, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2019) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2019.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2019.csv errors_2019.json\n",
    "!git commit -m 'Add data for 2019'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Collect Data for 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2018 data\n",
    "YEAR = 2018\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2018\n",
    "\n",
    "# Get historical ticker list for 2018\n",
    "us_tickers_2018 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2018, errors_2018 = collect_year_data(us_tickers_2018, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2018) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2018.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2018.csv errors_2018.json\n",
    "!git commit -m 'Add data for 2018'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Collect Data for 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2017 data\n",
    "YEAR = 2017\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2017\n",
    "\n",
    "# Get historical ticker list for 2017\n",
    "us_tickers_2017 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2017, errors_2017 = collect_year_data(us_tickers_2017, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2017) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2017.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2017.csv errors_2017.json\n",
    "!git commit -m 'Add data for 2017'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Collect Data for 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2016 data\n",
    "YEAR = 2016\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2016\n",
    "\n",
    "# Get historical ticker list for 2016\n",
    "us_tickers_2016 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2016, errors_2016 = collect_year_data(us_tickers_2016, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2016) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2016.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2016.csv errors_2016.json\n",
    "!git commit -m 'Add data for 2016'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Collect Data for 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2015 data\n",
    "YEAR = 2015\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2015\n",
    "\n",
    "# Get historical ticker list for 2015\n",
    "us_tickers_2015 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2015, errors_2015 = collect_year_data(us_tickers_2015, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2015) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2015.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2015.csv errors_2015.json\n",
    "!git commit -m 'Add data for 2015'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Collect Data for 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2014 data\n",
    "YEAR = 2014\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2014\n",
    "\n",
    "# Get historical ticker list for 2014\n",
    "us_tickers_2014 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2014, errors_2014 = collect_year_data(us_tickers_2014, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2014) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2014.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2014.csv errors_2014.json\n",
    "!git commit -m 'Add data for 2014'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Collect Data for 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2013 data\n",
    "YEAR = 2013\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2013\n",
    "\n",
    "# Get historical ticker list for 2013\n",
    "us_tickers_2013 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2013, errors_2013 = collect_year_data(us_tickers_2013, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2013) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2013.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2013.csv errors_2013.json\n",
    "!git commit -m 'Add data for 2013'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Collect Data for 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2012 data\n",
    "YEAR = 2012\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2012\n",
    "\n",
    "# Get historical ticker list for 2012\n",
    "us_tickers_2012 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2012, errors_2012 = collect_year_data(us_tickers_2012, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2012) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2012.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2012.csv errors_2012.json\n",
    "!git commit -m 'Add data for 2012'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Collect Data for 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2011 data\n",
    "YEAR = 2011\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2011\n",
    "\n",
    "# Get historical ticker list for 2011\n",
    "us_tickers_2011 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2011, errors_2011 = collect_year_data(us_tickers_2011, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2011) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2011.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2011.csv errors_2011.json\n",
    "!git commit -m 'Add data for 2011'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Collect Data for 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2010 data\n",
    "YEAR = 2010\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2010\n",
    "\n",
    "# Get historical ticker list for 2010\n",
    "us_tickers_2010 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2010, errors_2010 = collect_year_data(us_tickers_2010, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2010) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2010.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2010.csv errors_2010.json\n",
    "!git commit -m 'Add data for 2010'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18: Collect Data for 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2009 data\n",
    "YEAR = 2009\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2009\n",
    "\n",
    "# Get historical ticker list for 2009\n",
    "us_tickers_2009 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2009, errors_2009 = collect_year_data(us_tickers_2009, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2009) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2009.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2009.csv errors_2009.json\n",
    "!git commit -m 'Add data for 2009'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 19: Collect Data for 2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2008 data\n",
    "YEAR = 2008\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2008\n",
    "\n",
    "# Get historical ticker list for 2008\n",
    "us_tickers_2008 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2008, errors_2008 = collect_year_data(us_tickers_2008, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2008) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2008.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2008.csv errors_2008.json\n",
    "!git commit -m 'Add data for 2008'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 20: Collect Data for 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2007 data\n",
    "YEAR = 2007\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2007\n",
    "\n",
    "# Get historical ticker list for 2007\n",
    "us_tickers_2007 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2007, errors_2007 = collect_year_data(us_tickers_2007, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2007) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2007.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2007.csv errors_2007.json\n",
    "!git commit -m 'Add data for 2007'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 21: Collect Data for 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2006 data\n",
    "YEAR = 2006\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2006\n",
    "\n",
    "# Get historical ticker list for 2006\n",
    "us_tickers_2006 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2006, errors_2006 = collect_year_data(us_tickers_2006, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2006) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2006.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2006.csv errors_2006.json\n",
    "!git commit -m 'Add data for 2006'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 22: Collect Data for 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2005 data\n",
    "YEAR = 2005\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2005\n",
    "\n",
    "# Get historical ticker list for 2005\n",
    "us_tickers_2005 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2005, errors_2005 = collect_year_data(us_tickers_2005, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2005) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2005.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2005.csv errors_2005.json\n",
    "!git commit -m 'Add data for 2005'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 23: Collect Data for 2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2004 data\n",
    "YEAR = 2004\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2004\n",
    "\n",
    "# Get historical ticker list for 2004\n",
    "us_tickers_2004 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2004, errors_2004 = collect_year_data(us_tickers_2004, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2004) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2004.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2004.csv errors_2004.json\n",
    "!git commit -m 'Add data for 2004'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 24: Collect Data for 2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2003 data\n",
    "YEAR = 2003\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2003\n",
    "\n",
    "# Get historical ticker list for 2003\n",
    "us_tickers_2003 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2003, errors_2003 = collect_year_data(us_tickers_2003, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2003) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2003.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2003.csv errors_2003.json\n",
    "!git commit -m 'Add data for 2003'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 25: Collect Data for 2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2002 data\n",
    "YEAR = 2002\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2002\n",
    "\n",
    "# Get historical ticker list for 2002\n",
    "us_tickers_2002 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2002, errors_2002 = collect_year_data(us_tickers_2002, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2002) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2002.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2002.csv errors_2002.json\n",
    "!git commit -m 'Add data for 2002'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 26: Collect Data for 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2001 data\n",
    "YEAR = 2001\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2001\n",
    "\n",
    "# Get historical ticker list for 2001\n",
    "us_tickers_2001 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2001, errors_2001 = collect_year_data(us_tickers_2001, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2001) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2001.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2001.csv errors_2001.json\n",
    "!git commit -m 'Add data for 2001'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 27: Collect Data for 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 2000 data\n",
    "YEAR = 2000\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 2000\n",
    "\n",
    "# Get historical ticker list for 2000\n",
    "us_tickers_2000 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_2000, errors_2000 = collect_year_data(us_tickers_2000, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_2000) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_2000.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_2000.csv errors_2000.json\n",
    "!git commit -m 'Add data for 2000'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 28: Collect Data for 1999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 1999 data\n",
    "YEAR = 1999\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 1999\n",
    "\n",
    "# Get historical ticker list for 1999\n",
    "us_tickers_1999 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_1999, errors_1999 = collect_year_data(us_tickers_1999, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_1999) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_1999.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_1999.csv errors_1999.json\n",
    "!git commit -m 'Add data for 1999'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 29: Collect Data for 1998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 1998 data\n",
    "YEAR = 1998\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 1998\n",
    "\n",
    "# Get historical ticker list for 1998\n",
    "us_tickers_1998 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_1998, errors_1998 = collect_year_data(us_tickers_1998, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_1998) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_1998.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_1998.csv errors_1998.json\n",
    "!git commit -m 'Add data for 1998'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 30: Collect Data for 1997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 1997 data\n",
    "YEAR = 1997\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 1997\n",
    "\n",
    "# Get historical ticker list for 1997\n",
    "us_tickers_1997 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_1997, errors_1997 = collect_year_data(us_tickers_1997, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_1997) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_1997.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_1997.csv errors_1997.json\n",
    "!git commit -m 'Add data for 1997'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 31: Collect Data for 1996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 1996 data\n",
    "YEAR = 1996\n",
    "MAX_TICKERS = None  # Collect ALL US tickers that existed in 1996\n",
    "\n",
    "# Get historical ticker list for 1996\n",
    "us_tickers_1996 = get_historical_tickers(YEAR)\n",
    "\n",
    "# Collect data with optimized batch processing\n",
    "data_1996, errors_1996 = collect_year_data(us_tickers_1996, year=YEAR, max_tickers=MAX_TICKERS)\n",
    "\n",
    "if len(data_1996) > 0:\n",
    "    filename = f\"stock_data_{YEAR}.csv\"\n",
    "    data_1996.to_csv(filename, index=False)\n",
    "    print(f\"\\nâœ… Data saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add stock_data_1996.csv errors_1996.json\n",
    "!git commit -m 'Add data for 1996'\n",
    "!git push origin testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Review Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review what we've collected\n",
    "import glob\n",
    "\n",
    "print(\"ðŸ“ Available data files:\")\n",
    "data_files = sorted(glob.glob(\"stock_data_*.csv\"))\n",
    "\n",
    "total_rows = 0\n",
    "for file in data_files:\n",
    "    df = pd.read_csv(file)\n",
    "    total_rows += len(df)\n",
    "    print(f\"  {file}: {len(df):,} rows, {df['ticker'].nunique()} tickers\")\n",
    "    # Check for duplicates\n",
    "    duplicates = df.groupby(['ticker', 'quarter']).size()\n",
    "    if (duplicates > 1).any():\n",
    "        print(f\"    âš ï¸  Found {(duplicates > 1).sum()} duplicate ticker-quarter combinations\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Total: {total_rows:,} rows across {len(data_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors\n",
    "error_files = sorted(glob.glob(\"errors_*.json\"))\n",
    "\n",
    "if error_files:\n",
    "    print(\"ðŸ“ Error analysis:\")\n",
    "    \n",
    "    for error_file in error_files:\n",
    "        with open(error_file, 'r') as f:\n",
    "            errors = json.load(f)\n",
    "        \n",
    "        # Count error types\n",
    "        error_types = {}\n",
    "        for error in errors:\n",
    "            for err_msg in error.get('errors', []):\n",
    "                err_type = err_msg.split(':')[0] if ':' in err_msg else err_msg\n",
    "                error_types[err_type] = error_types.get(err_type, 0) + 1\n",
    "        \n",
    "        print(f\"\\n{error_file}: {len(errors)} failed tickers\")\n",
    "        for err_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "            print(f\"  - {err_type}: {count}\")\n",
    "else:\n",
    "    print(\"No error files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Combine All Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all years into one file\n",
    "years = [2019, 2020, 2021, 2022, 2023, 2024]\n",
    "all_data = []\n",
    "\n",
    "for year in years:\n",
    "    filename = f\"stock_data_{year}.csv\"\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df['quarter'] = pd.PeriodIndex(df['quarter'], freq='Q')\n",
    "        all_data.append(df)\n",
    "        print(f\"âœ“ Loaded {year}: {len(df)} rows\")\n",
    "    else:\n",
    "        print(f\"âœ— {filename} not found\")\n",
    "\n",
    "if all_data:\n",
    "    combined = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Final deduplication across all years\n",
    "    combined = combined.sort_values(['ticker', 'quarter']).drop_duplicates(['ticker', 'quarter'], keep='last')\n",
    "    \n",
    "    # Recalculate rankings\n",
    "    combined['mkt_cap_rank'] = combined.groupby('quarter')['mkt_cap'].rank(method='dense', ascending=False).astype(int)\n",
    "    \n",
    "    combined.to_csv(\"stock_data_combined_6years.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… Combined dataset saved!\")\n",
    "    print(f\"   Total: {len(combined):,} rows\")\n",
    "    print(f\"   Tickers: {combined['ticker'].nunique()}\")\n",
    "    print(f\"   Period: {combined['quarter'].min()} to {combined['quarter'].max()}\")\n",
    "    \n",
    "    # Show metric distributions\n",
    "    print(f\"\\nðŸ“Š Metric distributions:\")\n",
    "    print(f\"   Debt/Assets: {combined['debt_to_assets'].describe()[[\"mean\", \"50%\", \"std\"]].round(3).to_dict()}\")\n",
    "    print(f\"   Book/Market: {combined['book_to_market'].describe()[[\"mean\", \"50%\", \"std\"]].round(3).to_dict()}\")\n",
    "    print(f\"   Earnings Yield: {combined['earnings_yield'].describe()[[\"mean\", \"50%\", \"std\"]].round(3).to_dict()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
