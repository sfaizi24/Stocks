{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cdfd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stock Data Collector with Market Cap Filtering\n",
    "Collects financial data for US stocks with market cap > $1B\n",
    "Includes: debt/assets, market cap, price, book-to-market, earnings yield\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee789f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69211be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configuration\n",
    "API_KEY = \"7cNMpVzb43GKtm05iRTDWJtyJXSylX8J\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf8b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate limiting configuration\n",
    "API_CALLS_PER_MINUTE = 300\n",
    "SECONDS_PER_CALL = 60 / API_CALLS_PER_MINUTE  # 0.2 seconds per call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d87960",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Market cap threshold (1 billion)\n",
    "MARKET_CAP_THRESHOLD = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f378d7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_json(url: str, params: Dict[str, Any] = {}) -> Optional[Any]:\n",
    "    \"\"\"Safely get JSON data from API with error handling and rate limit retry\"\"\"\n",
    "    try:\n",
    "        params[\"apikey\"] = API_KEY\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        \n",
    "        # Handle rate limiting\n",
    "        if response.status_code == 429:\n",
    "            print(f\"‚ö†Ô∏è  Rate limit hit! Waiting 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "            return get_json(url, params)  # Retry\n",
    "            \n",
    "        response.raise_for_status()\n",
    "        js = response.json()\n",
    "        \n",
    "        # Handle different response formats\n",
    "        if isinstance(js, dict) and \"historical\" in js:\n",
    "            return js[\"historical\"]\n",
    "        elif isinstance(js, list):\n",
    "            return js\n",
    "        else:\n",
    "            return js\n",
    "            \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error {e.response.status_code}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dcd840",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_market_cap(ticker: str, year: int) -> Tuple[bool, Optional[float]]:\n",
    "    \"\"\"Check if ticker had market cap above threshold in given year\"\"\"\n",
    "    try:\n",
    "        # Get market cap for the year\n",
    "        start_date = f\"{year}-01-01\"\n",
    "        end_date = f\"{year}-12-31\"\n",
    "        \n",
    "        mc_data = get_json(\n",
    "            f\"https://financialmodelingprep.com/api/v3/historical-market-capitalization/{ticker}\",\n",
    "            {\"from\": start_date, \"to\": end_date}\n",
    "        )\n",
    "        \n",
    "        if not mc_data:\n",
    "            return False, None\n",
    "            \n",
    "        # Calculate average market cap for the year\n",
    "        mc_df = pd.DataFrame(mc_data)\n",
    "        avg_market_cap = mc_df[\"marketCap\"].mean()\n",
    "        \n",
    "        return avg_market_cap > MARKET_CAP_THRESHOLD, avg_market_cap\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking market cap for {ticker}: {e}\")\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aada1d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_ticker_year(ticker: str, year: int) -> Tuple[Optional[pd.DataFrame], Dict[str, Any], int]:\n",
    "    \"\"\"Process data for a single ticker for a specific year\"\"\"\n",
    "    error_log = {\"ticker\": ticker, \"year\": year, \"errors\": []}\n",
    "    api_calls = 0\n",
    "    \n",
    "    try:\n",
    "        # First check market cap (1 API call)\n",
    "        is_large_cap, avg_market_cap = check_market_cap(ticker, year)\n",
    "        api_calls += 1\n",
    "        \n",
    "        if not is_large_cap:\n",
    "            error_log[\"errors\"].append(f\"Market cap below threshold (avg: ${avg_market_cap:,.0f})\")\n",
    "            return None, error_log, api_calls\n",
    "        \n",
    "        # Calculate date range for the specific year\n",
    "        start_date = datetime(year, 1, 1)\n",
    "        end_date = datetime(year, 12, 31)\n",
    "        \n",
    "        # Get balance sheet data (API call 2)\n",
    "        bs = get_json(\n",
    "            f\"https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}\", \n",
    "            {\"period\": \"quarter\", \"limit\": 20}\n",
    "        )\n",
    "        api_calls += 1\n",
    "        \n",
    "        # Get income statement data for EPS (API call 3)\n",
    "        inc = get_json(\n",
    "            f\"https://financialmodelingprep.com/api/v3/income-statement/{ticker}\",\n",
    "            {\"period\": \"quarter\", \"limit\": 20}\n",
    "        )\n",
    "        api_calls += 1\n",
    "        \n",
    "        # Get market cap data (API call 4)\n",
    "        mc = get_json(\n",
    "            f\"https://financialmodelingprep.com/api/v3/historical-market-capitalization/{ticker}\", \n",
    "            {\"from\": start_date.strftime(\"%Y-%m-%d\"), \"to\": end_date.strftime(\"%Y-%m-%d\")}\n",
    "        )\n",
    "        api_calls += 1\n",
    "        \n",
    "        # Get price data (API call 5)\n",
    "        px = get_json(\n",
    "            f\"https://financialmodelingprep.com/api/v3/historical-price-full/{ticker}\", \n",
    "            {\"from\": start_date.strftime(\"%Y-%m-%d\"), \"to\": end_date.strftime(\"%Y-%m-%d\")}\n",
    "        )\n",
    "        api_calls += 1\n",
    "        \n",
    "        # Get company profile (API call 6)\n",
    "        profile = get_json(f\"https://financialmodelingprep.com/api/v3/profile/{ticker}\")\n",
    "        api_calls += 1\n",
    "        \n",
    "        # Track missing data\n",
    "        if not bs:\n",
    "            error_log[\"errors\"].append(\"No balance sheet data\")\n",
    "        if not inc:\n",
    "            error_log[\"errors\"].append(\"No income statement data\")\n",
    "        if not mc:\n",
    "            error_log[\"errors\"].append(\"No market cap data\")\n",
    "        if not px:\n",
    "            error_log[\"errors\"].append(\"No price data\")\n",
    "        if not profile:\n",
    "            error_log[\"errors\"].append(\"No profile data\")\n",
    "            \n",
    "        if not all([bs, inc, mc, px, profile]):\n",
    "            return None, error_log, api_calls\n",
    "        \n",
    "        # Extract company info\n",
    "        industry = profile[0].get(\"industry\", \"Unknown\")\n",
    "        sector = profile[0].get(\"sector\", \"Unknown\")\n",
    "        \n",
    "        # Process balance sheet data\n",
    "        bs_df = pd.DataFrame(bs)\n",
    "        bs_df['date'] = pd.to_datetime(bs_df['date'])\n",
    "        # Filter for specific year\n",
    "        bs_df = bs_df[bs_df['date'].dt.year == year]\n",
    "        \n",
    "        if len(bs_df) == 0:\n",
    "            error_log[\"errors\"].append(f\"No balance sheet data for year {year}\")\n",
    "            return None, error_log, api_calls\n",
    "            \n",
    "        bs_df = (\n",
    "            bs_df[['date', 'shortTermDebt', 'longTermDebt', 'totalAssets', \n",
    "                   'totalStockholdersEquity', 'commonStock']]\n",
    "            .assign(\n",
    "                quarter=lambda d: d.date.dt.to_period(\"Q\"),\n",
    "                debt_to_assets=lambda d: (\n",
    "                    (d.shortTermDebt.fillna(0) + d.longTermDebt.fillna(0)) / \n",
    "                    d.totalAssets.replace(0, pd.NA)\n",
    "                ),\n",
    "                book_value=lambda d: d.totalStockholdersEquity\n",
    "            )\n",
    "            .dropna(subset=[\"debt_to_assets\"])\n",
    "        )\n",
    "        \n",
    "        # Process income statement data\n",
    "        inc_df = pd.DataFrame(inc)\n",
    "        inc_df['date'] = pd.to_datetime(inc_df['date'])\n",
    "        inc_df = inc_df[inc_df['date'].dt.year == year]\n",
    "        \n",
    "        inc_df = (\n",
    "            inc_df[['date', 'eps', 'weightedAverageShsOut']]\n",
    "            .assign(quarter=lambda d: d.date.dt.to_period(\"Q\"))\n",
    "            .rename(columns={\"weightedAverageShsOut\": \"shares_outstanding\"})\n",
    "        )\n",
    "        \n",
    "        # Process market cap data\n",
    "        mc_df = (\n",
    "            pd.DataFrame(mc)\n",
    "            .assign(\n",
    "                date=lambda d: pd.to_datetime(d.date),\n",
    "                quarter=lambda d: d.date.dt.to_period(\"Q\")\n",
    "            )\n",
    "            .sort_values(\"date\")\n",
    "            .drop_duplicates(\"quarter\", keep=\"last\")\n",
    "            .rename(columns={\"marketCap\": \"mkt_cap\"})\n",
    "            [['quarter', 'mkt_cap']]\n",
    "        )\n",
    "        \n",
    "        # Process price data (using adjusted close)\n",
    "        px_df = (\n",
    "            pd.DataFrame(px)\n",
    "            .assign(\n",
    "                date=lambda d: pd.to_datetime(d.date),\n",
    "                quarter=lambda d: d.date.dt.to_period(\"Q\")\n",
    "            )\n",
    "            .sort_values(\"date\")\n",
    "            .drop_duplicates(\"quarter\", keep=\"last\")\n",
    "            .rename(columns={\"adjClose\": \"stock_price\"})  # Using adjusted close\n",
    "            [['quarter', 'stock_price']]\n",
    "        )\n",
    "        \n",
    "        # Merge all data\n",
    "        merged = (\n",
    "            bs_df.merge(inc_df, on=\"quarter\", how=\"left\")\n",
    "                 .merge(mc_df, on=\"quarter\", how=\"left\")\n",
    "                 .merge(px_df, on=\"quarter\", how=\"left\")\n",
    "        )\n",
    "        \n",
    "        # Calculate book-to-market and earnings yield\n",
    "        merged = merged.assign(\n",
    "            ticker=ticker,\n",
    "            industry=industry,\n",
    "            sector=sector,\n",
    "            # Book-to-Market = Book Value per Share / Price per Share\n",
    "            book_to_market=lambda d: (d.book_value / d.shares_outstanding) / d.stock_price,\n",
    "            # Earnings Yield = EPS / Price\n",
    "            earnings_yield=lambda d: d.eps / d.stock_price\n",
    "        )\n",
    "        \n",
    "        # Select final columns\n",
    "        merged = merged[[\n",
    "            'quarter', 'ticker', 'industry', 'sector', 'debt_to_assets', \n",
    "            'mkt_cap', 'stock_price', 'book_to_market', 'earnings_yield'\n",
    "        ]].dropna()\n",
    "        \n",
    "        if len(merged) == 0:\n",
    "            error_log[\"errors\"].append(\"No valid data after merging\")\n",
    "            return None, error_log, api_calls\n",
    "            \n",
    "        return merged, error_log, api_calls\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_log[\"errors\"].append(f\"Exception: {str(e)}\")\n",
    "        return None, error_log, api_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9aef7a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def collect_year_data(tickers: List[str], year: int, max_tickers: Optional[int] = None, \n",
    "                     save_progress: bool = True, progress_interval: int = 100) -> Tuple[pd.DataFrame, List[Dict], Dict]:\n",
    "    \"\"\"Collect data for multiple tickers for a specific year with strict rate limiting\"\"\"\n",
    "    all_data = []\n",
    "    all_errors = []\n",
    "    successful_tickers = []\n",
    "    failed_tickers = []\n",
    "    skipped_tickers = []\n",
    "    total_api_calls = 0\n",
    "    \n",
    "    tickers_to_process = tickers[:max_tickers] if max_tickers else tickers\n",
    "    total_tickers = len(tickers_to_process)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  COLLECTING DATA FOR YEAR {year}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total tickers to check: {total_tickers}\")\n",
    "    print(f\"Market cap filter: >${MARKET_CAP_THRESHOLD/1e9:.0f}B\")\n",
    "    print(f\"API rate limit: {API_CALLS_PER_MINUTE} calls/minute\")\n",
    "    print(f\"Progress saves: Every {progress_interval} tickers\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    last_call_time = time.time()\n",
    "    \n",
    "    for i, ticker in enumerate(tickers_to_process):\n",
    "        # Progress update\n",
    "        if i > 0 and i % 20 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_time = elapsed / i\n",
    "            remaining = (total_tickers - i) * avg_time\n",
    "            success_rate = len(successful_tickers) / (len(successful_tickers) + len(failed_tickers) + len(skipped_tickers)) * 100 if (len(successful_tickers) + len(failed_tickers) + len(skipped_tickers)) > 0 else 0\n",
    "            \n",
    "            print(f\"\\n[Progress: {i}/{total_tickers} ({i/total_tickers*100:.1f}%)]\")\n",
    "            print(f\"  Time: {elapsed/60:.1f}min elapsed, ~{remaining/60:.1f}min remaining\")\n",
    "            print(f\"  Success: {len(successful_tickers)}, Failed: {len(failed_tickers)}, Skipped (small cap): {len(skipped_tickers)}\")\n",
    "            print(f\"  API calls: {total_api_calls} ({total_api_calls/elapsed*60:.0f}/minute avg)\")\n",
    "            print(f\"  Current batch: \", end=\"\")\n",
    "        \n",
    "        # Rate limiting\n",
    "        time_since_last_call = time.time() - last_call_time\n",
    "        if time_since_last_call < SECONDS_PER_CALL:\n",
    "            time.sleep(SECONDS_PER_CALL - time_since_last_call)\n",
    "        last_call_time = time.time()\n",
    "        \n",
    "        # Process ticker\n",
    "        ticker_data, error_log, api_calls = process_ticker_year(ticker, year)\n",
    "        total_api_calls += api_calls\n",
    "        \n",
    "        if ticker_data is not None and len(ticker_data) > 0:\n",
    "            all_data.append(ticker_data)\n",
    "            successful_tickers.append(ticker)\n",
    "            print(\"‚úì\", end=\"\", flush=True)\n",
    "        elif any(\"Market cap below threshold\" in err for err in error_log.get(\"errors\", [])):\n",
    "            skipped_tickers.append(ticker)\n",
    "            print(\"‚óã\", end=\"\", flush=True)\n",
    "        else:\n",
    "            failed_tickers.append(ticker)\n",
    "            all_errors.append(error_log)\n",
    "            print(\"‚úó\", end=\"\", flush=True)\n",
    "        \n",
    "        # Save progress periodically\n",
    "        if save_progress and (i + 1) % progress_interval == 0 and all_data:\n",
    "            temp_df = pd.concat(all_data, ignore_index=True)\n",
    "            temp_df['mkt_cap_rank'] = temp_df.groupby('quarter')['mkt_cap'].rank(method='dense', ascending=False).astype(int)\n",
    "            progress_filename = f\"progress_{year}_tickers_{i+1}.csv\"\n",
    "            temp_df.to_csv(progress_filename, index=False)\n",
    "            print(f\"\\n  üíæ Progress saved: {progress_filename} ({len(temp_df)} rows)\")\n",
    "    \n",
    "    # Final summary\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n\\n{'='*70}\")\n",
    "    print(f\"  YEAR {year} COLLECTION COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
    "    print(f\"Successful: {len(successful_tickers)} tickers\")\n",
    "    print(f\"Failed: {len(failed_tickers)} tickers\")\n",
    "    print(f\"Skipped (small cap): {len(skipped_tickers)} tickers\")\n",
    "    print(f\"Total API calls: {total_api_calls:,} ({total_api_calls/total_time*60:.0f}/minute avg)\")\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        print(\"\\n‚ö†Ô∏è  No data collected!\")\n",
    "        return pd.DataFrame(columns=[\"quarter\", \"ticker\", \"industry\", \"sector\", \n",
    "                                    \"debt_to_assets\", \"mkt_cap\", \"stock_price\", \n",
    "                                    \"book_to_market\", \"earnings_yield\", \"mkt_cap_rank\"]), all_errors, {\n",
    "            \"total_api_calls\": total_api_calls,\n",
    "            \"successful_tickers\": successful_tickers,\n",
    "            \"failed_tickers\": failed_tickers,\n",
    "            \"skipped_tickers\": skipped_tickers\n",
    "        }\n",
    "    \n",
    "    # Combine all data\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df = final_df.sort_values(['ticker', 'quarter']).reset_index(drop=True)\n",
    "    \n",
    "    # Add market cap ranking\n",
    "    final_df['mkt_cap_rank'] = final_df.groupby('quarter')['mkt_cap'].rank(method='dense', ascending=False).astype(int)\n",
    "    \n",
    "    print(f\"\\nüìä Final dataset: {len(final_df)} rows, {final_df['ticker'].nunique()} tickers\")\n",
    "    print(f\"   Quarters: {sorted(final_df['quarter'].unique())}\")\n",
    "    \n",
    "    # Save error log\n",
    "    if all_errors:\n",
    "        error_filename = f\"errors_{year}.json\"\n",
    "        with open(error_filename, 'w') as f:\n",
    "            json.dump(all_errors, f, indent=2, default=str)\n",
    "        print(f\"\\nüìù Error log saved: {error_filename} ({len(all_errors)} errors)\")\n",
    "    \n",
    "    # Clean up progress files\n",
    "    if save_progress:\n",
    "        for progress_file in [f for f in os.listdir('.') if f.startswith(f'progress_{year}_')]:\n",
    "            os.remove(progress_file)\n",
    "        print(f\"üßπ Cleaned up progress files\")\n",
    "    \n",
    "    return final_df, all_errors, {\n",
    "        \"total_api_calls\": total_api_calls,\n",
    "        \"successful_tickers\": successful_tickers,\n",
    "        \"failed_tickers\": failed_tickers,\n",
    "        \"skipped_tickers\": skipped_tickers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d745387b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def estimate_collection_stats(num_tickers: int, success_rate: float = 0.15) -> Dict[str, Any]:\n",
    "    \"\"\"Estimate API calls and storage for data collection\"\"\"\n",
    "    # Estimate number of large cap stocks (>$1B market cap)\n",
    "    # Historically about 15-20% of all stocks are large cap\n",
    "    estimated_large_cap = int(num_tickers * success_rate)\n",
    "    \n",
    "    # API calls per ticker: 1 (market cap check) + 5 (data) for large cap\n",
    "    api_calls_small_cap = num_tickers - estimated_large_cap  # Just 1 call each\n",
    "    api_calls_large_cap = estimated_large_cap * 6  # 6 calls each\n",
    "    total_api_calls = api_calls_small_cap + api_calls_large_cap\n",
    "    \n",
    "    # Time estimate\n",
    "    time_minutes = total_api_calls / API_CALLS_PER_MINUTE\n",
    "    \n",
    "    # Storage estimate (rough approximation)\n",
    "    # Each ticker-quarter generates about 500 bytes\n",
    "    # Large cap stocks have 4 quarters per year\n",
    "    rows_per_year = estimated_large_cap * 4\n",
    "    bytes_per_row = 500\n",
    "    size_mb = (rows_per_year * bytes_per_row) / (1024 * 1024)\n",
    "    \n",
    "    return {\n",
    "        \"total_tickers\": num_tickers,\n",
    "        \"estimated_large_cap\": estimated_large_cap,\n",
    "        \"total_api_calls\": total_api_calls,\n",
    "        \"time_minutes\": time_minutes,\n",
    "        \"time_hours\": time_minutes / 60,\n",
    "        \"size_mb\": size_mb,\n",
    "        \"rows\": rows_per_year\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9ad1e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Collect stock financial data by year')\n",
    "    parser.add_argument('year', type=int, help='Year to collect data for (2019-2024)')\n",
    "    parser.add_argument('--max-tickers', type=int, default=None, help='Maximum number of tickers to process (for testing)')\n",
    "    parser.add_argument('--ticker-file', type=str, default=None, help='File containing list of tickers (one per line)')\n",
    "    parser.add_argument('--estimate-only', action='store_true', help='Only show estimates, don\\'t collect data')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Validate year\n",
    "    if args.year < 2019 or args.year > 2024:\n",
    "        print(f\"Error: Year must be between 2019 and 2024\")\n",
    "        return\n",
    "    \n",
    "    # Get tickers\n",
    "    if args.ticker_file and os.path.exists(args.ticker_file):\n",
    "        print(f\"Loading tickers from {args.ticker_file}...\")\n",
    "        with open(args.ticker_file, 'r') as f:\n",
    "            us_tickers = [line.strip() for line in f if line.strip()]\n",
    "        print(f\"Loaded {len(us_tickers)} tickers from file\")\n",
    "    else:\n",
    "        # Get list of US tickers from API\n",
    "        print(\"Fetching ticker list from API...\")\n",
    "        tickers_data = get_json(\"https://financialmodelingprep.com/api/v3/stock/list\")\n",
    "        \n",
    "        if tickers_data:\n",
    "            # Filter for US exchanges and remove penny stocks\n",
    "            us_tickers = [\n",
    "                d[\"symbol\"] for d in tickers_data \n",
    "                if d[\"exchangeShortName\"] in [\"NYSE\", \"NASDAQ\"] \n",
    "                and (d.get(\"price\") is not None and d.get(\"price\", 0) > 5)\n",
    "                and len(d[\"symbol\"]) <= 5\n",
    "                and \".\" not in d[\"symbol\"]\n",
    "            ]\n",
    "            \n",
    "            print(f\"‚úÖ Found {len(us_tickers)} US tickers\")\n",
    "            \n",
    "            # Save ticker list for future use\n",
    "            with open(\"us_tickers.txt\", 'w') as f:\n",
    "                for ticker in us_tickers:\n",
    "                    f.write(f\"{ticker}\\n\")\n",
    "            print(f\"   Saved to us_tickers.txt\")\n",
    "        else:\n",
    "            print(\"‚ùå Failed to fetch ticker list\")\n",
    "            return\n",
    "    \n",
    "    # Apply max tickers limit if specified\n",
    "    if args.max_tickers:\n",
    "        us_tickers = us_tickers[:args.max_tickers]\n",
    "    \n",
    "    # Show estimates\n",
    "    print(f\"\\nüìä Estimates for year {args.year}:\")\n",
    "    stats = estimate_collection_stats(len(us_tickers))\n",
    "    print(f\"   Total tickers to check: {stats['total_tickers']:,}\")\n",
    "    print(f\"   Estimated large cap (>$1B): ~{stats['estimated_large_cap']:,} stocks\")\n",
    "    print(f\"   Estimated API calls: ~{stats['total_api_calls']:,}\")\n",
    "    print(f\"   Estimated time: ~{stats['time_hours']:.1f} hours\")\n",
    "    print(f\"   Estimated data size: ~{stats['size_mb']:.1f} MB\")\n",
    "    print(f\"   Estimated rows: ~{stats['rows']:,}\")\n",
    "    \n",
    "    if args.estimate_only:\n",
    "        return\n",
    "    \n",
    "    # Confirm before proceeding\n",
    "    if not args.max_tickers or args.max_tickers > 100:\n",
    "        response = input(f\"\\nProceed with collecting data for {len(us_tickers)} tickers? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            print(\"Cancelled.\")\n",
    "            return\n",
    "    \n",
    "    # Collect data\n",
    "    data, errors, stats = collect_year_data(us_tickers, args.year, max_tickers=args.max_tickers)\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        filename = f\"stock_data_{args.year}.csv\"\n",
    "        data.to_csv(filename, index=False)\n",
    "        print(f\"\\n‚úÖ Data saved to '{filename}'\")\n",
    "        \n",
    "        # Show summary statistics\n",
    "        print(f\"\\nüìà Summary Statistics:\")\n",
    "        print(f\"   Debt/Assets - Mean: {data['debt_to_assets'].mean():.3f}, Median: {data['debt_to_assets'].median():.3f}\")\n",
    "        print(f\"   Book/Market - Mean: {data['book_to_market'].mean():.3f}, Median: {data['book_to_market'].median():.3f}\")\n",
    "        print(f\"   Earnings Yield - Mean: {data['earnings_yield'].mean():.3f}, Median: {data['earnings_yield'].median():.3f}\")\n",
    "        \n",
    "        # Show top companies\n",
    "        latest_quarter = data['quarter'].max()\n",
    "        latest_data = data[data['quarter'] == latest_quarter]\n",
    "        if len(latest_data) > 0:\n",
    "            print(f\"\\nüèÜ Top 10 companies by market cap ({latest_quarter}):\")\n",
    "            top_10 = latest_data.nsmallest(10, 'mkt_cap_rank')[['ticker', 'mkt_cap', 'book_to_market', 'earnings_yield', 'industry']]\n",
    "            top_10['mkt_cap'] = top_10['mkt_cap'].apply(lambda x: f\"${x/1e9:.1f}B\")\n",
    "            print(top_10.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d03fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
